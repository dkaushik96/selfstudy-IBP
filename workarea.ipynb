{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some examples, to try to clarify what is happening.\n",
    "\n",
    "If we draw a random matrix, without any priors, then there are $2^{N K^+}$ possible arrangements of 1s and 0s, given we know $K^+$.\n",
    "\n",
    "But in fact when we draw random matrix, with the infinite prior, there is no restriction on $K^+$, and we need to consider the probability of a matrix compared with any possible matrix drawn, including the ones where $K^+$ is really large, albeit with very tiny probability.\n",
    "\n",
    "Comparing with the earlier equation for the infinite limit of $P(\\mathbf{Z})$, it looks like the tutorial is asserting that:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{Z})_{IBP} =\n",
    "\\frac\n",
    " {\\prod_{h=1}^{2^N-1}K_h!}\n",
    "  {\\prod_{i=1}^N K_1^{(i)}!}\n",
    "P([\\mathbf{Z}])\n",
    "$$\n",
    "\n",
    "Let's denote the original $P(\\mathbf{Z})$, the one drawn from binomial features, conditionally independent on $\\pi_k$, as $P(\\mathbf{Z})_{BF}$.  And the probability of matrices drawn using the Indian Buffer Process, is $P(\\mathbf{Z})_{IBP}$.  So, we have:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{Z})_{IBP} =\n",
    "\\frac\n",
    " {\\prod_{h=1}^{2^N-1}K_h!}\n",
    "  {\\prod_{i=1}^N K_1^{(i)}!}\n",
    "P([\\mathbf{Z}])_{BF}\n",
    "$$\n",
    "\n",
    "From earlier, we have:\n",
    "\n",
    "$$P([\\mathbf{Z}])_{BF} = \\frac{K!}{\\prod_{h=0}^{2^{N}-1}K_h!} P(\\mathbf{Z})_{BF}$$\n",
    "\n",
    "So, this means that:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{Z})_{IBP} =\n",
    "\\frac{K!}\n",
    " {K_0!\\prod_{i=1}^N K_1^{(i)}!}\n",
    "P(\\mathbf{Z})_{BF}\n",
    "$$\n",
    "\n",
    "... but I cant quite see how this arises.  Perhaps, the result that the tutorial gives for $P(\\mathbf{Z})_{IBP}$ is derived from first principles?  Let's try that.\n",
    "\n",
    "For the first customer, we draw Poisson$(\\alpha)$ features.  Using the $K_1^{(i)}$ notation, the probability of the first draw is, from Poisson distribution:\n",
    "\n",
    "$$\n",
    "\\frac{\\alpha^{K_1^1}\\exp(-\\alpha)}{K_1^{(1)}!}\n",
    "$$\n",
    "\n",
    "For the second customer, we have the probability of drawing 1 and 0 for each of the existing dishes, and then the probability of drawing the next new dishes.  The number of existing dishes is: $K_1^{(1)}$.  And the probability of drawing... well...we need to know which features customer drew draw 1 for, and which ones they drew 0 for.  And looking at the final expression, we only have $K_1^{(i)}$ and $m_k$ to work with.  But we can calculate the draw for each feature using $m_k$.  Using $z_{2,k}$ for the draw of each feature for customer 2 we have:\n",
    "\n",
    "$$z_{2,k} = m_{\\dots} \\dots$$\n",
    "\n",
    ".. .wait, that wont work, because what we'd really need is the partial history of each feature.  But, what about, we simply calculate the probability of the history of each feature, after the time it's been drawn?  For this we just need to know:\n",
    "\n",
    "- when that feature was first drawn, so we know how long the history is, and\n",
    "- how many 1s in that history, which is $m_k$\n",
    "\n",
    "For the features drawn by customer 1, the length of the history, excluding the first 1, is $N - 1$.  And there are exactly $m_k - 1$ 1s in the history after the first customer, and exactly $N - m_k$ 0s.  So the probability of drawing the rest of the history, given the first customer drew a 1, i, binomial distribution.  We need some notation for the feature vector following customer 1, but we can just use the full history vector, conditional on customer 1 drawing a 1.  And let's use $\\mathbf{z}_{*,k}$ to denote the full history of feature k.  Then we have:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{z}_{*,k} \\mid z_{1,k} = 1)\n",
    "= \\mathrm{Binomial}(\\mathbf{z}_{*,k}; \\dots)\n",
    "$$\n",
    "\n",
    "... hmmm.... Still wont work, because the probability in the Binomial distribution is not fixed, eg $\\alpha$, but changes over time, ie $\\frac{m_{k_{i-1}} }{ i }$.\n",
    "\n",
    "... but it seems like there's not really any other way of figuring out the \"probability of any particular matrix being produced by this process\", so let's continue.  Let's introduce some new notation: $m_{i,k}$ is the number of times feature $k$ has been chosen by customers up to, and including, customer $i$.  Looking at the probability of the choice of dishes by customer 2, that were already chosen by customer 1, first let's write down the probability of choosing each dish.  It is:\n",
    "\n",
    "$$\n",
    "\\frac\n",
    "  {m_{i-1,k}}\n",
    "  {i}\n",
    "$$\n",
    "$$\n",
    "= \\frac\n",
    "  {m_{1,k}}\n",
    "  {2}\n",
    "$$\n",
    "\n",
    "The probability of the feature vector for customer 2, for $k \\le K_1^{(1)}$, is\n",
    "\n",
    "$$\n",
    "P(\\mathbf{z}_{2,k \\le K_1^{(1)}}) =\n",
    "\\prod_{j=1}^{K_1^{(1)}} \n",
    "  \\left(\n",
    "    \\frac\n",
    "      {m_{1,k}}\n",
    "      {2}\n",
    "   \\right)^{z_{2,j}}\n",
    "  \\left(\n",
    "    1 - \n",
    "    \\frac\n",
    "      {m_{1,k}}\n",
    "      {2}\n",
    "   \\right)^{1 - z_{2,j}}\n",
    "$$\n",
    "\n",
    "Or, for all customers, just the probability of each customer sampling existing dishes, not including the Poisson terms yet, the probability is:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{z}_{*,k \\le K_{1}^{(i-1)}}) =\n",
    "\\prod_{i=2}^N\n",
    "\\prod_{p=1}^{i-1}\n",
    "\\prod_{j=1}^{K_1^{p}} \n",
    "  \\left(\n",
    "    \\frac\n",
    "      {m_{i-1,k}}\n",
    "      {i}\n",
    "   \\right)^{z_{i,j}}\n",
    "  \\left(\n",
    "    1 - \n",
    "    \\frac\n",
    "      {m_{i - 1,k}}\n",
    "      {i}\n",
    "   \\right)^{1 - z_{i,j}}\n",
    "$$\n",
    "\n",
    "Note that there is no prior to add for any of the parameters.  In the original formulation, we were sampling from a Bernoulli distribution, parameterized on $\\pi_k$, which had a prior with hyperparameter $\\alpha$.  But here we are directly sampling based on $\\alpha$, and don't need to integerate over $\\pi_k$.  So this is the complete formulation for the probability of a matrix.  I think/hope.  Except, we do need to add in the Poisson draw probabilities.  The Poisson draw probability for customer $i$ is:\n",
    "\n",
    "$$\n",
    "\\mathrm{Poisson}\\left(\n",
    "  K_1^{(i)}; \\frac{\\alpha}{i}\n",
    "\\right) =\n",
    "\\frac{(\\alpha/i)^{K_1^{(i)}}\\exp(-\\alpha/i)}{K_1^{(i)}!}\n",
    "$$\n",
    "\n",
    "So the full probability is:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{Z})_{\\mathrm{IBP}} =\n",
    "\\prod_{i=1}^N \n",
    "  \\frac{(\\alpha/i)^{K_1^{(i)}}\\exp(-\\alpha/i)}{K_1^{(i)}!}\n",
    "\\cdot\n",
    "\\prod_{i=2}^N\n",
    "\\prod_{p=1}^{i-1}\n",
    "\\prod_{j=1}^{K_1^{p}} \n",
    "  \\left(\n",
    "    \\frac\n",
    "      {m_{i-1,k}}\n",
    "      {i}\n",
    "   \\right)^{z_{i,j}}\n",
    "  \\left(\n",
    "    1 - \n",
    "    \\frac\n",
    "      {m_{i - 1,k}}\n",
    "      {i}\n",
    "   \\right)^{1 - z_{i,j}}\n",
    "$$\n",
    "\n",
    "\n",
    "======\n",
    "\n",
    "\n",
    "$\\mathrm{Binomial}(\\mathbf{z}_{2,k \\le K_1^{(1)}}; K_1^{(1)}, )$\n",
    "\n",
    "$$\n",
    "P(\\mathbf{z}_{2,*} \\mid z_{1,k} = 1, k <= K_1^{(1)}) =\n",
    "\\prod_{j=1}^{K_1^{(1)}} (\\frac{}{}  \\frac{}{})\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
