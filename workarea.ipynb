{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example: A Linear-Gaussian Latent Feature Model with Binary Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Linear-Gaussian Model\n",
    "\n",
    "$\\mathbf{x}_i$ is generated from a Gaussian with:\n",
    "\n",
    "- mean $\\mathbf{z}_i \\mathbf{A}$\n",
    "- covariance $\\Sigma_X = \\sigma^2_X \\mathbf{I}$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{z}_i$ is a $K$-dimensional binary vector\n",
    "- $\\mathbf{A}$ is a $K$ x $D$ matrix of weights\n",
    "\n",
    "Since the mean of $\\mathbf{x}_i$ is $\\mathbf{z}_i \\mathbf{A}$, so the expectation of $\\mathbf{X}$, $\\mathbb{E}(\\mathbf{X})$, is $\\mathbf{Z} \\mathbf{A}$.\n",
    "\n",
    "Given that:\n",
    "\n",
    "- $\\mathbf{X}$ is an $N$ x $D$ matrix\n",
    "\n",
    "So:\n",
    "\n",
    "- $\\mathbf{Z}$ is an $N$ x $K$ matrix\n",
    "\n",
    "For the sake of thinking this through, this means that for example each observation $\\mathbf{x}_i$ could be low-dimensional, eg 1 or 2 dimensions, whilst the latent feature matrix $\\mathbf{Z}$ could have high-dimensional $K$, eg 1000, or more.\n",
    "\n",
    "Note that the covariance matrix is diagonal, so the Gaussians have spherical isocontours.  This constrains the solution, simplifying inference.\n",
    "\n",
    "As per the tutorial, and since each $\\mathbf{x}_i$ distributed as a symmetric Gaussian, the distribution of $\\mathbf{X}$, given $\\mathbf{Z}$, $\\mathbf{A}$ and $\\sigma_X$ is:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A}, \\sigma_X)\n",
    "=\n",
    "\\frac\n",
    "  {1}\n",
    "  {(2\\pi \\sigma_X^2)^{ND/2}}\n",
    "\\exp \\left(\n",
    "  -\n",
    "  \\frac{1}\n",
    "    {2\\sigma_X^2}\n",
    "    \\mathrm{tr}\n",
    "    (\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})^T\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})\n",
    "    )\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since $\\mathbf{Z}$ is a dimension in $N$, so the mean of each $\\mathbf{x}_i$ can be different.  $\\mathbf{A}$ does not have a dimension in $N$, and gives the properties of each mixing component. (I think)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial suggests that we should integrate out the model components $\\mathbf{A}$, and that we can do so, if we define a prior on it, which the tutorial suggests to be a Gaussian:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{A} \\mid \\sigma_A) =\n",
    "\\frac{1}\n",
    "  {(2 \\pi \\sigma_A^2)^{KD/2}}\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2\\sigma_A^2}\n",
    "  \\mathrm{tr}(\\mathbf{A}^T \\mathbf{A})\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying these two probabilities we get:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X}, \\mathbf{A} \\mid \\mathbf{Z}, \\sigma_X)\n",
    "= p(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A}, \\sigma_X) \\, p(\\mathbf{A} \\mid \\sigma_A)\n",
    "$$\n",
    "$$\n",
    "= \n",
    "\\frac\n",
    "  {1}\n",
    "  {(2\\pi \\sigma_X^2)^{ND/2}}\n",
    "\\exp \\left(\n",
    "  -\n",
    "  \\frac{1}\n",
    "    {2\\sigma_X^2}\n",
    "    \\mathrm{tr}\n",
    "    (\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})^T\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})\n",
    "    )\n",
    "\\right)\n",
    "\\cdot\n",
    "\\frac{1}\n",
    "  {(2 \\pi \\sigma_A^2)^{KD/2}}\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2\\sigma_A^2}\n",
    "  \\mathrm{tr}(\\mathbf{A}^T \\mathbf{A})\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradually working through the tutorial expressions:\n",
    "\n",
    "$$\n",
    "\\propto\n",
    "\\exp\n",
    "\\left(\n",
    "- \\frac{1}{2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{X}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{X}^T \\mathbf{Z} \\mathbf{A}\n",
    "    + \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{Z}\\mathbf{A}\n",
    "    + \\frac{1}{\\sigma_A^2} \\mathbf{A}^T \\mathbf{A}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{X}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{X}^T \\mathbf{Z} \\mathbf{A}\n",
    "    + \\mathbf{A}^T(\\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z} + \\frac{1}{\\sigma_A^2}\\mathbf{I})\\mathbf{A}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: Matrix and Gaussian revision\n",
    "\n",
    "At this point, I had to reach out to revise some properties of matrices and Gaussians that I ~~had forgotten~~ didnt know.  Some of the resources I used for this section:\n",
    "\n",
    "- \"Bayesian Linear Regression\", Minka, 1998 (revised 2010)\n",
    "- https://en.wikipedia.org/wiki/Gaussian_integral\n",
    "- https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n",
    "- https://en.wikipedia.org/wiki/Matrix_normal_distribution\n",
    "- [https://en.wikipedia.org/wiki/Vectorization_(mathematics)](https://en.wikipedia.org/wiki/Vectorization_(mathematics%29)\n",
    "- https://en.wikipedia.org/wiki/Conjugate_transpose\n",
    "\n",
    "#### Matrix normal distribution, first glance\n",
    "\n",
    "So, going through, bit by bit: the integral of a Multivariate normal distribution is:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}\n",
    "\\exp \\left(\n",
    "   -\\frac{1}{2}\n",
    "   (\\mathbf{x}^T \\mathbf{A} \\mathbf{x})\n",
    "\\right)\n",
    "\\,\n",
    "d\\mathbf{x}\n",
    "=\n",
    "\\sqrt{\\frac{(2\\pi)^D}{\\mathrm{det}\\,\\mathbf{A}}}\n",
    "$$\n",
    "\n",
    "*However*, in this case, $\\mathbf{x}$ is a vector, but we will have $\\mathbf{X}$, or something of this sort, ie: a matrix.  So, what we will have is in fact: a Matrix normal distribution.  A Matrix normal distribution, per wikipedia article, has the form:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X} \\mid \\mathbf{M}, \\mathbf{U}, \\mathbf{V})\n",
    "= \\frac\n",
    "  {\\exp(\n",
    "     -\\frac{1}{2}\n",
    "     \\mathrm{tr}(\n",
    "       \\mathbf{V}^{-1}\n",
    "       (\\mathbf{X} - \\mathbf{M})^T\n",
    "       \\mathbf{U}^{-1}\n",
    "       (\\mathbf{X} - \\mathbf{M})\n",
    "     )\n",
    "  }\n",
    "  {(2\\pi)^{NK/2} \\left| \\mathbf{V} \\right|^{N/2} \\left| \\mathbf{U} \\right|^{K/2}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathrm{tr}$ denotes \"trace\"\n",
    "- $\\mathrm{M}$ is $N$ x $K$\n",
    "- $\\mathrm{U}$ is $N$ x $N$ (so: square)\n",
    "- $\\mathrm{V}$ is $v$ x $v$ (also square)\n",
    "\n",
    "(where I've changed $n$ in the Wikipedia article to $N$, and $p$ to $K$, in line with the notation we are using elsewhere)\n",
    "\n",
    "The article then states that the relationship to Multivariate normal is:\n",
    "\n",
    "$$\n",
    "\\mathrm{vec}(\\mathbf{X})\n",
    "\\sim\n",
    "\\mathcal{N}_{NK}(\\mathrm{vec}(\\mathbf{M}, \\mathbf{V} \\otimes \\mathbf{U}))\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\otimes$ is Kronecker Product\n",
    "- $\\mathrm{vec}$ is vectorization\n",
    "\n",
    "#### Vectorization\n",
    "\n",
    "What is Kronecker Product, and what is vectorization?  The [Wikipedia article](https://en.wikipedia.org/wiki/Vectorization_(mathematics%29) gives a good description of vectorization.  You stack each column of the matrix on top of each other, so it becomes a vector:\n",
    "\n",
    "If:\n",
    "$$\n",
    "\\mathbf{A}\n",
    "=\n",
    "\\begin{bmatrix} a & b \\\\\n",
    "c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "\\mathrm{vec}(\\mathbf{A})\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "c \\\\\n",
    "b \\\\\n",
    "e \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, if $\\mathbf{A}$ is $M$ x $N$, and the result of $\\mathrm{vec}(\\mathbf{A})$ is $C$, then:\n",
    "$$\n",
    "a_{i,j}\n",
    "= \n",
    "c_{jM + i,1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kronecker Product\n",
    "\n",
    "The Kronecker product of matrices $\\mathbf{A}$ and $\\mathbf{B}$ is formed by tiling the matrices $\\mathbf{A}$ and $\\mathbf{B}$ as follows, and then forming the per-element product.  If we have the following matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\n",
    "=\n",
    "\\begin{bmatrix} a & b \\\\\n",
    "c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{B}\n",
    "=\n",
    "\\begin{bmatrix} e & f \\\\\n",
    "g & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then matrix $\\mathbf{A}$ will be tiled like:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & a & b & b \\\\\n",
    "a & a & b & b \\\\\n",
    "c & c & d & d \\\\\n",
    "c & c & d & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Matrix $\\mathbf{B}$ will be tiled like:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e & f & e & f \\\\\n",
    "g & h & g & h \\\\\n",
    "e & f & e & f \\\\\n",
    "g & h & g & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "... and the Kronecker product is the Hadamard (per-element) product of these tiled matrices:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae & af & be & bf \\\\\n",
    "ag & ah & bg & bh \\\\\n",
    "ce & cf & de & df \\\\\n",
    "cg & ch & dg & dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's compare this with the matrix product of these two matrices.  This is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + bg & af + bh \\\\\n",
    "ce + dg & cf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are 8 pairs of products in the matrix product above, and 16 in this Kronecker product, so it seems not obvious to relate the two, eg via vectorization, somehow?\n",
    "\n",
    "#### Relationship between vectorization and inner product\n",
    "\n",
    "Wikipedia states that we can form a relationship between vectorization and the matrix product. For square, real matrices:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}\n",
    "(\\mathbf{A}^T \\mathbf{B})\n",
    "= \\mathrm{vec}(\\mathbf{A})^T \\mathrm{vec}(\\mathbf{B})\n",
    "$$\n",
    "\n",
    "\n",
    "Let's try this, for the example matrices above.\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{A})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "c \\\\\n",
    "b \\\\\n",
    "d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{B})$ is:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "e \\\\\n",
    "g \\\\\n",
    "f \\\\\n",
    "h \\\\\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So:\n",
    "\n",
    "$$\\mathrm{vec}^T(\\mathbf{A})\\mathrm{vec}(\\mathbf{B}) = ae + cg + bf + dh$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{A}^T$ is:\n",
    "$$\n",
    "\\begin{bmatrix} a & c \\\\\n",
    "b & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "...and $\\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} e & f \\\\\n",
    "g & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $\\mathbf{A}^T \\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + cg & af + ch \\\\\n",
    "be + dg & bf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{tr}$ is the sum of the diagonal, for a square matrix.  For example, for matrix $\\mathrm{X}$ of size $m$, $\\mathrm{tr}(\\mathbf{X})$ is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\mathbf{X}) = \\sum_{i=1}^m x_{i,i}\n",
    "$$\n",
    "\n",
    "So, $\\mathrm{tr}(\\mathbf{A}^T \\mathbf{B})$, in our example above, is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\mathbf{A}^T \\mathbf{B})\n",
    "= ae + cg + bf + dh\n",
    "$$\n",
    "\n",
    "... which matches the result for $\\mathrm{vec}(\\mathbf{A})^T \\mathrm{vec}(\\mathbf{B})$\n",
    "\n",
    "More generally, let's try for two $n$ x $n$ matrices $\\mathbf{A}$ and $\\mathbf{B}$.  The vectorizations will look like:\n",
    "\n",
    "$$\n",
    "a_{i,j} = \\mathrm{vec}(\\mathbf{A})_{jn + i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{k,l} = \\mathrm{vec}(\\mathbf{B})_{ln + k}\n",
    "$$\n",
    "\n",
    "To form the inner product of the vectorizations, let's use two nested sums.  The innermost sum will be over each row in a column, and the outermost will be over columns.  So this will give:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \n",
    "\\sum_{j=1}^{n}\n",
    "a_{j,i} b_{j,i}\n",
    "$$\n",
    "\n",
    "Meanwhile, $\\mathbf{A}^T$ is:\n",
    "\n",
    "$$\n",
    "(\\mathbf{A}^T)_{i,j} = a_{j,i}\n",
    "$$\n",
    "\n",
    "... and the matrix product $\\mathbf{A}^T\\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "(\\mathbf{A}^T\\mathbf{B})_{i,j} = \\sum_{k=1}^n a_{k,i} b_{k,j}\n",
    "$$\n",
    "The trace of this, is the sum over the diagonal, ie the sum of terms where $i = j$.  This gives:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\mathbf{A}^T\\mathbf{B}) = \\sum_{l=1}^n \\sum_{k=1}^n a_{k,l} b_{k,l}\n",
    "$$\n",
    "\n",
    "By inspection, this is identical to the expression for $\\mathrm{vec}(\\mathbf{A})^T \\mathrm{vec}(\\mathbf{B})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationship between vectorization and Kronecker product\n",
    "\n",
    "Wikipedia asserts that:\n",
    "\n",
    "$$\n",
    "\\mathrm{vec}(\\mathbf{A} \\mathbf{B})\n",
    "= (\\mathbf{I}_m \\otimes \\mathbf{A})\\mathrm{vec}(\\mathbf{B})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try for the example matrices above:\n",
    "\n",
    "$\\mathbf{I}_m \\otimes \\mathbf{A}$ is Hadamard product of:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b & a & b \\\\\n",
    "c & d & c & d \\\\\n",
    "a & b & a & b \\\\\n",
    "c & d & c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b & 0 & 0 \\\\\n",
    "c & d & 0 & 0 \\\\\n",
    "0 & 0 & a & b \\\\\n",
    "0 & 0 & c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{B})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e \\\\\n",
    "g \\\\\n",
    "f \\\\\n",
    "h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $(\\mathbf{I}_m \\otimes \\mathbf{A})\\mathrm{vec}(\\mathbf{B})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + bg \\\\\n",
    "ce + dg \\\\\n",
    "af + bh \\\\\n",
    "cf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Meanwhile, $\\mathbf{A} \\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + bg & af + bh \\\\\n",
    "ce + dg & cf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And vectorization of this is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + bg \\\\\n",
    "ce + dg \\\\\n",
    "af + bh \\\\\n",
    "cf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "... which matches the value in this case for $(\\mathbf{I}_m \\otimes \\mathbf{A})\\mathrm{vec}(\\mathbf{B})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationship between Kronecker product and vectorization, part 2\n",
    "\n",
    "More generally, we have:\n",
    "\n",
    "$$\n",
    "(\\mathbf{B}^T \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{X})\n",
    "= \\mathrm{vec}(\\mathbf{A} \\mathbf{X} \\mathbf{B})\n",
    "$$\n",
    "\n",
    "Let's try this for the example matrices $\\mathbf{A}$ and $\\mathbf{B}$ above, and a new example matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "i & j \\\\\n",
    "k & l \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, $\\mathbf{B}^T$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e & g \\\\\n",
    "f & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathbf{B}^T \\otimes \\mathbf{A}$ is Hadamard product, $\\odot$, of:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e & e & g & g \\\\\n",
    "e & e & g & g \\\\\n",
    "f & f & h & h \\\\\n",
    "f & f & h & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b & a & b\\\\\n",
    "c & d & c & d \\\\\n",
    "a & b & a & b\\\\\n",
    "c & d & c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ea & eb & ga & gb\\\\\n",
    "ec & ed & gc & gd \\\\\n",
    "fa & fb & ha & hb\\\\\n",
    "fc & fd & hc & hd \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{X})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "i \\\\\n",
    "k \\\\\n",
    "j \\\\\n",
    "l \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And so $(\\mathbf{B}^T \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{X})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "eai + ebk + gaj + gbl \\\\\n",
    "eci + edk + gcj + gdl \\\\\n",
    "fai + fbk + haj + hbl \\\\\n",
    "fci + fdk + hcj + hdl \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate $\\mathrm{vec}(\\mathbf{A}\\mathbf{X}\\mathbf{B})$:\n",
    "\n",
    "$\\mathbf{A}\\mathbf{X}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ai + bk & aj + bl \\\\\n",
    "ci + dk & cj + dl \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, $\\mathbf{A}\\mathbf{X}\\mathbf{B}$ is:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "aie + bke + ajg + blg & aif + bkf + ajh + blh \\\\\n",
    "cie + dke + cjg + dlg & cif + dkf + cjh + dlh \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "And then vectorization of this is:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "aie + bke + ajg + blg \\\\\n",
    "cie + dke + cjg + dlg \\\\\n",
    "aif + bkf + ajh + blh \\\\\n",
    "cif + dkf + cjh + dlh \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "... which matches the result for $(\\mathbf{B}^T \\otimes \\mathbf{A})\\mathrm{vec}(\\mathbf{X})$ above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that this means that:\n",
    "\n",
    "$\\mathrm{vec}(AB) = (\\mathbf{B}^T \\otimes \\mathbf{A})\\mathrm{vec}(\\mathbf{I})$\n",
    "\n",
    "Let's try, with the example matrices above.  $\\mathbf{B}^T \\otimes \\mathbf{A}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ea & eb & ga & gb\\\\\n",
    "ec & ed & gc & gd \\\\\n",
    "fa & fb & ha & hb\\\\\n",
    "fc & fd & hc & hd \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{I})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, forming the matrix product of these we have:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ea + gb \\\\\n",
    "ec + gd \\\\\n",
    "fa + hb \\\\\n",
    "fc + hd \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{A}\\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + bg & af + bh \\\\\n",
    "ce + dg & cf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "... and the vectorization of this matches $(\\mathbf{B}^T \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{I})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kronecker product, intuition\n",
    "\n",
    "The Kronecker product forms every possible pair of products between the elements of the two input matrices.  It doesnt add any of these terms though, unlike a matrix product.  To form a matrix product, we therefore need some way of:\n",
    "\n",
    "- filtering out the terms we want (eg for two 2x2 matrices, the Kronecker product has 16 pairs of products, but we only need 8 for the matrix product)\n",
    "- adding these\n",
    "\n",
    "By forming the matrix product with the vectorization of the identity matrix $\\mathbf{I}$, we can handle both of these requirements.\n",
    "\n",
    "But the Kronecker product is more general than this, since it contains every pair of products between the two input matrices.  Hence eg can be used to form the matrix product of three matrices, $\\mathbf{A}\\mathbf{X}\\mathbf{B}$, using only one single matrix product.  (The Kronecker product itself is a tiling operation followed by per-element multiply)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-normal distribution revisited\n",
    "\n",
    "Working through the Minka paper, \"Bayesian linear regression\", and starting at section 1, we have the following data model:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{e}$$\n",
    "$$\\mathbf{e} \\sim \\mathcal{N}(0, \\mathbf{V})$$\n",
    "$$p(\\mathbf{y} \\mid \\mathbf{x}, \\mathbf{A}, \\mathbf{V}) \\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x}, \\mathbf{V})$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{x}$ is an input vector of length $m$, ie $m$ features\n",
    "- $\\mathbf{A}$ is a coefficient matrix, or personally I'd normally call this the \"parameter\", or \"weight\", matrix\n",
    "- $\\mathbf{e}$ is Gaussian noise\n",
    "- $\\mathbf{y}$ is the output vector, of length $d$\n",
    "\n",
    "We are only concerned with the conditional model for $\\mathbf{y}$, $p(\\mathbf{y} \\mid \\mathbf{x})$.  The distribution of $\\mathbf{x}$ is not considered, and is irrelevant for the contents of Minka's paper.\n",
    "\n",
    "We have $N$ inpt vectors, $\\mathbf{x}_1,\\dots,\\mathbf{x}_N$ and target values $\\mathbf{y}_1,\\dots,\\mathbf{y}_N$ forming a data set of $N$ exchangeable data points $D = \\{(\\mathbf{y}_1,\\mathbf{x}_1),\\dots,(\\mathbf{y}_N,\\mathbf{x}_N)\\}$.  $\\mathbf{Y}$ is $[\\mathbf{y}_1,\\dots,\\mathbf{y}_N]$, and $\\mathbf{X}$ is $[\\mathbf{x}_1,\\dots,\\mathbf{x}_N]$.  The distribution of $\\mathbf{Y}$ given $\\mathbf{X}$ is:\n",
    "\n",
    "$$p(\\mathbf{Y} \\mid \\mathbf{X}, \\mathbf{A}, \\mathbf{V})\n",
    "= \\prod_i\n",
    "p(\\mathbf{y}_i \\mid \\mathbf{x}_i, \\mathbf{A}, \\mathbf{V})\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{\\left| 2\\pi \\mathbf{V}\\right|^{N/2}}\n",
    "\\exp \\left(\n",
    "   - \\frac{1}{2}\n",
    "   \\sum_i(\\mathbf{y}_i - \\mathbf{A}\\mathbf{x}_i)^T \\mathbf{V}^{-1}(\\mathbf{y}_i - \\mathbf{A}\\mathbf{x}_i)\n",
    "\\right)\n",
    "$$\n",
    "Minka then writes down the sum inside the exponential as a trace of matrix products:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\n",
    "  \\mathbf{V}^{-1}(\\mathbf{Y} - \\mathbf{A}\\mathbf{X})(\\mathbf{Y} - \\mathbf{A}\\mathbf{X})^T)\n",
    ")\n",
    "$$\n",
    "\n",
    "I can kind of see this is plausibly the same, but I think it might be educational to work through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace of a square matrix $\\mathbf{X}$ is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\mathbf{X})\n",
    "= \\sum_{i=1}^m x_{i,i}\n",
    "$$\n",
    "\n",
    "where $m$ is the size of each dimension of the square matrix $\\mathbf{X}$\n",
    "\n",
    "Let's say we have:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i^T \\mathbf{R} \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i$ is a vector, and $\\mathbf{R}$ is a matrix.\n",
    "\n",
    "And form the sum over $N$ values of $\\mathbf{x}_i$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\mathbf{x}_i^T \\mathbf{R}\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\mathbf{x}_i^T \\mathbf{R})_{j}\n",
    "=\n",
    "\\sum_{k=1}^m x_{i,k} r_{k,j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\mathbf{x}_i^T \\mathbf{R}\\mathbf{x}_i)\n",
    "=\n",
    "\\sum_{l=1}^m\n",
    "\\sum_{k=1}^m\n",
    "x_{i,k} r_{k,l} x_{i,l}\n",
    "$$\n",
    "\n",
    "(where here, $x_{i,j}$ means: the $j$th value of the vector $\\mathbf{x}_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, let's form the product $\\mathbf{X}^T \\mathbf{R} \\mathbf{X}$, and investigate the properties of this product:\n",
    "\n",
    "$$\n",
    "(\\mathbf{X}^T \\mathbf{R})_{i,j}\n",
    "=\n",
    "\\sum_{k=1}^m\n",
    "x_{k,i}\n",
    "r_{k,j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\mathbf{X}^T \\mathbf{R} \\mathbf{X})_{i,j}\n",
    "= \\sum_{l=1}^m\n",
    "\\sum_{k=1}^m\n",
    "x_{k,i}\n",
    "r_{k,l}\n",
    "x_{l,j}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Old notes, not working yet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial says we should complete the square now, ~~since we want to find the mean and covariance matrix of the resulting Gaussian:~~  I had to think about this.  We're actually completing the square in terms of $\\mathbf{A}$, which I guess is because we intend to integrate over $\\mathbf{A}$ later?\n",
    "\n",
    "$$\n",
    "= \\exp \\left(\n",
    " - \\frac{1}{2}\n",
    " \\mathrm{tr}\n",
    "   \\left(\n",
    "     ()^T()\n",
    "   \\right)\n",
    " \\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
