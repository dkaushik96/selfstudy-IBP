{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example: A Linear-Gaussian Latent Feature Model with Binary Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Linear-Gaussian Model\n",
    "\n",
    "$\\mathbf{x}_i$ is generated from a Gaussian with:\n",
    "\n",
    "- mean $\\mathbf{z}_i \\mathbf{A}$\n",
    "- covariance $\\Sigma_X = \\sigma^2_X \\mathbf{I}$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{z}_i$ is a $K$-dimensional binary vector\n",
    "- $\\mathbf{A}$ is a $K$ x $D$ matrix of weights\n",
    "\n",
    "Since the mean of $\\mathbf{x}_i$ is $\\mathbf{z}_i \\mathbf{A}$, so the expectation of $\\mathbf{X}$, $\\mathbb{E}(\\mathbf{X})$, is $\\mathbf{Z} \\mathbf{A}$.\n",
    "\n",
    "Given that:\n",
    "\n",
    "- $\\mathbf{X}$ is an $N$ x $D$ matrix\n",
    "\n",
    "So:\n",
    "\n",
    "- $\\mathbf{Z}$ is an $N$ x $K$ matrix\n",
    "\n",
    "For the sake of thinking this through, this means that for example each observation $\\mathbf{x}_i$ could be low-dimensional, eg 1 or 2 dimensions, whilst the latent feature matrix $\\mathbf{Z}$ could have high-dimensional $K$, eg 1000, or more.\n",
    "\n",
    "Note that the covariance matrix is diagonal, so the Gaussians have spherical isocontours.  This constrains the solution, simplifying inference.\n",
    "\n",
    "As per the tutorial, and since each $\\mathbf{x}_i$ distributed as a symmetric Gaussian, the distribution of $\\mathbf{X}$, given $\\mathbf{Z}$, $\\mathbf{A}$ and $\\sigma_X$ is:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A}, \\sigma_X)\n",
    "=\n",
    "\\frac\n",
    "  {1}\n",
    "  {(2\\pi \\sigma_X^2)^{ND/2}}\n",
    "\\exp \\left(\n",
    "  -\n",
    "  \\frac{1}\n",
    "    {2\\sigma_X^2}\n",
    "    \\mathrm{tr}\n",
    "    (\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})^T\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})\n",
    "    )\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since $\\mathbf{Z}$ is a dimension in $N$, so the mean of each $\\mathbf{x}_i$ can be different.  $\\mathbf{A}$ does not have a dimension in $N$, and gives the properties of each mixing component. (I think)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial suggests that we should integrate out the model components $\\mathbf{A}$, and that we can do so, if we define a prior on it, which the tutorial suggests to use a matrix Gaussian:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{A} \\mid \\sigma_A) =\n",
    "\\frac{1}\n",
    "  {(2 \\pi \\sigma_A^2)^{KD/2}}\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2\\sigma_A^2}\n",
    "  \\mathrm{tr}(\\mathbf{A}^T \\mathbf{A})\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying these two probabilities we get the joint probability of $\\mathbf{X}$ and $\\mathbf{A}$:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X}, \\mathbf{A} \\mid \\mathbf{Z}, \\sigma_X)\n",
    "= p(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A}, \\sigma_X) \\, p(\\mathbf{A} \\mid \\sigma_A)\n",
    "$$\n",
    "$$\n",
    "= \n",
    "\\frac\n",
    "  {1}\n",
    "  {(2\\pi \\sigma_X^2)^{ND/2}}\n",
    "\\exp \\left(\n",
    "  -\n",
    "  \\frac{1}\n",
    "    {2\\sigma_X^2}\n",
    "    \\mathrm{tr}\n",
    "    (\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})^T\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})\n",
    "    )\n",
    "\\right)\n",
    "\\cdot\n",
    "\\frac{1}\n",
    "  {(2 \\pi \\sigma_A^2)^{KD/2}}\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2\\sigma_A^2}\n",
    "  \\mathrm{tr}(\\mathbf{A}^T \\mathbf{A})\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradually working through the tutorial expressions:\n",
    "\n",
    "$$\n",
    "\\propto\n",
    "\\exp\n",
    "\\left(\n",
    "- \\frac{1}{2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{X}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{X}^T \\mathbf{Z} \\mathbf{A}\n",
    "    + \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{Z}\\mathbf{A}\n",
    "    + \\frac{1}{\\sigma_A^2} \\mathbf{A}^T \\mathbf{A}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{X}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{X}^T \\mathbf{Z} \\mathbf{A}\n",
    "    + \\mathbf{A}^T(\\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z} + \\frac{1}{\\sigma_A^2}\\mathbf{I})\\mathbf{A}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: Matrix and Gaussian revision\n",
    "\n",
    "At this point, I had to reach out to revise some properties of matrices and Gaussians that I ~~had forgotten~~ never learned.  Some of the resources I used for this section:\n",
    "\n",
    "- \"Bayesian Linear Regression\", Minka, 1998 (revised 2010)\n",
    "- https://en.wikipedia.org/wiki/Gaussian_integral\n",
    "- https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n",
    "- https://en.wikipedia.org/wiki/Matrix_normal_distribution\n",
    "- [https://en.wikipedia.org/wiki/Vectorization_(mathematics)](https://en.wikipedia.org/wiki/Vectorization_(mathematics%29)\n",
    "- https://en.wikipedia.org/wiki/Conjugate_transpose\n",
    "\n",
    "(Note that it turns out in hind-sight, that it is sufficient to remember that probability distributions integrate to 1, to know how to integrate expressions in the form of a matrix-normal distribution, so you can skip pretty much most of the below section if you want.  But personally I thought learning about Kronecker products and matrix-normal distributions was really interesting, and gave me a bit more confidence that my maths is fractionally less terrible than it is)\n",
    "\n",
    "### Matrix normal distribution, first glance\n",
    "\n",
    "So, going through, bit by bit: the integral of a Multivariate normal distribution is:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}\n",
    "\\exp \\left(\n",
    "   -\\frac{1}{2}\n",
    "   (\\mathbf{x}^T \\mathbf{A} \\mathbf{x})\n",
    "\\right)\n",
    "\\,\n",
    "d\\mathbf{x}\n",
    "=\n",
    "\\sqrt{\\frac{(2\\pi)^D}{\\mathrm{det}\\,\\mathbf{A}}}\n",
    "$$\n",
    "\n",
    "*However*, in this case, $\\mathbf{x}$ is a vector, but we will have $\\mathbf{X}$, or something of this sort, ie: a matrix.  So, what we will have is in fact: a Matrix normal distribution.  A Matrix normal distribution, per wikipedia article, has the form:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X} \\mid \\mathbf{M}, \\mathbf{U}, \\mathbf{V})\n",
    "= \\frac\n",
    "  {\\exp(\n",
    "     -\\frac{1}{2}\n",
    "     \\mathrm{tr}(\n",
    "       \\mathbf{V}^{-1}\n",
    "       (\\mathbf{X} - \\mathbf{M})^T\n",
    "       \\mathbf{U}^{-1}\n",
    "       (\\mathbf{X} - \\mathbf{M})\n",
    "     )\n",
    "  }\n",
    "  {(2\\pi)^{NK/2} \\left| \\mathbf{V} \\right|^{N/2} \\left| \\mathbf{U} \\right|^{K/2}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathrm{tr}$ denotes \"trace\"\n",
    "- $\\mathrm{M}$ is $N$ x $K$\n",
    "- $\\mathrm{U}$ is $N$ x $N$ (so: square)\n",
    "- $\\mathrm{V}$ is $v$ x $v$ (also square)\n",
    "\n",
    "(where I've changed $n$ in the Wikipedia article to $N$, and $p$ to $K$, in line with the notation we are using elsewhere)\n",
    "\n",
    "The article then states that the relationship to Multivariate normal is:\n",
    "\n",
    "$$\n",
    "\\mathrm{vec}(\\mathbf{X})\n",
    "\\sim\n",
    "\\mathcal{N}_{NK}(\\mathrm{vec}(\\mathbf{M}, \\mathbf{V} \\otimes \\mathbf{U}))\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\otimes$ is Kronecker Product\n",
    "- $\\mathrm{vec}$ is vectorization\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "What is Kronecker Product, and what is vectorization?  The [Wikipedia article](https://en.wikipedia.org/wiki/Vectorization_(mathematics%29) gives a good description of vectorization.  You stack each column of the matrix on top of each other, so it becomes a vector:\n",
    "\n",
    "If:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\n",
    "=\n",
    "\\begin{bmatrix} a & b \\\\\n",
    "c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\mathrm{vec}(\\mathbf{A})\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "c \\\\\n",
    "b \\\\\n",
    "e \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, if $\\mathbf{A}$ is $M$ x $N$, and the result of $\\mathrm{vec}(\\mathbf{A})$ is $C$, then:\n",
    "\n",
    "$$\n",
    "a_{i,j}\n",
    "= \n",
    "c_{jM + i,1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kronecker Product\n",
    "\n",
    "The Kronecker product of matrices $\\mathbf{A}$ and $\\mathbf{B}$ is formed by tiling the matrices $\\mathbf{A}$ and $\\mathbf{B}$ as follows, and then forming the per-element product.  If we have the following matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\n",
    "=\n",
    "\\begin{bmatrix} a & b \\\\\n",
    "c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{B}\n",
    "=\n",
    "\\begin{bmatrix} e & f \\\\\n",
    "g & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then matrix $\\mathbf{A}$ will be tiled like:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & a & b & b \\\\\n",
    "a & a & b & b \\\\\n",
    "c & c & d & d \\\\\n",
    "c & c & d & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Matrix $\\mathbf{B}$ will be tiled like:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e & f & e & f \\\\\n",
    "g & h & g & h \\\\\n",
    "e & f & e & f \\\\\n",
    "g & h & g & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "... and the Kronecker product is the Hadamard (per-element) product of these tiled matrices:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae & af & be & bf \\\\\n",
    "ag & ah & bg & bh \\\\\n",
    "ce & cf & de & df \\\\\n",
    "cg & ch & dg & dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's compare this with the matrix product of these two matrices.  This is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + bg & af + bh \\\\\n",
    "ce + dg & cf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are 8 pairs of products in the matrix product above, and 16 in this Kronecker product, so it seems not obvious to relate the two, eg via vectorization, somehow?\n",
    "\n",
    "### Relationship between vectorization and inner product\n",
    "\n",
    "Wikipedia states that we can form a relationship between vectorization and the matrix product. For square, real matrices:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}\n",
    "(\\mathbf{A}^T \\mathbf{B})\n",
    "= \\mathrm{vec}(\\mathbf{A})^T \\mathrm{vec}(\\mathbf{B})\n",
    "$$\n",
    "\n",
    "\n",
    "Let's try this, for the example matrices above.\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{A})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "c \\\\\n",
    "b \\\\\n",
    "d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{B})$ is:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "e \\\\\n",
    "g \\\\\n",
    "f \\\\\n",
    "h \\\\\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So:\n",
    "\n",
    "$$\\mathrm{vec}^T(\\mathbf{A})\\mathrm{vec}(\\mathbf{B}) = ae + cg + bf + dh$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{A}^T$ is:\n",
    "$$\n",
    "\\begin{bmatrix} a & c \\\\\n",
    "b & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "...and $\\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} e & f \\\\\n",
    "g & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $\\mathbf{A}^T \\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + cg & af + ch \\\\\n",
    "be + dg & bf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{tr}$ is the sum of the diagonal, for a square matrix.  For example, for matrix $\\mathrm{X}$ of size $m$, $\\mathrm{tr}(\\mathbf{X})$ is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\mathbf{X}) = \\sum_{i=1}^m x_{i,i}\n",
    "$$\n",
    "\n",
    "So, $\\mathrm{tr}(\\mathbf{A}^T \\mathbf{B})$, in our example above, is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\mathbf{A}^T \\mathbf{B})\n",
    "= ae + cg + bf + dh\n",
    "$$\n",
    "\n",
    "... which matches the result for $\\mathrm{vec}(\\mathbf{A})^T \\mathrm{vec}(\\mathbf{B})$\n",
    "\n",
    "More generally, let's try for two $n$ x $n$ matrices $\\mathbf{A}$ and $\\mathbf{B}$.  The vectorizations will look like:\n",
    "\n",
    "$$\n",
    "a_{i,j} = \\mathrm{vec}(\\mathbf{A})_{jn + i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{k,l} = \\mathrm{vec}(\\mathbf{B})_{ln + k}\n",
    "$$\n",
    "\n",
    "To form the inner product of the vectorizations, let's use two nested sums.  The innermost sum will be over each row in a column, and the outermost will be over columns.  So this will give:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \n",
    "\\sum_{j=1}^{n}\n",
    "a_{j,i} b_{j,i}\n",
    "$$\n",
    "\n",
    "Meanwhile, $\\mathbf{A}^T$ is:\n",
    "\n",
    "$$\n",
    "(\\mathbf{A}^T)_{i,j} = a_{j,i}\n",
    "$$\n",
    "\n",
    "... and the matrix product $\\mathbf{A}^T\\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "(\\mathbf{A}^T\\mathbf{B})_{i,j} = \\sum_{k=1}^n a_{k,i} b_{k,j}\n",
    "$$\n",
    "\n",
    "The trace of this, is the sum over the diagonal, ie the sum of terms where $i = j$.  This gives:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\mathbf{A}^T\\mathbf{B}) = \\sum_{l=1}^n \\sum_{k=1}^n a_{k,l} b_{k,l}\n",
    "$$\n",
    "\n",
    "By inspection, this is identical to the expression for $\\mathrm{vec}(\\mathbf{A})^T \\mathrm{vec}(\\mathbf{B})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between vectorization and Kronecker product\n",
    "\n",
    "Wikipedia asserts that:\n",
    "\n",
    "$$\n",
    "\\mathrm{vec}(\\mathbf{A} \\mathbf{B})\n",
    "= (\\mathbf{I}_m \\otimes \\mathbf{A})\\mathrm{vec}(\\mathbf{B})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try for the example matrices above:\n",
    "\n",
    "$\\mathbf{I}_m \\otimes \\mathbf{A}$ is Hadamard product of:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b & a & b \\\\\n",
    "c & d & c & d \\\\\n",
    "a & b & a & b \\\\\n",
    "c & d & c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b & 0 & 0 \\\\\n",
    "c & d & 0 & 0 \\\\\n",
    "0 & 0 & a & b \\\\\n",
    "0 & 0 & c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{B})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e \\\\\n",
    "g \\\\\n",
    "f \\\\\n",
    "h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $(\\mathbf{I}_m \\otimes \\mathbf{A})\\mathrm{vec}(\\mathbf{B})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + bg \\\\\n",
    "ce + dg \\\\\n",
    "af + bh \\\\\n",
    "cf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Meanwhile, $\\mathbf{A} \\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + bg & af + bh \\\\\n",
    "ce + dg & cf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And vectorization of this is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + bg \\\\\n",
    "ce + dg \\\\\n",
    "af + bh \\\\\n",
    "cf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "... which matches the value in this case for $(\\mathbf{I}_m \\otimes \\mathbf{A})\\mathrm{vec}(\\mathbf{B})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between Kronecker product and vectorization, part 2\n",
    "\n",
    "More generally, we have:\n",
    "\n",
    "$$\n",
    "(\\mathbf{B}^T \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{X})\n",
    "= \\mathrm{vec}(\\mathbf{A} \\mathbf{X} \\mathbf{B})\n",
    "$$\n",
    "\n",
    "Let's try this for the example matrices $\\mathbf{A}$ and $\\mathbf{B}$ above, and a new example matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "i & j \\\\\n",
    "k & l \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, $\\mathbf{B}^T$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e & g \\\\\n",
    "f & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathbf{B}^T \\otimes \\mathbf{A}$ is Hadamard product, $\\odot$, of:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e & e & g & g \\\\\n",
    "e & e & g & g \\\\\n",
    "f & f & h & h \\\\\n",
    "f & f & h & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b & a & b\\\\\n",
    "c & d & c & d \\\\\n",
    "a & b & a & b\\\\\n",
    "c & d & c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ea & eb & ga & gb\\\\\n",
    "ec & ed & gc & gd \\\\\n",
    "fa & fb & ha & hb\\\\\n",
    "fc & fd & hc & hd \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{X})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "i \\\\\n",
    "k \\\\\n",
    "j \\\\\n",
    "l \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And so $(\\mathbf{B}^T \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{X})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "eai + ebk + gaj + gbl \\\\\n",
    "eci + edk + gcj + gdl \\\\\n",
    "fai + fbk + haj + hbl \\\\\n",
    "fci + fdk + hcj + hdl \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate $\\mathrm{vec}(\\mathbf{A}\\mathbf{X}\\mathbf{B})$:\n",
    "\n",
    "$\\mathbf{A}\\mathbf{X}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ai + bk & aj + bl \\\\\n",
    "ci + dk & cj + dl \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, $\\mathbf{A}\\mathbf{X}\\mathbf{B}$ is:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "aie + bke + ajg + blg & aif + bkf + ajh + blh \\\\\n",
    "cie + dke + cjg + dlg & cif + dkf + cjh + dlh \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "And then vectorization of this is:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "aie + bke + ajg + blg \\\\\n",
    "cie + dke + cjg + dlg \\\\\n",
    "aif + bkf + ajh + blh \\\\\n",
    "cif + dkf + cjh + dlh \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "... which matches the result for $(\\mathbf{B}^T \\otimes \\mathbf{A})\\mathrm{vec}(\\mathbf{X})$ above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that this means that:\n",
    "\n",
    "$$\\mathrm{vec}(AB) = (\\mathbf{B}^T \\otimes \\mathbf{A})\\mathrm{vec}(\\mathbf{I})$$\n",
    "\n",
    "Let's try, with the example matrices above.  $\\mathbf{B}^T \\otimes \\mathbf{A}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ea & eb & ga & gb\\\\\n",
    "ec & ed & gc & gd \\\\\n",
    "fa & fb & ha & hb\\\\\n",
    "fc & fd & hc & hd \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{I})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, forming the matrix product of these we have:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ea + gb \\\\\n",
    "ec + gd \\\\\n",
    "fa + hb \\\\\n",
    "fc + hd \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{A}\\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + bg & af + bh \\\\\n",
    "ce + dg & cf + dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "... and the vectorization of this matches $(\\mathbf{B}^T \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{I})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kronecker product, intuition\n",
    "\n",
    "The Kronecker product forms every possible pair of products between the elements of the two input matrices.  It doesnt add any of these terms though, unlike a matrix product.  To form a matrix product, we therefore need some way of:\n",
    "\n",
    "- filtering out the terms we want (eg for two 2x2 matrices, the Kronecker product has 16 pairs of products, but we only need 8 for the matrix product)\n",
    "- adding these\n",
    "\n",
    "By forming the matrix product with the vectorization of the identity matrix $\\mathbf{I}$, we can handle both of these requirements.\n",
    "\n",
    "But the Kronecker product is more general than this, since it contains every pair of products between the two input matrices.  Hence eg can be used to form the matrix product of three matrices, $\\mathbf{A}\\mathbf{X}\\mathbf{B}$, using only one single matrix product.  (The Kronecker product itself is a tiling operation followed by per-element multiply)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-normal distribution revisited\n",
    "\n",
    "Working through the Minka paper, \"Bayesian linear regression\", and starting at section 1, we have the following data model:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{e}$$\n",
    "\n",
    "$$\\mathbf{e} \\sim \\mathcal{N}(0, \\mathbf{V})$$\n",
    "\n",
    "$$p(\\mathbf{y} \\mid \\mathbf{x}, \\mathbf{A}, \\mathbf{V}) \\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x}, \\mathbf{V})$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{x}$ is an input vector of length $m$, ie $m$ features\n",
    "- $\\mathbf{A}$ is a coefficient matrix, or personally I'd normally call this the \"parameter\", or \"weight\", matrix\n",
    "- $\\mathbf{e}$ is Gaussian noise\n",
    "- $\\mathbf{y}$ is the output vector, of length $d$\n",
    "\n",
    "We are only concerned with the conditional model for $\\mathbf{y}$, $p(\\mathbf{y} \\mid \\mathbf{x})$.  The distribution of $\\mathbf{x}$ is not considered, and is irrelevant for the contents of Minka's paper.\n",
    "\n",
    "We have $N$ inpt vectors, $\\mathbf{x}_1,\\dots,\\mathbf{x}_N$ and target values $\\mathbf{y}_1,\\dots,\\mathbf{y}_N$ forming a data set of $N$ exchangeable data points $D = \\{(\\mathbf{y}_1,\\mathbf{x}_1),\\dots,(\\mathbf{y}_N,\\mathbf{x}_N)\\}$.  $\\mathbf{Y}$ is $[\\mathbf{y}_1 \\dots \\mathbf{y}_N]$, and $\\mathbf{X}$ is $[\\mathbf{x}_1 \\dots \\mathbf{x}_N]$.  The distribution of $\\mathbf{Y}$ given $\\mathbf{X}$ is:\n",
    "\n",
    "$$p(\\mathbf{Y} \\mid \\mathbf{X}, \\mathbf{A}, \\mathbf{V})\n",
    "= \\prod_{i=1}^N\n",
    "p(\\mathbf{y}_i \\mid \\mathbf{x}_i, \\mathbf{A}, \\mathbf{V})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\prod_{i=1}^N\n",
    "\\frac{1}{\\left| 2\\pi\\mathbf{V}\\right|^{1/2}}\n",
    "\\exp \\left(\n",
    "  -\\frac{1}{2}\n",
    "     (\\mathbf{y}_i - \\mathbf{A}\\mathbf{x}_i)^T\\mathbf{V}^{-1}(\\mathbf{y}_i - \\mathbf{A}\\mathbf{x}_i)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}{\\left| 2\\pi\\mathbf{V}\\right|^{N/2}}\n",
    "\\exp \\left(\n",
    "   - \\frac{1}{2}\n",
    "   \\sum_{i=1}^N(\\mathbf{y}_i - \\mathbf{A}\\mathbf{x}_i)^T \\mathbf{V}^{-1}(\\mathbf{y}_i - \\mathbf{A}\\mathbf{x}_i)\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minka then writes down the sum inside the exponential as a trace of matrix products:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\n",
    "  \\mathbf{V}^{-1}(\\mathbf{Y} - \\mathbf{A}\\mathbf{X})(\\mathbf{Y} - \\mathbf{A}\\mathbf{X})^T)\n",
    ")\n",
    "$$\n",
    "\n",
    "I can kind of see this is plausibly the same, but I think it might be educational to work through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace of a square matrix $\\mathbf{X}$ is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\mathbf{X})\n",
    "= \\sum_{i=1}^m x_{i,i}\n",
    "$$\n",
    "\n",
    "where $m$ is the size of each dimension of the square matrix $\\mathbf{X}$\n",
    "\n",
    "Let's say we have:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i^T \\mathbf{R} \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i$ is a vector, and $\\mathbf{R}$ is a matrix.\n",
    "\n",
    "And form the sum over $N$ values of $\\mathbf{x}_i$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\mathbf{x}_i^T \\mathbf{R}\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\mathbf{x}_i^T \\mathbf{R})_{j}\n",
    "=\n",
    "\\sum_{k=1}^m x_{i,k} r_{k,j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\mathbf{x}_i^T \\mathbf{R}\\mathbf{x}_i)\n",
    "=\n",
    "\\sum_{l=1}^m\n",
    "\\sum_{k=1}^m\n",
    "x_{i,k} r_{k,l} x_{i,l}\n",
    "$$\n",
    "\n",
    "(where here, $x_{i,j}$ means: the $j$th value of the vector $\\mathbf{x}_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, let's form the product $\\mathbf{X}^T \\mathbf{R} \\mathbf{X}$, and investigate the properties of this product:\n",
    "\n",
    "$$\n",
    "(\\mathbf{X}^T \\mathbf{R})_{i,j}\n",
    "=\n",
    "\\sum_{k=1}^m\n",
    "x_{k,i}\n",
    "r_{k,j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\mathbf{X}^T \\mathbf{R} \\mathbf{X})_{i,j}\n",
    "= \\sum_{l=1}^m\n",
    "\\sum_{k=1}^m\n",
    "x_{k,i}\n",
    "r_{k,l}\n",
    "x_{l,j}\n",
    "$$\n",
    "\n",
    "Let's obtain each component $j$ of the $\\mathrm{diag}$ of this result:\n",
    "\n",
    "$$\n",
    "\\mathrm{diag}(\n",
    "(\\mathbf{X}^T \\mathbf{R} \\mathbf{X}))_{j}\n",
    "= \\sum_{l=1}^m\n",
    "\\sum_{k=1}^m\n",
    "x_{k,j}\n",
    "x_{l,j}\n",
    "r_{k,l}\n",
    "$$\n",
    "\n",
    "By inspection, each component of $j$ of the diagonal corresponds to each value of $\\mathbf{x}_i^T\\mathbf{R}\\mathbf{x}_i$, for $i = j$, if $\\mathbf{x}_i$ are columns of $\\mathbf{X}$.\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\\sum_i \\mathbf{x}_i^T\\mathbf{R}\\mathbf{x}_i = \\mathrm{tr}(\\mathbf{X}^T\\mathbf{R}\\mathbf{X})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the original expression, which was:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\left| 2\\pi\\mathbf{V}\\right|^{N/2}}\n",
    "\\exp \\left(\n",
    "   - \\frac{1}{2}\n",
    "   \\sum_i(\\mathbf{y}_i - \\mathbf{A}\\mathbf{x}_i)^T \\mathbf{V}^{-1}(\\mathbf{y}_i - \\mathbf{A}\\mathbf{x}_i)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can thus be written as:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\left| 2\\pi\\mathbf{V}\\right|^{N/2}}\n",
    "\\exp \\left(\n",
    "   - \\frac{1}{2}\n",
    "   \\mathrm{tr}\\left(\n",
    "      (\\mathbf{Y} - \\mathbf{A}\\mathbf{X})^T \\mathbf{V}^{-1}(\\mathbf{Y} - \\mathbf{A}\\mathbf{X})\n",
    "   \\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Since the trace is invariant under cyclic permutations, ie $\\mathrm{tr}(ABC)  = \\mathrm{tr}(BCA) = \\mathrm{tr}(CAB)$, so we can write this as:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}{\\left| 2\\pi\\mathbf{V}\\right|^{N/2}}\n",
    "\\exp \\left(\n",
    "   - \\frac{1}{2}\n",
    "   \\mathrm{tr}\\left(\n",
    "      \\mathbf{V}^{-1}\n",
    "      (\\mathbf{Y} - \\mathbf{A}\\mathbf{X})\n",
    "      (\\mathbf{Y} - \\mathbf{A}\\mathbf{X})^T\n",
    "   \\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}{\\left| 2\\pi\\mathbf{V}\\right|^{N/2}}\n",
    "\\exp \\left(\n",
    "   - \\frac{1}{2}\n",
    "   \\mathrm{tr}\\left(\n",
    "      \\mathbf{V}^{-1}\n",
    "      (\\mathbf{Y} - \\mathbf{A}\\mathbf{X})\n",
    "      (\\mathbf{Y}^T - \\mathbf{X}^T\\mathbf{A}^T)\n",
    "   \\right)\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying this out, we get:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}{\\left| 2\\pi\\mathbf{V}\\right|^{N/2}}\n",
    "\\exp \\left(\n",
    "   - \\frac{1}{2}\n",
    "   \\mathrm{tr}\\left(\n",
    "      \\mathbf{V}^{-1}\n",
    "      \\left(\n",
    "         \\mathbf{A}\\mathbf{X}\\mathbf{X}^T\\mathbf{A}^T\n",
    "         -\\mathbf{A}\\mathbf{X}\\mathbf{Y}^T\n",
    "         -\\mathbf{Y}\\mathbf{X}^T\\mathbf{A}^T\n",
    "         + \\mathbf{Y}\\mathbf{Y}^T\n",
    "      \\right)\n",
    "   \\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}{\\left| 2\\pi\\mathbf{V}\\right|^{N/2}}\n",
    "\\exp \\left(\n",
    "   - \\frac{1}{2}\n",
    "   \\mathrm{tr}\\left(\n",
    "      \\mathbf{V}^{-1}\n",
    "      \\left(\n",
    "         \\mathbf{A}\\mathbf{X}\\mathbf{X}^T\\mathbf{A}^T\n",
    "         -2\\mathbf{Y}\\mathbf{X}^T\\mathbf{A}^T\n",
    "         + \\mathbf{Y}\\mathbf{Y}^T\n",
    "      \\right)\n",
    "   \\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "... which matches the end of section 1 of the Minka paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-normal distribution\n",
    "\n",
    "Moving to section 2 of the Minka paper, he writes down the definition of a matrix-normal distribution.  A random $d$ by $m$ matrix $\\mathbf{A}$ is matrix-normal distributed with parameters $\\mathbf{M}$, $\\mathbf{V}$ and $\\mathbf{K}$ if the density of $\\mathbf{A}$ is:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{A}) \\sim \\mathcal{N}(\\mathbf{M}, \\mathbf{V}, mathbf{K})\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{\\left| \\mathbf{K} \\right|^{d/2}}\n",
    "  { \\left|2\\pi \\mathbf{V} \\right|^{m/2}}\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\mathrm{tr}\n",
    "  \\left(\n",
    "     (\\mathbf{A} - \\mathbf{M})^T\n",
    "     \\mathbf{V}^{-1}\n",
    "     (\\mathbf{A} - \\mathbf{M})\n",
    "     \\mathbf{K}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{A}$ is $d$ by $m$ (so $m$ is like our $N$, and $d$ is like our $K$; $\\mathbf{A}$ has $m$ columns, each of $d$ features)\n",
    "- $\\mathbf{M}$ is $d$ by $m$ too (it I guess represents the mean *of each datapoint*)\n",
    "- $\\mathbf{V}$ is $d$ by $d$ (so, like our $K$ by $K$, seems to plausibly represent our covariance matrix?)\n",
    "- $\\mathbf{K}$ is $m$ by $m$ (like our $N$ by $N$; plausibly represents covariance between data points; for us this should be $\\mathbf{I}$ I guess?)\n",
    "\n",
    "Interestingly, $M$ has as many columns as there are datapoints, so it looks like the mean is per data-point.  This means if the mean of multiple datapoints should be shared, then the values of that mean should be replicated to multiple locations in $\\mathbf{M}$, I guess?  It seems like this might be useful to us, if we are going to have a mean that is conditional on a latent class? Or even a mixture of classes/features, which could mean that the mean really is different for each data point?\n",
    "\n",
    "Minka states that if $\\mathbf{K}$ is diagonal, that means the columns of $\\mathbf{A}$ are independent normal vectors.  That makes sense, since it is $m$ by $m$, and plausibly represents the covariance between the data points.   I think that $\\mathbf{K}$ will be the identity matrix for us, meaning that each data point is drawn $i.i.d.$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minka then states that if we vectorize $\\mathbf{A}$, stack each column one on top of the other, then the resulting vector will be distributed as:\n",
    "\n",
    "$$\n",
    "p(\\mathrm{vec}(\\mathbf{A}))\n",
    "= \\mathcal{N}\n",
    "(\\mathrm{vec}(\\mathbf{M}),\n",
    "\\mathbf{K}^{-1} \\otimes \\mathbf{V}\n",
    ")\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, this kind of makes sense.  $\\mathbf{M}$ is the mean per data point.  If we vectorize it, we now have the means stacked one on top of the other.  If $\\mathbf{M}$ looks like:\n",
    "\n",
    "$$\n",
    "M = [\\mathbf{m}_1 \\mathbf{m}_2 \\dots \\mathbf{m}_m]$$\n",
    "\n",
    "where unfortunately we have $\\mathbf{m}_m$ is the $m$th column vector of $\\mathbf{M}$ matrix, which conflicting notation I should probably fix, but I think it's kind of manageable for now...\n",
    "\n",
    "Then, vectorizing this, we'll have:\n",
    "\n",
    "$$\n",
    "\\mathrm{vec}(\\mathbf{M})\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{m}_1 \\\\\n",
    "\\mathbf{m}_2 \\\\\n",
    "\\dots \\\\\n",
    "\\mathbf{m}_m \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "... where $\\mathbf{m}_i$ are column vectors.\n",
    "\n",
    "Then we're going to draw from a Gaussian with this mean.  The covariance for the Gaussian should have the following characteristics:\n",
    "\n",
    "- each individual point should be independent, meaning each block of $d$ (our $K$) rows of the input/mean should be independent, have no covariance\n",
    "- each feature within each datapoint should have a covariance matching our desired original covariance, which in this case I think is $\\mathbf{V}$.\n",
    "\n",
    "Then, the Kronecker product $\\mathbf{K}^{-1} \\otimes \\mathbf{V}$ is going to tile $\\mathbf{V}$ down the diagonal of the resulting matrix, assuming that $\\mathbf{K}$ is the identity matrix $\\mathbf{I}$.  The resulting matrix will look something like, where $\\mathbf{V}$ is the matrix $\\mathbf{V}$, and $\\mathbf{0}$ is the empty matrix, filled with $0$s:\n",
    "\n",
    "$$\n",
    "\\mathbf{K}^{-1} \\otimes \\mathbf{V}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\mathbf{I} \\otimes \\mathbf{V}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{V} & \\mathbf{0} & \\dots & \\mathbf{0} \\\\\n",
    "\\mathbf{0} & \\mathbf{V} & \\dots & \\mathbf{0} \\\\\n",
    "\\dots & \\dots & \\dots & \\dots \\\\\n",
    "\\mathbf{0} & \\mathbf{0} & \\dots & \\mathbf{V} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Basically, the resulting matrix is block diagonal.  The number of blocks of $\\mathbf{V}$ along each dimension will be $m$, corresponding to our $N$.\n",
    "\n",
    "The covariance matrix corresponds to the requirements listed earlier:\n",
    "\n",
    "- features within each block of $d$ features are correlated, according to the covariance matrix $\\mathbf{V}$\n",
    "- features between blocks of $d$ features are independent, drawn i.i.d, corresponding to our data point being independently drawn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviating somewhat from Minka's paper, given the equivalence of a matrix-normal distribution as a multivariate distribution, this means we can write down the integral of a matrix-normal distribution.  We have, for a multivariate Gaussian:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}\n",
    "\\exp \\left(\n",
    "   -\\frac{1}{2}\n",
    "   (\\mathbf{x}^T \\mathbf{V}^{-1} \\mathbf{x})\n",
    "\\right)\n",
    "\\,\n",
    "d\\mathbf{x}\n",
    "=\n",
    "\\sqrt{\\frac{(2\\pi)^d}{\\mathrm{det}\\,\\mathbf{V}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in our case we have the matrix-normal distribution:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}\n",
    "\\exp \\left(\n",
    "   -\\frac{1}{2}\n",
    "   (\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X})\n",
    "\\right)\n",
    "\\,\n",
    "d\\mathbf{x}\n",
    "$$\n",
    "\n",
    "But we should normalize it really.\n",
    "\n",
    "Oh.... Heh!  .... a matrix-normal distribution is a probability distribution ... it integrates to 1 :-P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, actually ~~most~~ all of the above is kind of superfluous to knowing the integral of a matrix-normal distribution, since:\n",
    "\n",
    "- the integral over a matrix-normal distribution is simply $1$, as for any other probability distribution :-), and\n",
    "- the integral of an un-normalized exponential expression in the form of a matrix-normal, is given by the normalization constant at the start of the matrix-normal expression, right at the start of all of this :-P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating expressions having the form of a matrix-normal distribution\n",
    "\n",
    "So:\n",
    "\n",
    "a matrix-normal distribution has the form:\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{A}; \\mathbf{M}, \\mathbf{V}, \\mathbf{K})\n",
    "=\n",
    "\\frac\n",
    "{\\left| \\mathbf{K} \\right|^{d/2}}\n",
    "{\\left| 2\\pi \\mathbf{V} \\right|^{m/2} }\n",
    "\\exp\n",
    "\\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    (\\mathbf{A} - \\mathbf{M})^T\n",
    "    \\mathbf{V}^{-1}\n",
    "    (\\mathbf{A} - \\mathbf{M})\n",
    "    \\mathbf{K}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "It integrates necessarily to 1, being a probability distribution.  Therefore we can write down directly, based on this, that:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}\n",
    "\\exp\n",
    "\\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    (\\mathbf{A} - \\mathbf{M})^T\n",
    "    \\mathbf{V}^{-1}\n",
    "    (\\mathbf{A} - \\mathbf{M})\n",
    "    \\mathbf{K}\n",
    "  \\right)\n",
    "\\right)\n",
    "\\,d\\mathbf{A}\n",
    "= \n",
    "\\frac\n",
    "{\\left| 2\\pi \\mathbf{V} \\right|^{m/2} }\n",
    "{\\left| \\mathbf{K} \\right|^{d/2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to finite linear-Gaussian model :-)\n",
    "\n",
    "We had got as far as writing down the joint probability distribution over $\\mathbf{X}$ and $\\mathbf{A}$:\n",
    "\n",
    "$$\n",
    "\\propto\n",
    "\\exp\n",
    "\\left(\n",
    "- \\frac{1}{2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{X}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{X}^T \\mathbf{Z} \\mathbf{A}\n",
    "    + \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{Z}\\mathbf{A}\n",
    "    + \\frac{1}{\\sigma_A^2} \\mathbf{A}^T \\mathbf{A}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{X}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{X}^T \\mathbf{Z} \\mathbf{A}\n",
    "    + \\mathbf{A}^T(\\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z} + \\frac{1}{\\sigma_A^2}\\mathbf{I})\\mathbf{A}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{X}^T\\mathbf{X}\n",
    "    - (\\frac{2}{\\sigma_X^2}\\mathbf{X}^T \\mathbf{Z})\\mathbf{A}\n",
    "    + \\mathbf{A}^T(\\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z} + \\frac{1}{\\sigma_A^2}\\mathbf{I})\\mathbf{A}\n",
    "  \\right)\n",
    "\\right)\n",
    "\\tag{4.1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial then completes the square for $\\mathbf{A}$ which I supposed/suppose is because we want to integrate over it shortly?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the square\n",
    "\n",
    "To complete the square, let's examine what happens if we multiply an expression like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(\\mathbf{A}\\mathbf{X} - \\mathbf{B}\\mathbf{Y})^T\\mathbf{S}(\\mathbf{A}\\mathbf{X} - \\mathbf{B}\\mathbf{Y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= (\\mathbf{X}^T\\mathbf{A}^T\\mathbf{S} - \\mathbf{Y}^T\\mathbf{B}^T\\mathbf{S})(\\mathbf{A}\\mathbf{X} - \\mathbf{B}\\mathbf{Y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\\mathbf{X}^T\\mathbf{A}^T\\mathbf{S}\\mathbf{A}\\mathbf{X}\n",
    "-\\mathbf{Y}^T\\mathbf{B}^T\\mathbf{S}\\mathbf{A}\\mathbf{X}\n",
    "-\\mathbf{X}^T\\mathbf{A}^T\\mathbf{S}\\mathbf{B}\\mathbf{Y}\n",
    "+\\mathbf{Y}^T\\mathbf{B}^T\\mathbf{S}\\mathbf{B}\\mathbf{Y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\mathbf{X}^T\\mathbf{A}^T\\mathbf{S}\\mathbf{A}\\mathbf{X}\n",
    "-2\\mathbf{X}^T\\mathbf{A}^T\\mathbf{S}\\mathbf{B}\\mathbf{Y}\n",
    "+ \\mathbf{Y}^T\\mathbf{B}^T\\mathbf{S}\\mathbf{B}\\mathbf{Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, the constant term on the left is not a constraint we need to fit, since we will complete the square, but we should create an expression whose resulting terms in $\\mathbf{Y}$ and $\\mathbf{Y}^T\\dots\\mathbf{Y}$, which in our case are expressions in $\\mathbf{A}$, matches the corresponding terms inside equation (4.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $(\\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z} + \\frac{1}{\\sigma_A^2}\\mathbf{I})$ is not obviously decomposable into the product of a matrix and its transpose, so this should probably be in the $\\mathbf{S}$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then have $\\mathbf{B}$ as the identity matrix, and $\\mathbf{X}^T\\mathbf{A}^T$ together can be:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma_X^2} \\mathbf{X}^T\\mathbf{Z} \\left( \\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z} + \\frac{1}{\\sigma_A^2}\\mathbf{I} \\right)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $\\mathbf{A}\\mathbf{X}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma_X^2}\n",
    "\\left(\n",
    "  \\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z} + \\frac{1}{\\sigma_A^2}\\mathbf{I}\n",
    "\\right)^{-1^T}\n",
    "\\mathbf{Z}^T\\mathbf{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: Gramian matrix is symmetric\n",
    "\n",
    "Let's suppose we have:\n",
    "\n",
    "$$\n",
    "\\mathbf{B} = \\mathbf{A}^T\\mathbf{A}\n",
    "$$\n",
    "\n",
    "(where $A$ is $m$ x $n$)\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "b_{i,j} = \\sum_{k=1}^m a_{k,i} a_{k,j}\n",
    "$$\n",
    "\n",
    "Conversely:\n",
    "\n",
    "$$\n",
    "(\\mathbf{B}^T)_{i,j} = b_{j,i} = \\sum_{k=1}^m a_{k,j} a_{k,i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{k=1}^m a_{k,i} a_{k,j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= b_{i,j}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\\mathbf{A}^T\\mathbf{A} = (\\mathbf{A}^T\\mathbf{A})^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expression for joint probability of $\\mathbf{X}$ and $\\mathbf{A}$ continued\n",
    "\n",
    "Therefore, for our $\\mathbf{A}\\mathbf{X}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{X}\n",
    "=\n",
    "\\frac{1}{\\sigma_X^2}\n",
    "\\left(\n",
    "  \\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z} + \\frac{1}{\\sigma_A^2}\\mathbf{I}\n",
    "\\right)^{-1^T}\n",
    "\\mathbf{Z}^T\\mathbf{X}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \n",
    "\\frac{1}{\\sigma_X^2}\n",
    "\\left(\n",
    "  \\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z} + \\frac{1}{\\sigma_A^2}\\mathbf{I}\n",
    "\\right)^{-1}\n",
    "\\mathbf{Z}^T\\mathbf{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define:\n",
    "\n",
    "$$\\mathbf{S} =\n",
    "\\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z}\n",
    "    +\n",
    "    \\frac{1}{\\sigma_A^2}\\mathbf{I}\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so the expression inside (4.1) becomes:\n",
    "\n",
    "$$\n",
    "\\frac{1}\n",
    "  {\\sigma_X^2}\n",
    "  \\mathbf{X}^T\\mathbf{X}\n",
    "- \\frac{1}{\\sigma_X^2}\\mathbf{X}^T\\mathbf{Z}\\mathbf{S}^{-1}\n",
    "  \\mathbf{S}\n",
    "  \\frac{1}{\\sigma_X^2}\\mathbf{X}\n",
    "  \\mathbf{S}^{-1} \\mathbf{Z}^T\\mathbf{X}\n",
    "+ \\left( \\frac{1}{\\sigma_X^2}\\mathbf{S}^{-1}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} \\right)^T\n",
    "\\mathbf{S}\n",
    "\\left( \\frac{1}{\\sigma_X^2}\\mathbf{S}^{-1}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}\n",
    "  {\\sigma_X^2}\n",
    "  \\mathbf{X}^T\\mathbf{X}\n",
    "- \\frac{1}{\\sigma_X^4}\\mathbf{X}^T\\mathbf{Z}\n",
    "  \\mathbf{X}\n",
    "  \\mathbf{S}^{-1} \\mathbf{Z}^T\\mathbf{X}\n",
    "+ \\left( \\frac{1}{\\sigma_X^2}\\mathbf{S}^{-1}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} \\right)^T\n",
    "\\mathbf{S}\n",
    "\\left( \\frac{1}{\\sigma_X^2}\\mathbf{S}^{-1}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial then defines:\n",
    "\n",
    "$$\n",
    "\\mathbf{M}\n",
    "= \\frac{ \\mathbf{S}^{-1}}{\\sigma_X^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives:\n",
    "\n",
    "$$\n",
    "\\frac{1}\n",
    "  {\\sigma_X^2}\n",
    "  \\mathbf{X}^T\\mathbf{X}\n",
    "- \\frac{1}{\\sigma_X^2}\\mathbf{X}^T\\mathbf{Z}\n",
    "  \\mathbf{X}\n",
    "  \\mathbf{M} \\mathbf{Z}^T\\mathbf{X}\n",
    "+ \\left( \\mathbf{M}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} \\right)^T\n",
    "(\\sigma_X^2\\mathbf{M})^{-1}\n",
    "\\left( \\mathbf{M}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}\n",
    "  {\\sigma_X^2}\n",
    "  \\mathbf{X}^T\n",
    "  \\left(\n",
    "     \\mathbf{I}\n",
    "    - \\mathbf{Z}\\mathbf{M} \\mathbf{Z}^T\n",
    "  \\right)\n",
    "  \\mathbf{X}\n",
    "+ \\left( \\mathbf{M}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} \\right)^T\n",
    "(\\sigma_X^2\\mathbf{M})^{-1}\n",
    "\\left( \\mathbf{M}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which matches the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginalize over $\\mathbf{A}$\n",
    "\n",
    "We want to marginalize over $\\mathbf{A}$, to obtain:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)\n",
    "$$\n",
    "\n",
    "This is calculated as:\n",
    "\n",
    "$$\n",
    "\\int\n",
    "p(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A}, \\sigma_X)\n",
    "p(\\mathbf{A} \\mid \\sigma_A) \\, d\\mathbf{A}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=\n",
    "\\frac{1}\n",
    "  {(2\\pi \\sigma_X^2)^{ND/2}}\n",
    "  \\frac{1}\n",
    "  {(2\\pi \\sigma_A^2)^{KD/2}}\n",
    "\\int\n",
    "\\exp \\left(\n",
    "- \\frac{1}{2}\n",
    "\\mathrm{tr} \\left(\n",
    "      \\frac{1}\n",
    "      {\\sigma_X^2}\n",
    "      \\mathbf{X}^T\n",
    "      \\left(\n",
    "         \\mathbf{I}\n",
    "        - \\mathbf{Z}\\mathbf{M} \\mathbf{Z}^T\n",
    "      \\right)\n",
    "      \\mathbf{X}\n",
    "    + \\left( \\mathbf{M}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} \\right)^T\n",
    "    (\\sigma_X^2\\mathbf{M})^{-1}\n",
    "    \\left( \\mathbf{M}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} \\right)\n",
    "   \\right)\n",
    "\\right)\n",
    "\\,\n",
    "d\\mathbf{A}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and it's becoming obvious now why $\\mathbf{M}$ is defined as it is, and why we completed the square for $\\mathbf{A}$.  To compute the integral over the matrix Gaussian form, we need ideally $\\mathbf{A}$ to not have any coefficients etc, and we need ideally the variance term, in this case $\\sigma_X^2\\mathbf{M}$, to be factorized out, in the way that it now is.  We can factorize the exponential into two parts, moving the term that is independent of $\\mathbf{A}$ to outside of the integral:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=\n",
    "\\frac{1}\n",
    "  {(2\\pi \\sigma_X^2)^{ND/2}}\n",
    "  \\frac{1}\n",
    "  {(2\\pi \\sigma_A^2)^{KD/2}}\n",
    "\\exp \\left(\n",
    "    - \\frac{1}{2}\n",
    "    \\mathrm{tr} \\left(\n",
    "      \\frac{1}{\\sigma_X^2}\n",
    "      \\mathbf{X}^T\n",
    "      (\\mathbf{I} - \\mathbf{Z}\\mathbf{M} \\mathbf{Z}^T )\n",
    "      \\mathbf{X}\n",
    "   \\right)\n",
    "\\right)\n",
    "\\int\n",
    "\\exp \\left(\n",
    "   - \\frac{1}{2}\n",
    "   \\mathrm{tr} \\left(\n",
    "        ( \\mathbf{M}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} )^T\n",
    "        (\\sigma_X^2\\mathbf{M})^{-1}\n",
    "        ( \\mathbf{M}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A} )\n",
    "   \\right)\n",
    "\\right)\n",
    "\\,\n",
    "d\\mathbf{A}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the matrix-normal form, the integral is just the inverse of normalizing constant of a matrix-normal distribution ie, for the notation used in the Minka paper:\n",
    "\n",
    "$$\n",
    "\\frac\n",
    "  {\\left| 2\\pi \\mathbf{V} \\right|^{m/2}}\n",
    "  {\\left| \\mathbf{K} \\right|^{d/2}}\n",
    "$$\n",
    "\n",
    "In our case, for the marginalize integral above, $\\mathbf{K}$ is $\\mathbf{I}$, and $\\mathbf{V}$ is $\\sigma_X^2\\mathbf{M}$ $m$ is the number of columns of $\\mathbf{A}$, which is $D$.  So the integral is:\n",
    "\n",
    "$$\n",
    "\\frac\n",
    "  {\\left| 2\\pi \\mathbf{\\sigma_X^2\\mathbf{M}} \\right|^{D/2}}\n",
    "  {1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the marginalized form becomes:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{\\left| 2\\pi \\mathbf{\\sigma_X^2\\mathbf{M}} \\right|^{D/2}}\n",
    "  { (2\\pi \\sigma_X^2)^{ND/2}  (2\\pi \\sigma_A^2)^{KD/2}}\n",
    "\\exp \\left(\n",
    "    - \\frac{1}{2}\n",
    "    \\mathrm{tr} \\left(\n",
    "      \\frac{1}{\\sigma_X^2}\n",
    "      \\mathbf{X}^T\n",
    "      (\\mathbf{I} - \\mathbf{Z}\\mathbf{M} \\mathbf{Z}^T )\n",
    "      \\mathbf{X}\n",
    "   \\right)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the normalization term:\n",
    "\n",
    "$$\n",
    "\\frac{\\left| 2\\pi \\mathbf{\\sigma_X^2\\mathbf{M}} \\right|^{D/2}}\n",
    "  { (2\\pi \\sigma_X^2)^{ND/2}  (2\\pi \\sigma_A^2)^{KD/2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: scalar multiple of a matrix determinant\n",
    "\n",
    "So, at this point, I realized that Minka's normalization constnat has power of $d/2$, using his notation, whereas the Gaussian for $\\mathbf{X}$ in the tutorial has $ND/2$.   So, let's look at what's happening:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the general matrix-normal distribution, from Minka's report, the normalization constant is:\n",
    "\n",
    "$$\n",
    "\\frac\n",
    "   {\\left|\\mathbf{K}\\right|^{N/2}}\n",
    "   {\\left| 2\\pi \\mathbf{V}\\right|^{D/2}}\n",
    "$$\n",
    "$$\n",
    "= \n",
    "\\frac\n",
    "   {\\left|\\mathbf{K}\\right|^{N/2}}\n",
    "   {(2\\pi)^{ND/2} \\left| \\mathbf{V}\\right|^{D/2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here:\n",
    "\n",
    "$$\\mathbf{K} = \\mathbf{I}$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\\mathbf{V} = \\sigma_X^2 \\mathbf{I}$$\n",
    "\n",
    "For a determinant, we have:\n",
    "\n",
    "$$|c\\mathbf{A}| = c^n|\\mathbf{A}|$$\n",
    "\n",
    "...where $\\mathbf{A}$ is a square matrix of dimension $n$ x $n$\n",
    "\n",
    "I found this from Wikipedia, https://en.wikipedia.org/wiki/Determinant , but we can easily intuit on this:\n",
    "\n",
    "- a determinant can be written as a linear of each value in the first row of the matrix, multiplied by the determinant of a minor\n",
    "- multiplying the matrix by a scalar will increase the magnitude of the values of the first row by this scalar, thus multiply the determinant by this scalar\n",
    "- however, it will also have a similar effect on the determinant of each minor\n",
    "- by induction, the magnitude of the determinant will be increased by $c^n$, as above\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\left|\\mathbf{V}\\right| = (\\sigma_X^2)^N\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And:\n",
    "\n",
    "$$\n",
    "\\frac\n",
    "  {\\left| \\mathbf{K} \\right|^{N/2}}\n",
    "  {\\left| 2\\pi \\mathbf{V} \\right|^{D/2}}\n",
    "=\n",
    "\\frac\n",
    "  {1}\n",
    "  {(2\\pi\\sigma_X^2)^{ND/2}}\n",
    "$$\n",
    "\n",
    "... which matches the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant of normalization, for marginalization over $\\mathbf{A}$\n",
    "\n",
    "So, going back to the marginalized probability, the constant of normalization so far is:\n",
    "\n",
    "$$\n",
    "\\frac{\\left| 2\\pi \\mathbf{\\sigma_X^2\\mathbf{M}} \\right|^{D/2}}\n",
    "  { (2\\pi \\sigma_X^2)^{ND/2}  (2\\pi \\sigma_A^2)^{KD/2}}\n",
    "$$\n",
    "\n",
    "$\\mathbf{M}$ is:\n",
    "\n",
    "$$\\mathbf{M} = \\frac{\\mathbf{S}^{-1}}{\\sigma_X^2}$$\n",
    "\n",
    "$$= \\frac{{\\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z}\n",
    "    +\n",
    "    \\frac{1}{\\sigma_A^2}\\mathbf{I}\n",
    "\\right)\n",
    "}^{-1}}{\\sigma_X^2}$$\n",
    "\n",
    "For a determinant, we have:\n",
    "\n",
    "$$|\\mathbf{A}^{-1}| = \\frac{1}{|\\mathbf{A}|}$$\n",
    "\n",
    "(Where $\\mathbf{A}$ is an invertable matrix)\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "|\\mathbf{M}| = \\left| \\frac{{\\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z}\n",
    "    +\n",
    "    \\frac{1}{\\sigma_A^2}\\mathbf{I}\n",
    "\\right)\n",
    "}^{-1}}{\\sigma_X^2}\n",
    "\\right|\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\left| \\frac\n",
    "  {1}\n",
    "  {\\sigma_X^2 \\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z}\n",
    "    +\n",
    "    \\frac{1}{\\sigma_A^2}\\mathbf{I}\n",
    "  \\right)\n",
    "}\n",
    "\\right|\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    " \\frac\n",
    "  {1}\n",
    "  {\\left|\n",
    "    \\mathbf{Z}^T\\mathbf{Z}\n",
    "    +\n",
    "    \\frac{\\sigma_X^2 }{\\sigma_A^2}\\mathbf{I}\n",
    "  \\right|\n",
    "}\n",
    "$$\n",
    "\n",
    "We will substitute in the value of $|\\mathbf{M}|$ into the expression for the normalization constant.  Wd note that $\\mathbf{Z}$ is $N$ x $K$, and therefore $\\mathbf{Z}^T\\mathbf{Z}$ is $K$ x $K$.  So, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{ (2\\pi \\sigma_X^2)^{KD/2} }\n",
    "  { (2\\pi)^{\\frac{ND + KD}{2}} ( \\sigma_X^2)^{ND/2}  ( \\sigma_A^2)^{KD/2}\n",
    "    \\left|\n",
    "    \\mathbf{Z}^T\\mathbf{Z}\n",
    "    +\n",
    "    \\frac{\\sigma_X^2 }{\\sigma_A^2}\\mathbf{I}\n",
    "    \\right|^{D/2}\n",
    "  }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "=\n",
    "\\frac{ 1 }\n",
    "  { (2\\pi)^{\\frac{ND}{2}} ( \\sigma_X^2)^{ND/2 - KD/2}  ( \\sigma_A^2)^{KD/2}\n",
    "    \\left|\n",
    "    \\mathbf{Z}^T\\mathbf{Z}\n",
    "    +\n",
    "    \\frac{\\sigma_X^2 }{\\sigma_A^2}\\mathbf{I}\n",
    "    \\right|^{D/2}\n",
    "  }\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{ 1 }\n",
    "  { (2\\pi)^{\\frac{ND}{2}} \\sigma_X^{(N - K)D}  \\sigma_A^{KD}\n",
    "    \\left|\n",
    "    \\mathbf{Z}^T\\mathbf{Z}\n",
    "    +\n",
    "    \\frac{\\sigma_X^2 }{\\sigma_A^2}\\mathbf{I}\n",
    "    \\right|^{D/2}\n",
    "  }\n",
    "$$\n",
    "\n",
    "... which matches the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential trace expression for marginalization over $\\mathbf{A}$\n",
    "\n",
    "As far as the expression inside the exponential trace, we have:\n",
    "\n",
    "$$\n",
    "      \\frac{1}{\\sigma_X^2}\n",
    "      \\mathbf{X}^T\n",
    "      (\\mathbf{I} - \\mathbf{Z}\\mathbf{M} \\mathbf{Z}^T )\n",
    "      \\mathbf{X}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And:\n",
    "\n",
    "$$\\mathbf{M} = \\frac{{\\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z}\n",
    "    +\n",
    "    \\frac{1}{\\sigma_A^2}\\mathbf{I}\n",
    "\\right)\n",
    "}^{-1}}{\\sigma_X^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a matrix inverse we have:\n",
    "\n",
    "$$(c\\mathbf{A})^{-1} = \\frac{1}{c}\\mathbf{A}^{-1}$$\n",
    "\n",
    "... for scalar $c$, and invertible matrix $\\mathbf{A}$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\mathbf{M} =\n",
    "\\left(\n",
    "    \\mathbf{Z}^T\\mathbf{Z}\n",
    "    +\n",
    "    \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}\n",
    "\\right)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, substituting into the exponential trace expression, we get:\n",
    "\n",
    "$$\n",
    "      \\frac{1}{\\sigma_X^2}\n",
    "      \\mathbf{X}^T\n",
    "      (\\mathbf{I} - \\mathbf{Z}\\mathbf{M} \\mathbf{Z}^T )\n",
    "      \\mathbf{X}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \n",
    "      \\frac{1}{\\sigma_X^2}\n",
    "      \\mathbf{X}^T\n",
    "      (\\mathbf{I} - \\mathbf{Z}\\left(\n",
    "    \\mathbf{Z}^T\\mathbf{Z}\n",
    "    +\n",
    "    \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}\n",
    "\\right)^{-1} \\mathbf{Z}^T )\n",
    "      \\mathbf{X}\n",
    "$$\n",
    "\n",
    "... as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional expression for one latent element\n",
    "\n",
    "For Gibbs sampling, we need the conditional probability of one latent element $z_{i,k}$, conditional on the other latent elements, and also on the observed data values.\n",
    "\n",
    "ie, we need:\n",
    "\n",
    "$$\n",
    "P(z_{i,k} \\mid \\mathbf{X}, \\mathbf{Z}_{-(i,k)}, \\sigma_X, \\sigma_A)\n",
    "$$\n",
    "\n",
    "Note that since we are marginalizing over $\\mathbf{A}$, there is no $\\mathbf{A}$ in this expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Bayes Rules, the conditional probability for $z_{i,k}$ is:\n",
    "\n",
    "$$\n",
    "P(z_{i,k} | \\mathbf{X}, \\mathbf{Z}_{-(i,k)}, \\sigma_X, \\sigma_A)\n",
    "=\n",
    "\\frac\n",
    "  {P(\\mathbf{Z}_{-(i,k)} \\mid z_{i,k}, \\mathbf{X}, \\sigma_X, \\sigma_A)\\,P(z_{i,k} \\mid \\mathbf{X}, \\sigma_X, \\sigma_A)}\n",
    "  {P(\\mathbf{Z}_{-(i,k)} \\mid \\mathbf{X}, \\sigma_X, \\sigma_A)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac\n",
    "  {P(\\mathbf{Z}_{-(i,k)} \\mid z_{i,k}, \\mathbf{X}, \\sigma_X, \\sigma_A)\\,P(z_{i,k})}\n",
    "  {P(\\mathbf{Z}_{-(i,k)}}\n",
    "$$\n",
    "\n",
    "or, we can try slightly differently:\n",
    "\n",
    "$$\n",
    "P(z_{i,k} | \\mathbf{X}, \\mathbf{Z}_{-(i,k)}, \\sigma_X, \\sigma_A)\n",
    "=\n",
    "\\frac\n",
    "  {P(\\mathbf{X} \\mid z_{i,k}, \\mathbf{Z}_{-(i,k)}, \\mathbf{X}, \\sigma_X, \\sigma_A)\\,P(z_{i,k} \\mid \\mathbf{Z}_{-(i,k)},\\sigma_X, \\sigma_A)}\n",
    "  {P(\\mathbf{X} \\mid \\mathbf{Z}_{-(i,k)}, \\sigma_X, \\sigma_A)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac\n",
    "  {P(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{X}, \\sigma_X, \\sigma_A)\\,P(z_{i,k} \\mid \\mathbf{Z}_{-(i,k)},\\sigma_X, \\sigma_A)}\n",
    "  {P(\\mathbf{X} \\mid \\mathbf{Z}_{-(i,k)}, \\sigma_X, \\sigma_A)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\propto P(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{X}, \\sigma_X, \\sigma_A)\\,P(z_{i,k} \\mid \\mathbf{Z}_{-(i,k)},\\sigma_X, \\sigma_A)\n",
    "$$\n",
    "\n",
    "(since the denominator is constant wrt $z_{i,k}$, therefore doesnt change the shape of the probability distribution of $z_{i,k}$, simply acts as a constant of normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\mathbf{X} \\mid \\mathbf{X}, \\sigma_X, \\sigma_A)$, we've just calculated, by marginalizing the joint probability of $\\mathbf{X}$ and $\\mathbf{A}$ over $\\mathbf{A}$.\n",
    "\n",
    "$P(z_{i,k} \\mid \\mathbf{Z}_{-(i,k)})$, we calculated in the previous section, for Gibbs sampling of a finite feature model:\n",
    "\n",
    "$$\n",
    "P(z_{i,k} \\mid \\mathbf{z}_{-i,k})\n",
    "= \\frac{m_{-i,k} + \\frac{\\alpha}{K}}\n",
    "  {N + \\frac{\\alpha}{K}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: rank-1 updates\n",
    "\n",
    "The formula for the exponential trace expression of the marginalized form over $\\mathbf{A}$ needs the calculation of an inverse:\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "  \\mathbf{Z}^T\\mathbf{Z}\n",
    "  + \\frac{\\sigma_X^2}{\\sigma_A^2}\n",
    "  \\mathbf{I}\n",
    "\\right)^{-1}\n",
    "$$\n",
    "\n",
    "Matrix inversions are generally expensive, computationally. For example, if a matrix is symmetric and positive-definite, one can use Cholesky Decomposition to calculate the inverse, which uses about $\\frac{1}{3}n^3$ flops, where $n$ is the size of the matrix.  This is less than LU decomposition, which uses about $\\frac{2}{3}n^3$ flops, but is still cubic in $n$.\n",
    "\n",
    "Therefore if we can avoid re-calculating an inverse repeatedly during an algorithmic implementation, then the implementation will run faster.\n",
    "\n",
    "One way to avoid calculating the inverse repeatedly, is to use rank-1 updates, where this is possible.\n",
    "\n",
    "Given the Cholesky decomposition of matrix $\\mathbf{A}$ into $\\mathbf{L}\\mathbf{L}^T$, where $\\mathbf{L}$ is a lower triangular matrix, then the rank-1 update calculates $\\mathbf{L}'$ where:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} + \\mathbf{x}\\mathbf{x}^T\n",
    "= \\mathbf{L}' \\mathbf{L}'^T\n",
    "$$\n",
    "\n",
    "...given vector $\\mathbf{x}$, and the original decomposition of $\\mathbf{A} = \\mathbf{L}\\mathbf{L}^T$\n",
    "\n",
    "Per \"Methods for Modifying Matrix Factorizations\", by Gill et al, 1974, rank-1 update to Cholesky factorization can be carried out in $O(n^2)$ flops. (method 'C1').\n",
    "\n",
    "From a practical, engineering, point of view, it looks like numpy doesnt provide an implementation for rank-1 updates.  LINPACK does, and matlab does, but numpy doesnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wikipedia](https://en.wikipedia.org/wiki/Cholesky_decomposition) provides a matlab implementation of a rank-1 update.  Let's try implementing it in Python, and then try a `numpy` built-in method (which presumably uses BLAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3000)\n",
      "[[ 2970.41996429    21.31644394]\n",
      " [   21.31644394  2942.49052004]]\n",
      "time for full rank Cholesky 0.3525505065917969 seconds\n",
      "[[ 54.50155928   0.           0.        ]\n",
      " [  0.39111622  54.243318     0.        ]\n",
      " [  0.93970653   0.2525172   54.49271706]]\n",
      "(3000,)\n",
      "[-0.14441793  0.18327688 -1.23138222]\n",
      "3007.10831474\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e08ed7b840d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mLLT2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mdiff\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "N = 3000\n",
    "B = np.random.randn(N, N)\n",
    "A = B.T.dot(B)\n",
    "print(A.shape)\n",
    "print(A[:2,:2])\n",
    "start = time.time()\n",
    "L = np.linalg.cholesky(A)\n",
    "print('time for full rank Cholesky %s seconds' % (time.time() - start))\n",
    "print(L[:3,:3])\n",
    "\n",
    "# check\n",
    "LLT = L.dot(L.T)\n",
    "assert np.abs(A - LLT).max() < 1e-6\n",
    "\n",
    "x = np.random.randn(N)\n",
    "print(x.shape)\n",
    "print(x[:3])\n",
    "\n",
    "def update_cholesky(L, x):\n",
    "    N = x.shape[0]\n",
    "    for k in range(N):\n",
    "        r = math.sqrt(L[k, k] * L[k, k] + x[k] * x[k])\n",
    "        c = r / L[k, k]\n",
    "        s = x[k] / L[k, k]\n",
    "        L[k, k] = r\n",
    "        L[k + 1:, k] = (L[k + 1:, k] + s * x[k + 1:]) / c\n",
    "        x[k + 1:] = c * x[k + 1:] - s * L[k + 1:, k]\n",
    "\n",
    "update_cholesky(L, x)\n",
    "LLT2 = L.dot(L.T)\n",
    "diff = np.abs(A + x.dot(x.T) - LLT2).max()\n",
    "print(diff)\n",
    "assert diff < 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... fails, not sure why.  But anyway, looking at the Griffiths and Ghahramani tutorial again, it looks like it's not actually using a generic rank-1 Cholesky update: they're designing/deriving their own rank-1 update formulae.  So let's go through that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rank-1 updates to marginal probability of $\\mathbf{X}$\n",
    "\n",
    "Reminder: $\\mathbf{M}$ is:\n",
    "\n",
    "$$\\mathbf{M}\n",
    "= \\left(\n",
    "  \\mathbf{Z}^T\\mathbf{Z} + \\frac{\\alpha_X^2}{\\alpha_A^2}\n",
    "  \\mathbf{I}\n",
    "\\right)^{-1}\n",
    "$$\n",
    "\n",
    "Per the tutorial, we should define:\n",
    "\n",
    "$$\n",
    "\\mathbf{M}_{-i}\n",
    "= \\left(\n",
    "\\sum_{j \\ne i}\n",
    "z_j^T z_j\n",
    "+ \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}\n",
    "\\right)\n",
    "^{-1}\n",
    "$$\n",
    "\n",
    "And then, I cant quite guess/see what the tutorial is/is going to do, so I'm just going to work through what the tutorial does:\n",
    "\n",
    "$$\n",
    "\\mathbf{M}_{-i}\n",
    "= (\\mathbf{M}^{-1} - \\mathbf{z}_i^T\\mathbf{z}_i)^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= ( \\dots\n",
    "$$\n",
    "\n",
    "### Sheman-Morrison Formula for rank-1 updates\n",
    "\n",
    "I stared at this for a while, and then decided the tutorial must probably be using a generic, well-known, rank-1 update formula.  I googled for 'matrix inverse rank-1 update', and found:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula\n",
    "\n",
    "This gives the following formula:\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "  \\mathbf{A} + \\mathbf{u}\\mathbf{v}^T \n",
    "\\right)^{-1}\n",
    "=\n",
    "\\mathbf{A}^{-1}\n",
    "-\n",
    "\\frac{\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{v}^T\\mathbf{A}^{-1}}\n",
    "  {1 + \\mathbf{v}^T\\mathbf{A}^{-1}\\mathbf{u}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty similar to equation (23) in the tutorial, so let's work with the idea that the tutorial is using the Sherman-Morrison formula for now.  Let's first modify the Sherman-Morrison formula in two ways:\n",
    "\n",
    "- write it for a downdate, ie for $(\\mathbf{A} - \\dots)^{-1}$ rather than $(\\mathbf{A} + \\dots)^{-1}$, and\n",
    "- modify to write for the case that there is only one vector $\\mathbf{v}$, rather than the more general form of two unequal vectors $\\mathbf{u}$ and $\\mathbf{v}$\n",
    "\n",
    "We can do this by substituting $\\mathbf{u} = - \\mathbf{v}$:\n",
    "\n",
    "$$\n",
    "(\\mathbf{A} - \\mathbf{v}\\mathbf{v}^T)^{-1}\n",
    "=\n",
    "\\mathbf{A}^{-1}\n",
    "-\n",
    "\\frac\n",
    "  {\\mathbf{A}^{-1} \\cdot (-\\mathbf{v}) \\cdot \\mathbf{v}^T \\cdot \\mathbf{A}^{-1}}\n",
    "  {1 + \\mathbf{v}^T \\cdot \\mathbf{A}^{-1} \\cdot (-\\mathbf{v})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\mathbf{A}^{-1}\n",
    "+\n",
    "\\frac\n",
    "  {\\mathbf{A}^{-1} \\cdot (\\mathbf{v}) \\cdot \\mathbf{v}^T \\cdot \\mathbf{A}^{-1}}\n",
    "  {1 - \\mathbf{v}^T \\cdot \\mathbf{A}^{-1} \\cdot (\\mathbf{v})}\n",
    "$$\n",
    "\n",
    "Or, by comparison with what is in the tutorial, giving some idea of where this might be going, reversing the order of the terms in the denominator, and the sign of the right-hand side:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\mathbf{A}^{-1}\n",
    "-\n",
    "\\frac\n",
    "  {\\mathbf{A}^{-1} \\cdot (\\mathbf{v}) \\cdot \\mathbf{v}^T \\cdot \\mathbf{A}^{-1}}\n",
    "  {\\mathbf{v}^T \\cdot \\mathbf{A}^{-1} \\cdot (\\mathbf{v}) - 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\mathbf{A}^{-1}\n",
    "-\n",
    "\\frac\n",
    "  {\\mathbf{A}^{-1} \\mathbf{v} \\mathbf{v}^T \\mathbf{A}^{-1}}\n",
    "  {\\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{v} - 1}\n",
    "$$\n",
    "\n",
    "This kind of matches what's in the tutorial, except that we have inverses here, like $\\mathbf{A}^{-1}$, whereas the tutorial has $\\mathbf{M}$, but since $\\mathbf{M}$ is in fact defined as an inverse, ie $(\\mathbf{Z}^T\\mathbf{Z} - \\frac{\\alpha_X^2}{\\alpha_A^2}\\mathbf{I})^{-1}$, then this might work out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's substitute in the terms/notation for our actual concrete expression, into the modified Sherman-Morrison formula.  This means we will subsitute $\\mathbf{A}$ by $\\mathbf{M}^{-1}$, and $\\mathbf{v}$ with $\\mathbf{z}_i^T$.  Note that $\\mathbf{v}$ is a column vector, whereas $\\mathbf{z}_i$ is a row vector.  $\\mathbf{z}_i^T$ is a column vector, which matches therefore the form of $\\mathbf{v}$.  So we get:\n",
    "\n",
    "$$\n",
    "(\\mathbf{M}^{-1} - \\mathbf{z}_i^T\\mathbf{z})^{-1}\n",
    "=\n",
    "\\mathbf{M}\n",
    "-\n",
    "\\frac{\\mathbf{M}\\mathbf{z}_i^T\\mathbf{z}_i \\mathbf{M}}\n",
    "  {\\mathbf{z}_i\\mathbf{M}\\mathbf{z}_i^T - 1}\n",
    "$$\n",
    "\n",
    "... which matches exactly the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Sherman-Morrison in numpy\n",
    "\n",
    "Let's retry rank-1 updates in numpy, using the Sherman-Morrison formula.  Note that the Sherman-Morrison formula doesnt need decomposition, eg QR or Cholesky, just needs the inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3000)\n",
      "[[ 2982.31866914    96.38717338]\n",
      " [   96.38717338  3039.32098946]]\n",
      "time for full rank inverse 2.2654874324798584 seconds\n",
      "[[ 1.36423992 -0.47994222  0.13859518]\n",
      " [-0.47994222  0.30385504 -0.01174672]\n",
      " [ 0.13859518 -0.01174672  0.33828845]]\n",
      "6.32098817732e-11\n",
      "(3000, 1)\n",
      "[[-1.22683011]\n",
      " [ 1.1851708 ]\n",
      " [ 0.35445188]]\n",
      "time for sherman morrison 1.4911704063415527 seconds\n",
      "diff after rank 1 update: 0.312939937953\n",
      "time for full rank inverse 2.178471326828003 seconds\n",
      "diff after full rank inverse 2.45563569479e-11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "N = 3000\n",
    "B = np.random.randn(N, N)\n",
    "A = B.T.dot(B)\n",
    "print(A.shape)\n",
    "print(A[:2,:2])\n",
    "start = time.time()\n",
    "Ainv = np.linalg.inv(A)\n",
    "print('time for full rank inverse %s seconds' % (time.time() - start))\n",
    "print(Ainv[:3,:3])\n",
    "\n",
    "# check\n",
    "A_Ainv = A.dot(Ainv)\n",
    "diff = np.abs(A_Ainv - np.identity(N)).max()\n",
    "print(diff)\n",
    "assert diff < 1e-8\n",
    "\n",
    "x = np.random.randn(N, 1)\n",
    "print(x.shape)\n",
    "print(x[:3])\n",
    "\n",
    "def sherman_morrison(Ainv, x):\n",
    "    numerator = Ainv.dot(x).dot(x.T).dot(Ainv)\n",
    "    denominator = x.T.dot(Ainv).dot(x) - 1\n",
    "    return Ainv - numerator / denominator\n",
    "\n",
    "start = time.time()\n",
    "Ainv2 = sherman_morrison(Ainv, x)\n",
    "print('time for sherman morrison %s seconds' % (time.time() - start))\n",
    "A2 = A + x.dot(x.T)\n",
    "A2_Ainv2 = A2.dot(Ainv2)\n",
    "diff = np.abs(A2_Ainv2 - np.identity(N)).max()\n",
    "print('diff after rank 1 update: %s' % diff)\n",
    "# assert diff < 1e-8\n",
    "\n",
    "# try full rank inverse\n",
    "start = time.time()\n",
    "Ainv2 = np.linalg.inv(A2)\n",
    "print('time for full rank inverse %s seconds' % (time.time() - start))\n",
    "A2_Ainv2 = A2.dot(Ainv2)\n",
    "diff = np.abs(A2_Ainv2 - np.identity(N)).max()\n",
    "print('diff after full rank inverse %s' % diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmm, Sherman-Morrison took only 40% less time than full-rank, but vastly more inaccurate.\n",
    "\n",
    "But ... what if we re-arrange the order of the matrix multiplication slightly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for sherman morrison 0.23725247383117676 seconds\n",
      "diff after rank 1 update: 0.312939937938\n",
      "time for full rank inverse 2.1450388431549072 seconds\n",
      "diff after full rank inverse 2.45563569479e-11\n"
     ]
    }
   ],
   "source": [
    "def sherman_morrison_v2(Ainv, x):\n",
    "    n1 = Ainv.dot(x)\n",
    "    n2 = x.T.dot(Ainv)\n",
    "    numerator = n1.dot(n2)\n",
    "    denominator = x.T.dot(Ainv).dot(x) - 1\n",
    "    return Ainv - numerator / denominator\n",
    "\n",
    "start = time.time()\n",
    "Ainv2 = sherman_morrison_v2(Ainv, x)\n",
    "print('time for sherman morrison %s seconds' % (time.time() - start))\n",
    "A2 = A + x.dot(x.T)\n",
    "A2_Ainv2 = A2.dot(Ainv2)\n",
    "diff = np.abs(A2_Ainv2 - np.identity(N)).max()\n",
    "print('diff after rank 1 update: %s' % diff)\n",
    "# assert diff < 1e-8\n",
    "\n",
    "# try full rank inverse\n",
    "start = time.time()\n",
    "Ainv2 = np.linalg.inv(A2)\n",
    "print('time for full rank inverse %s seconds' % (time.time() - start))\n",
    "A2_Ainv2 = A2.dot(Ainv2)\n",
    "diff = np.abs(A2_Ainv2 - np.identity(N)).max()\n",
    "print('diff after full rank inverse %s' % diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better: still looking pretty inaccurate, compared to full-rank, but at least it's ~10 times faster, for matrices with side $3000$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the section entitled 'Application' on the wikipedia page, which describes the assymptotic complexity, it occurred to me that `n1` and `n2` are basically the same matrix, so we can do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for sherman morrison 0.2752869129180908 seconds\n",
      "diff after rank 1 update: 0.312939914919\n",
      "time for full rank inverse 2.1179018020629883 seconds\n",
      "diff after full rank inverse 2.45563569479e-11\n"
     ]
    }
   ],
   "source": [
    "def sherman_morrison_v3(Ainv, x):\n",
    "    n1 = Ainv.dot(x)\n",
    "    numerator = n1.dot(n1.T)\n",
    "    denominator = x.T.dot(Ainv).dot(x) - 1\n",
    "    return Ainv - numerator / denominator\n",
    "\n",
    "start = time.time()\n",
    "Ainv2 = sherman_morrison_v3(Ainv, x)\n",
    "print('time for sherman morrison %s seconds' % (time.time() - start))\n",
    "A2 = A + x.dot(x.T)\n",
    "A2_Ainv2 = A2.dot(Ainv2)\n",
    "diff = np.abs(A2_Ainv2 - np.identity(N)).max()\n",
    "print('diff after rank 1 update: %s' % diff)\n",
    "# assert diff < 1e-8\n",
    "\n",
    "# try full rank inverse\n",
    "start = time.time()\n",
    "Ainv2 = np.linalg.inv(A2)\n",
    "print('time for full rank inverse %s seconds' % (time.time() - start))\n",
    "A2_Ainv2 = A2.dot(Ainv2)\n",
    "diff = np.abs(A2_Ainv2 - np.identity(N)).max()\n",
    "print('diff after full rank inverse %s' % diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but the time is the same.  So presumably the time is dominated by the `n1.dot(n1.T)` calculation.  Check this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for n1 0.0075109004974365234\n",
      "time for numerator 0.052231788635253906\n",
      "time for denominator 0.010253190994262695\n",
      "time for final per-element 0.06621360778808594\n",
      "time for sherman morrison 0.138261079788208 seconds\n",
      "diff after rank 1 update: 0.312939914919\n"
     ]
    }
   ],
   "source": [
    "def sherman_morrison_with_timing(Ainv, x):\n",
    "    last = time.time()\n",
    "    n1 = Ainv.dot(x)\n",
    "    print('time for n1 %s' % (time.time() - last))\n",
    "    last = time.time()\n",
    "    numerator = n1.dot(n1.T)\n",
    "    print('time for numerator %s' % (time.time() - last))\n",
    "    last = time.time()\n",
    "    denominator = x.T.dot(Ainv).dot(x) - 1\n",
    "    print('time for denominator %s' % (time.time() - last))\n",
    "    last = time.time()\n",
    "    res = Ainv - numerator / denominator\n",
    "    print('time for final per-element %s' % (time.time() - last))\n",
    "    return res\n",
    "\n",
    "start = time.time()\n",
    "Ainv2 = sherman_morrison_with_timing(Ainv, x)\n",
    "print('time for sherman morrison %s seconds' % (time.time() - start))\n",
    "A2_Ainv2 = A2.dot(Ainv2)\n",
    "diff = np.abs(A2_Ainv2 - np.identity(N)).max()\n",
    "print('diff after rank 1 update: %s' % diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dominated by the final per-element operations, interestingly.  And surprisingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this because the per-element scalar division is being implemented using division, rather than as multiplication by 1 over the divisor?  Check this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for n1 0.008286476135253906\n",
      "time for numerator 0.07381415367126465\n",
      "time for denominator 0.011293649673461914\n",
      "time for final per-element 0.12431550025939941\n",
      "time for sherman morrison 0.2201707363128662 seconds\n",
      "diff after rank 1 update: 0.312939914921\n"
     ]
    }
   ],
   "source": [
    "def sherman_morrison_with_timing_v2(Ainv, x):\n",
    "    last = time.time()\n",
    "    n1 = Ainv.dot(x)\n",
    "    print('time for n1 %s' % (time.time() - last))\n",
    "    last = time.time()\n",
    "    numerator = n1.dot(n1.T)\n",
    "    print('time for numerator %s' % (time.time() - last))\n",
    "    last = time.time()\n",
    "    denominator = x.T.dot(Ainv).dot(x) - 1\n",
    "    print('time for denominator %s' % (time.time() - last))\n",
    "    last = time.time()\n",
    "    res = Ainv - numerator * (1 / denominator)\n",
    "    print('time for final per-element %s' % (time.time() - last))\n",
    "    return res\n",
    "\n",
    "start = time.time()\n",
    "Ainv2 = sherman_morrison_with_timing_v2(Ainv, x)\n",
    "print('time for sherman morrison %s seconds' % (time.time() - start))\n",
    "A2_Ainv2 = A2.dot(Ainv2)\n",
    "diff = np.abs(A2_Ainv2 - np.identity(N)).max()\n",
    "print('diff after rank 1 update: %s' % diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse. bizarrely.  It's almost as though the operations are being executed asynchonously, and the final operation is a sync point.  It is surprising for per-element operations to take relatively significant time.\n",
    "\n",
    "Is it something like, the `denominator` value is a matrix, and somehow this makes it take longer?  Let's convert it to a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for n1 0.009202003479003906\n",
      "time for numerator 0.05918169021606445\n",
      "time for denominator 0.008460283279418945\n",
      "time for final per-element 0.06301307678222656\n",
      "time for sherman morrison 0.14108610153198242 seconds\n",
      "diff after rank 1 update: 0.312939914921\n"
     ]
    }
   ],
   "source": [
    "def sherman_morrison_with_timing_v2(Ainv, x):\n",
    "    last = time.time()\n",
    "    n1 = Ainv.dot(x)\n",
    "    print('time for n1 %s' % (time.time() - last))\n",
    "    last = time.time()\n",
    "    numerator = n1.dot(n1.T)\n",
    "    print('time for numerator %s' % (time.time() - last))\n",
    "    last = time.time()\n",
    "    denominator = x.T.dot(Ainv).dot(x) - 1\n",
    "    print('time for denominator %s' % (time.time() - last))\n",
    "    last = time.time()\n",
    "    res = Ainv - numerator * (1 / denominator.item())\n",
    "    print('time for final per-element %s' % (time.time() - last))\n",
    "    return res\n",
    "\n",
    "start = time.time()\n",
    "Ainv2 = sherman_morrison_with_timing_v2(Ainv, x)\n",
    "print('time for sherman morrison %s seconds' % (time.time() - start))\n",
    "A2_Ainv2 = A2.dot(Ainv2)\n",
    "diff = np.abs(A2_Ainv2 - np.identity(N)).max()\n",
    "print('diff after rank 1 update: %s' % diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weirdly, making the denominator explicitly a scalar accelerates the per-element operations by 50%, and now the matrix multiplication of the numerator dominates.\n",
    "\n",
    "The Sherman-Morrison update is now about ~15 times faster than the full-rank inverse, though still just as imprecise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the tutorial, we still need the expression for $\\mathbf{M}$, in terms of $\\mathbf{M}_{-i}$, but it is clear that it is just Sherman-Morrison again, the exact same formula, just with slightly different inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the infinite limit\n",
    "\n",
    "Just following through the tutorial directly:\n",
    "\n",
    "We take $\\mathbf{Z}$ to be in left-ordered form, and write it as $[ \\mathbf{Z}_+ \\mathbf{Z}_0]$.\n",
    "\n",
    "Therefore, $|\\mathbf{Z}^T\\mathbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}|$ becomes:\n",
    "\n",
    "$$\n",
    "\\left|\n",
    "  \\begin{bmatrix}\n",
    "    \\mathbf{Z}^T_+ \\mathbf{Z}_+ & \\mathbf{0} \\\\\n",
    "    \\mathbf{0} & \\mathbf{0} \\\\\n",
    "  \\end{bmatrix}\n",
    "  +\n",
    "  \\frac{\\sigma_X^2}{\\sigma_A^2}\n",
    "  \\mathbf{I}_K\n",
    "\\right|\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\left(\n",
    "  \\frac{\\sigma_X^2}{\\sigma_A^2}\n",
    "\\right)^{K_0}\n",
    "\\left|\n",
    "  \\mathbf{Z}_+^T\\mathbf{Z}_+\n",
    "  +\n",
    "  \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}_{K^+}\n",
    "\\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And: $\\mathbf{Z}(\\mathbf{Z}^T\\mathbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I})^{-1}\\mathbf{Z}^T$ becomes:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}_+\n",
    "\\left(\n",
    "\\mathbf{Z}^T_+\n",
    "\\mathbf{Z}_+\n",
    "+\n",
    "\\frac{\\sigma_X^2}{\\sigma_A^2}\n",
    "\\mathbf{I}_{K_+}\n",
    "\\right)^{-1}\n",
    "\\mathbf{Z}_+^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder, equation 21 from the tutorial is the marginal probability of $\\mathbf{X}$ given $\\mathbf{Z}$, with $\\mathbf{A}$ marginalized out:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{X} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)\n",
    "=\n",
    "\\frac\n",
    "  {1}\n",
    "  {(2\\pi)^{ND/2} \\sigma_X^{(N-K)D} \\sigma_A^{KD} |\\mathbf{Z}^T\\mathbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2} \\mathbf{I}|^{D/2} }\n",
    "\\exp \\left(\n",
    "  -\\frac{1}{2\\sigma_X^2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    \\mathbf{X}^T(\\mathbf{I} - \\mathbf{Z}(\\mathbf{Z}^T\\mathbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2} \\mathbf{I})^{-1}\\mathbf{Z}^T)\\mathbf{X}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac\n",
    "  {1}\n",
    "  {(2\\pi)^{ND/2} \\sigma_X^{(N-K_+)D} \\sigma_A^{K_+D} |\\mathbf{Z_+}^T\\mathbf{Z_+} + \\frac{\\sigma_X^2}{\\sigma_A^2} \\mathbf{I}_{K_+}|^{D/2} }\n",
    "\\exp \\left(\n",
    "  -\\frac{1}{2\\sigma_X^2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    \\mathbf{X}^T(\\mathbf{I} - \\mathbf{Z}_+(\\mathbf{Z}_+^T\\mathbf{Z}_+ + \\frac{\\sigma_X^2}{\\sigma_A^2} \\mathbf{I}_{K+})^{-1}\\mathbf{Z}_+^T)\\mathbf{X}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
