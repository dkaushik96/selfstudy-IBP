{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example: A Linear-Gaussian Latent Feature Model with Binary Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Linear-Gaussian Model\n",
    "\n",
    "$\\mathbf{x}_i$ is generated from a Gaussian with:\n",
    "\n",
    "- mean $\\mathbf{z}_i \\mathbf{A}$\n",
    "- covariance $\\Sigma_X = \\sigma^2_X \\mathbf{I}$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{z}_i$ is a $K$-dimensional binary vector\n",
    "- $\\mathbf{A}$ is a $K$ x $D$ matrix of weights\n",
    "\n",
    "Since the mean of $\\mathbf{x}_i$ is $\\mathbf{z}_i \\mathbf{A}$, so the expectation of $\\mathbf{X}$, $\\mathbb{E}(\\mathbf{X})$, is $\\mathbf{Z} \\mathbf{A}$.\n",
    "\n",
    "Given that:\n",
    "\n",
    "- $\\mathbf{X}$ is an $N$ x $D$ matrix\n",
    "\n",
    "So:\n",
    "\n",
    "- $\\mathbf{Z}$ is an $N$ x $K$ matrix\n",
    "\n",
    "For the sake of thinking this through, this means that for example each observation $\\mathbf{x}_i$ could be low-dimensional, eg 1 or 2 dimensions, whilst the latent feature matrix $\\mathbf{Z}$ could have high-dimensional $K$, eg 1000, or more.\n",
    "\n",
    "Note that the covariance matrix is diagonal, so the Gaussians have spherical isocontours.  This constrains the solution, simplifying inference.\n",
    "\n",
    "As per the tutorial, and since each $\\mathbf{x}_i$ distributed as a symmetric Gaussian, the distribution of $\\mathbf{X}$, given $\\mathbf{Z}$, $\\mathbf{A}$ and $\\sigma_X$ is:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A}, \\sigma_X)\n",
    "=\n",
    "\\frac\n",
    "  {1}\n",
    "  {(2\\pi \\sigma_X^2)^{ND/2}}\n",
    "\\exp \\left(\n",
    "  -\n",
    "  \\frac{1}\n",
    "    {2\\sigma_X^2}\n",
    "    \\mathrm{tr}\n",
    "    (\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})^T\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})\n",
    "    )\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since $\\mathbf{Z}$ is a dimension in $N$, so the mean of each $\\mathbf{x}_i$ can be different.  $\\mathbf{A}$ does not have a dimension in $N$, and gives the properties of each mixing component. (I think)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial suggests that we should integrate out the model components $\\mathbf{A}$, and that we can do so, if we define a prior on it, which the tutorial suggests to be a Gaussian:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{A} \\mid \\sigma_A) =\n",
    "\\frac{1}\n",
    "  {(2 \\pi \\sigma_A^2)^{KD/2}}\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2\\sigma_A^2}\n",
    "  \\mathrm{tr}(\\mathbf{A}^T \\mathbf{A})\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying these two probabilities we get:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X}, \\mathbf{A} \\mid \\mathbf{Z}, \\sigma_X)\n",
    "= p(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A}, \\sigma_X) \\, p(\\mathbf{A} \\mid \\sigma_A)\n",
    "$$\n",
    "$$\n",
    "= \n",
    "\\frac\n",
    "  {1}\n",
    "  {(2\\pi \\sigma_X^2)^{ND/2}}\n",
    "\\exp \\left(\n",
    "  -\n",
    "  \\frac{1}\n",
    "    {2\\sigma_X^2}\n",
    "    \\mathrm{tr}\n",
    "    (\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})^T\n",
    "      (\\mathbf{X} - \\mathbf{Z}\\mathbf{A})\n",
    "    )\n",
    "\\right)\n",
    "\\cdot\n",
    "\\frac{1}\n",
    "  {(2 \\pi \\sigma_A^2)^{KD/2}}\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2\\sigma_A^2}\n",
    "  \\mathrm{tr}(\\mathbf{A}^T \\mathbf{A})\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradually working through the tutorial expressions:\n",
    "\n",
    "$$\n",
    "\\propto\n",
    "\\exp\n",
    "\\left(\n",
    "- \\frac{1}{2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{X}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{X}^T \\mathbf{Z} \\mathbf{A}\n",
    "    + \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{Z}\\mathbf{A}\n",
    "    + \\frac{1}{\\sigma_A^2} \\mathbf{A}^T \\mathbf{A}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\mathrm{tr} \\left(\n",
    "    \\frac{1}{\\sigma_X^2}\\mathbf{X}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{A}^T\\mathbf{Z}^T\\mathbf{X}\n",
    "    - \\frac{1}{\\sigma_X^2} \\mathbf{X}^T \\mathbf{Z} \\mathbf{A}\n",
    "    + \\mathbf{A}^T(\\frac{1}{\\sigma_X^2}\\mathbf{Z}^T\\mathbf{Z} + \\frac{1}{\\sigma_A^2}\\mathbf{I})\\mathbf{A}\n",
    "  \\right)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: Matrix and Gaussian revision\n",
    "\n",
    "At this point, I had to reach out to revise some properties of matrices and Gaussians that I ~~had forgotten~~ didnt know.  Some of the resources I used for this section:\n",
    "\n",
    "- \"Bayesian Linear Regression\", Minka, 1998 (revised 2010)\n",
    "- https://en.wikipedia.org/wiki/Gaussian_integral\n",
    "- https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n",
    "- https://en.wikipedia.org/wiki/Matrix_normal_distribution\n",
    "- [https://en.wikipedia.org/wiki/Vectorization_(mathematics)](https://en.wikipedia.org/wiki/Vectorization_(mathematics%29)\n",
    "- https://en.wikipedia.org/wiki/Conjugate_transpose\n",
    "\n",
    "#### Matrix normal distribution\n",
    "\n",
    "So, going through, bit by bit: the integral of a Multivariate normal distribution is:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}\n",
    "\\exp \\left(\n",
    "   -\\frac{1}{2}\n",
    "   (\\mathbf{x}^T \\mathbf{A} \\mathbf{x})\n",
    "\\right)\n",
    "\\,\n",
    "d\\mathbf{x}\n",
    "=\n",
    "\\sqrt{\\frac{(2\\pi)^D}{\\mathrm{det}\\,\\mathbf{A}}}\n",
    "$$\n",
    "\n",
    "*However*, in this case, $\\mathbf{x}$ is a vector, but we will have $\\mathbf{X}$, or something of this sort, ie: a matrix.  So, what we will have is in fact: a Matrix normal distribution.  A Matrix normal distribution, per wikipedia article, has the form:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X} \\mid \\mathbf{M}, \\mathbf{U}, \\mathbf{V})\n",
    "= \\frac\n",
    "  {\\exp(\n",
    "     -\\frac{1}{2}\n",
    "     \\mathrm{tr}(\n",
    "       \\mathbf{V}^{-1}\n",
    "       (\\mathbf{X} - \\mathbf{M})^T\n",
    "       \\mathbf{U}^{-1}\n",
    "       (\\mathbf{X} - \\mathbf{M})\n",
    "     )\n",
    "  }\n",
    "  {(2\\pi)^{NK/2} \\left| \\mathbf{V} \\right|^{N/2} \\left| \\mathbf{U} \\right|^{K/2}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathrm{tr}$ denotes \"trace\"\n",
    "- $\\mathrm{M}$ is $N$ x $K$\n",
    "- $\\mathrm{U}$ is $N$ x $N$ (so: square)\n",
    "- $\\mathrm{V}$ is $v$ x $v$ (also square)\n",
    "\n",
    "(where I've changed $n$ in the Wikipedia article to $N$, and $p$ to $K$, in line with the notation we are using elsewhere)\n",
    "\n",
    "The article then states that the relationship to Multivariate normal is:\n",
    "\n",
    "$$\n",
    "\\mathrm{vec}(\\mathbf{X})\n",
    "\\sim\n",
    "\\mathcal{N}_{NK}(\\mathrm{vec}(\\mathbf{M}, \\mathbf{V} \\otimes \\mathbf{U}))\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\otimes$ is Kronecker Product\n",
    "- $\\mathrm{vec}$ is vectorization\n",
    "\n",
    "#### Vectorization\n",
    "\n",
    "Where is Kronecker Product, and what is vectorization?  The [Wikipedia article](https://en.wikipedia.org/wiki/Vectorization_(mathematics%29) gives a good description of vectorization.  You stack each column of the matrix on top of each other, so it becomes a vector:\n",
    "\n",
    "If:\n",
    "$$\n",
    "\\mathbf{A}\n",
    "=\n",
    "\\begin{bmatrix} a & b \\\\\n",
    "c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "\\mathrm{vec}(\\mathbf{A})\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "c \\\\\n",
    "b \\\\\n",
    "e \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, if $\\mathbf{A}$ is $M$ x $N$, and the result of $\\mathrm{vec}(\\mathbf{A})$ is $C$, then:\n",
    "$$\n",
    "a_{i,j}\n",
    "= \n",
    "c_{jM + i,1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kronecker Product\n",
    "\n",
    "The Kronecker product of matrices $\\mathbf{A}$ and $\\mathbf{B}$ is formed by tiling the matrices $\\mathbf{A}$ and $\\mathbf{B}$ as follows, and then forming the per-element product.  If we have the following matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\n",
    "=\n",
    "\\begin{bmatrix} a & b \\\\\n",
    "c & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{B}\n",
    "=\n",
    "\\begin{bmatrix} e & f \\\\\n",
    "g & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then matrix $\\mathbf{A}$ will be tiled like:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & a & b & b \\\\\n",
    "a & a & b & b \\\\\n",
    "c & c & d & d \\\\\n",
    "c & c & d & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Matrix $\\mathbf{B}$ will be tiled like:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e & f & e & f \\\\\n",
    "g & h & g & h \\\\\n",
    "e & f & e & f \\\\\n",
    "g & h & g & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "... and the Kronecker product is the Hadamard (per-element) product of these tiled matrices:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae & af & be & bf \\\\\n",
    "ag & ah & bg & bh \\\\\n",
    "ce & cf & de & df \\\\\n",
    "cg & ch & dg & dh \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's compare this with the matrix product of these two matrices.  This is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + cg & be + df \\\\\n",
    "ce + dg & cf + ch \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Relationship between vectorization and inner product\n",
    "\n",
    "There are 8 pairs of products in the matrix product above, and 16 in this Kronecker product, so it seems not obvious to relate the two, eg via vectorization, somehow?\n",
    "\n",
    "Wikipedia however states that we can form a relationship between vectorization and the matrix product. For square, real matrices:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}\n",
    "(\\mathbf{A}^T \\mathbf{B})\n",
    "= \\mathrm{vec}(\\mathbf{A})^T \\mathrm{vec}(\\mathbf{B})\n",
    "$$\n",
    "\n",
    "\n",
    "Let's try this, for the example matrices above.\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{A})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "c \\\\\n",
    "b \\\\\n",
    "d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{vec}(\\mathbf{B})$ is:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "e \\\\\n",
    "g \\\\\n",
    "f \\\\\n",
    "h \\\\\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So:\n",
    "\n",
    "$$\\mathrm{vec}^T(\\mathbf{A})\\mathrm{vec}(\\mathbf{B}) = ae + cg + bf + dh$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{A}^T$ is:\n",
    "$$\n",
    "\\begin{bmatrix} a & c \\\\\n",
    "b & d \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "...and $\\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} e & f \\\\\n",
    "g & h \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $\\mathbf{A}^T \\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "ae + cg & ae + cg \\\\\n",
    "be + dg & cf + ch \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\mathrm{tr}$ is the sum of the diagonal, for a square matrix.  For example, for matrix $\\mathrm{X}$ of size $m$, $\\mathrm{tr}(\\mathbf{X})$ is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\mathbf{X}) = \\sum_{i=1}^m x_{i,i}\n",
    "$$\n",
    "\n",
    "So, $\\mathrm{tr}(\\mathbf{A}^T \\mathbf{B})$, in our example above, is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\mathbf{A}^T \\mathbf{B})\n",
    "= ae + cg + cf + ch\n",
    "$$\n",
    "\n",
    "... which matches the result for $\\mathrm{vec}(\\mathbf{A})^T \\mathrm{vec}(\\mathbf{B})$\n",
    "\n",
    "More generally, let's try for two $n$ x $n$ matrices $\\mathbf{A}$ and $\\mathbf{B}$.  The vectorizations will look like:\n",
    "\n",
    "$$\n",
    "a_{i,j} = \\mathrm{vec}(\\mathbf{A})_{jn + i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{k,l} = \\mathrm{vec}(\\mathbf{B})_{ln + k}\n",
    "$$\n",
    "\n",
    "To form the inner product of the vectorizations, let's use two nested sums.  The innermost sum will be over each row in a column, and the outermost will be over columns.  So this will give:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \n",
    "\\sum_{j=1}^{n}\n",
    "a_{j,i} b_{j,i}\n",
    "$$\n",
    "\n",
    "Meanwhile, $\\mathbf{A}^T$ is:\n",
    "\n",
    "$$\n",
    "(\\mathbf{A}^T)_{i,j} = a_{j,i}\n",
    "$$\n",
    "\n",
    "... and the matrix product $\\mathbf{A}^T\\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "(\\mathbf{A}^T\\mathbf{B})_{i,j} = \\sum_{k=1}^n a_{k,i} b_{k,j}\n",
    "$$\n",
    "The trace of this, is the sum over the diagonal, ie the sum of terms where $i = j$.  This gives:\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\mathbf{A}^T\\mathbf{B}) = \\sum_{l=1}^n \\sum_{k=1}^n a_{k,l} b_{k,l}\n",
    "$$\n",
    "\n",
    "By inspection, this is identical to the expression for $\\mathrm{vec}(\\mathbf{A})^T \\mathrm{vec}(\\mathbf{B})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old notes, not working yet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial says we should complete the square now, ~~since we want to find the mean and covariance matrix of the resulting Gaussian:~~  I had to think about this.  We're actually completing the square in terms of $\\mathbf{A}$, which I guess is because we intend to integrate over $\\mathbf{A}$ later?\n",
    "\n",
    "$$\n",
    "= \\exp \\left(\n",
    " - \\frac{1}{2}\n",
    " \\mathrm{tr}\n",
    "   \\left(\n",
    "     ()^T()\n",
    "   \\right)\n",
    " \\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
