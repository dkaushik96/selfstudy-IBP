{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: presentation \"Accelerated Gibbs Sampling for the Indian Buffet Process\", Doshi-Velez, Ghahramani\n",
    "\n",
    "It's here: http://videolectures.net/icml09_doshi_velez_ags/\n",
    "\n",
    "Corresponding slides: http://videolectures.net/site/normal_dl/tag=47975/icml09_doshi_velez_ags_01.pdf\n",
    "\n",
    "It's still advised by Ghahramani, but the student is now Doshi-Velez.  Going through the lecture:\n",
    "\n",
    "Finale notes that, for model:\n",
    "\n",
    "$$\\mathbf{X} = \\mathbf{Z}\\mathbf{A} + \\mathbf{E}$$\n",
    "\n",
    "- can compute $P(\\mathbf{Z} \\mid \\mathbf{Z})$, but expensive\n",
    "- can compute $P(\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z})$ (just a linear expression (?))\n",
    "- cannot compute $P(\\mathbf{Z}, \\mathbf{A} \\mid \\mathbf{Z})$\n",
    "\n",
    "For this lecture, Doshi-Velez is going to target putting a window over the data, $\\mathbf{X}$, and $\\mathbf{Z}$, so they will only consider a window of $W$ points at a time, splitting each matrix into:\n",
    "\n",
    "$$\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{X}_{-W} \\\\\n",
    "\\mathbf{X}_{W} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\\mathbf{Z} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{Z}_{-W} \\\\\n",
    "\\mathbf{Z}_{W} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "(where here the data points are arranged as one row per data point, and the features are the columns).\n",
    "\n",
    "They do this, because they want to target large datasets.\n",
    "\n",
    "Then, Doshi-Velez goes over the following two principle sampling schemes:\n",
    "\n",
    "### Uncollapsed sampling\n",
    "\n",
    "![](img/basicsampling_1.png)\n",
    "\n",
    "![](img/basicsampling_2.png)\n",
    "\n",
    "![](img/basicsampling_A.png)\n",
    "\n",
    "Uncollapsed sampling:\n",
    "\n",
    "- each iteration is quick (or should be, otherwise we have other issues to deal with)\n",
    "- but mixing very slow\n",
    "\n",
    "### Collapsed sampling\n",
    "\n",
    "![](img/collapsed_gibbs.png)\n",
    "\n",
    "- mixing fast\n",
    "- but each iteration can be slow/challenging, and\n",
    "- we lose the independences between the samples and latent features within the window, and outside of the window\n",
    "\n",
    "### Their idea: Keep a posterior on A\n",
    "\n",
    "![](img/acceleratedsampling.png)\n",
    "\n",
    "- data samples and latent variables within the window remain independent of those outside of it\n",
    "- (conditional on the posterior of A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, Doshi-Velez goes over the maths of the accelerated sampling:\n",
    "\n",
    "Consider one element, and here the notation has changed from $w$/$-w$ to $n$/$-n$ (edit in retrospect: the notation changed, because here $n$ represents a single element, whereas $w$ represents a window over multiple elements):\n",
    "\n",
    "$$P(z_{nk} = 1 \\mid Z_{-nk}, \\mathbf{X})$$\n",
    "&nbsp;\n",
    "$$\\propto P(z_{nk} \\mid \\mathbf{Z}_{-nk})\\,P(\\mathbf{X} \\mid \\mathbf{Z})$$\n",
    "\n",
    "So, it took me a while to figure out why this statement is proportionality, not equality.  Finally I realized we can do, following the hint about 'bayes rule' in the slides:\n",
    "\n",
    "$$P(z_{nk} = 1 \\mid \\mathbf{Z}_{-nk}, \\mathbf{X})\n",
    "= \n",
    "\\frac{\n",
    "   P(\\mathbf{X} \\mid z_{nk} = 1, \\mathbf{Z}_{-nk})\\, P(z_{nk} = 1 \\mid \\mathbf{Z}_{-nk})\n",
    "}\n",
    "{\n",
    "   P(\\mathbf{X} \\mid \\mathbf{Z}_{-nk})\n",
    "}$$\n",
    "&nbsp;\n",
    "$$\\propto P(z_{nk} \\mid \\mathbf{Z}_{-nk})\\,P(\\mathbf{X} \\mid \\mathbf{Z})$$\n",
    "\n",
    "as stated.\n",
    "\n",
    "Continuing, by writing $P(\\mathbf{X} \\mid \\mathbf{Z})$ as a marginalization over $\\mathbf{A}$:\n",
    "\n",
    "$$=\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk})\n",
    "\\int_A P(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A})\\, P(\\mathbf{A})\\,d\\mathbf{A}\n",
    "$$\n",
    "\n",
    "Factorizing over $\\mathbf{X}_n$ and $\\mathbf{X}_{-n}$:\n",
    "\n",
    "$$\n",
    "=\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk})\n",
    "\\int_A P(\\mathbf{X}_n \\mid \\mathbf{Z}_n, \\mathbf{A})\\, P(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n}, \\mathbf{A})\\, P(\\mathbf{A})\\,d\\mathbf{A}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this identity (or plausibly: proportionality):\n",
    "\n",
    "$$\n",
    "P(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n}, \\mathbf{A})\n",
    "=\n",
    "P(\\mathbf{A} \\mid \\mathbf{Z}_{-n}, \\mathbf{X}_{-n})\n",
    "$$\n",
    "\n",
    "... lets a try a few different things.  Let's try swapping $\\mathbf{A}$ and $\\mathbf{X}_{-n}$:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n}, \\mathbf{A})\n",
    "=\n",
    "\\frac{P(\\mathbf{A} \\mid \\mathbf{Z}_{-n}, \\mathbf{X}_{-n}) \\, P(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n})}\n",
    "  {P(\\mathbf{A} \\mid \\mathbf{Z}_{-n})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: revising graphical models\n",
    "\n",
    "At this point, I realized that my knowledge of graphical models sucks, and started thinking them through from the start.  So, here goes.\n",
    "\n",
    "### Meaning of removing conditioning variable?\n",
    "\n",
    "So, the first thing I realized I didnt know is, what does it mean $P(z_nk=1 \\mid \\mathbf{Z}_{-nk})$?  How is this different / not different to $P(z_{nk} = 1 \\mid \\mathbf{Z}_{-nk}, \\mathbf{X})$?\n",
    "\n",
    "So, what I think is: for the network above, we can attempt to calculate $P(z_{nk} \\mid \\mathbf{Z}_{-nk})$, without a conditional dependence on $\\mathbf{X}$.  It's a valid expression, and it has some possible answers, depending plausibly on different assumptions.  But basically, it means \"if I know the other latent values, for sure, or at least, think I know them, and I dont know anything about the observed data $\\mathbf{X}$, whats the probability for different values of $z_{nk}$?\".\n",
    "\n",
    "So, we could obtain an expression for $P(z_{nk} \\mid \\mathbf{Z}_{-nk})$.  It is/would be a perfectly valid probability distribution.  It would tell us the probability distribution over $z_{nk}$, given that we know nothing about the observed data.\n",
    "\n",
    "But sometimes/often, we want the values of the observed data to influence the probability distribution of $z_{nk}$.  So, we want $P(z_{nk} \\mid \\mathbf{X}, \\mathbf{Z}_{-nk})$.  This doesnt mean that $P(z_{nk} \\mid \\mathbf{Z}_{-nk})$ doesnt exist, and note that $P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\mathbf{X})$ most certainly is not the same thing was $P(z_{nk} \\mid \\mathbf{Z}_{-nk})$.  This was my mistake for a while: thinking that $P(z_{nk} \\mid \\mathbf{Z}_{-nk})$ is either identical to $P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\mathbf{X})$, or better than it, because it depends on fewer things.  In fact, having the dependency on $\\mathbf{X}$ means that if we do know $\\mathbf{X}$, or have a good estimate of it, then we will have a more precise/accurate/something estimate for the probability distribution of $z_{nk}$.  So, we actually in fact want to have the dependence on $\\mathbf{X}$ in the probability distribution, in the case that we know $\\mathbf{X}$.  And in this case, we do know $\\mathbf{X}$; its one of the few/only things we actually know.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So when we write:\n",
    "\n",
    "$$P(z_{nk} = 1 \\mid \\mathbf{Z}_{-nk}, \\mathbf{X})\n",
    "= \n",
    "\\frac{\n",
    "   P(\\mathbf{X} \\mid z_{nk} = 1, \\mathbf{Z}_{-nk})\\, P(z_{nk} = 1 \\mid \\mathbf{Z}_{-nk})\n",
    "}\n",
    "{\n",
    "   P(\\mathbf{X} \\mid \\mathbf{Z}_{-nk})\n",
    "}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "= \n",
    "\\frac{\n",
    "   P(\\mathbf{X} \\mid \\mathbf{Z})\\, P(z_{nk} = 1 \\mid \\mathbf{Z}_{-nk})\n",
    "}\n",
    "{\n",
    "   P(\\mathbf{X} \\mid \\mathbf{Z}_{-nk})\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...the term $P(z_{nk} \\mid \\mathbf{Z}_{-nk})$, on the right hand side, is the probability distribution of $z_{nk}$, in the case that we know nothing about the observed data, and only know the values of the other latent variables.  I dont think it's a prior on $z_{nk}$, since it does depend on some other data values, albeit latent ones.  So, I think it's something like a 'posterior' on $z_{nk}$, given we know the other $z$ values.  I'm not very sure on this point yet though :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term $P(\\mathbf{X} \\mid \\mathbf{Z})$ is the 'likelihood' term, I think.  It is the probability of the observed data, given the latent variables.  I'm not entirely sure if 'likelihood' term does/doesnt require marginalization over latent variables.  Anyway, here, this particular term means, \"for each of many values of the latent variables $\\mathbf{Z}$, here is the corresponding probability distribution over $\\mathbf{X}$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I guess, thinking this through, we could have something like, the latent variable is what the weather will be tomorrow (is unobserved, at the moment, I'm not sure if that's an acceptable thing to use as a 'latent variable'? but it sounds plausible-ish?).  And then $\\mathbf{X}$ could be how far we will swim, in the sea.  So, if it's raining, we might not swim very far (or at all).  But if it's sunny, how far we swim might depend on a whole bunch of other things, like whether it's windy, what time we go to sleep tonight, and so on.\n",
    "\n",
    "So, $P(\\mathbf{X})$ is the probability distribution over how far we will swim tomorrow.  $P(\\mathbf{X} \\mid \\mathbf{Z})$ is the probability distribution given we do in fact know the weather.  We could probably also have things like $P(\\mathbf{X} \\mid \\mathbf{Z}, \\mathrm{cloudy}, \\mathrm{slept\\_late})$ etc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"proportional to\"?\n",
    "\n",
    "So, anyway, the challenge, a challenge, I'm facing is, how do we decide which of the probability distributions on the right hand side are just constants of normalization, and can be replaced with proportionality, $\\propto$, and which of them modify the shape of the probability distribution?\n",
    "\n",
    "The term $P(z_{nk}=1 \\mid \\mathbf{Z}_{-nk})$ is a probability distribution over $z_{nk}$.  So its logical I think to assume that this term can modify the shape of the overall probability distribution, and almost certainly will modify it, since a uniform probability distribution would be infinitely improbable everywhere (not technical terms I think), and probably quite hard to deal with/design.\n",
    "\n",
    "But the question is, why is $P(\\mathbf{X} \\mid \\mathbf{Z}_{-nk})$ removed as 'just a constant of normalization', whilst $P(\\mathbf{X} \\mid z_{nk} = 1, \\mathbf{Z}_{-nk})$ is not?  After all, they are both a probability distribution over $\\mathbf{X}$, and $\\mathbf{X}$ is fixed, its in the list of conditional dependencies, in the left hand expression $P(z_{nk} = 1 \\mid \\mathbf{Z}_{-nk}, \\mathbf{X})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing I note is that in $P(\\mathbf{X} \\mid \\mathbf{Z}_{-nk})$, both the dependency, and the term we want the probability distribution of, are both in the dependencies of the left-hand term.  Whereas in $P(\\mathbf{X} \\mid z_nk = 1, \\mathbf{Z}_{-nk})$, the term we are obtaining the probability of, in the left hand side of the equation, ie $z_{nk}$ does exist in the dependencies, even if its not what the probability distribution itself is over, ie $\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps it is something like, whilst $P(\\mathbf{X} \\mid \\mathbf{Z})$ is a probability distribution over $\\mathbf{X}$, meaning for example that if one was to integrate over $\\mathbf{X}$, the integral would be exactly $1$, the result of the $P(\\dots)$ expression is just a number, that will vary, as a function over $z_{nk}$.  For the conditional direction bit, if we integrated over $\\mathbf{Z}$ it probably would not integrate to $1$, but that doesnt mean the likelihood probability is not a number, that varies with $z_{nk}$.  Let's look at a concrete example, to help think this through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example\n",
    "\n",
    "Lets say we have:\n",
    "\n",
    "$$p(x \\mid z=1) = \\mathcal{N}(3, 0.1)$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$p(x \\mid z=0) = \\mathcal{N}(-1, 0.2)$$\n",
    "\n",
    "(And here the support of $z$ is $z \\in \\{0, 1\\}$).  We can draw some points from these two distributions.  First the two probability distributions ,for each value of $z$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAFkCAYAAABfHiNRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nACIQAAAAEQgAAIAIBAAAQNL/B91HF8OcurR0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22d74a0400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import math\n",
    "\n",
    "dists = [\n",
    "    {'mu': -1, 'variance': 0.2},\n",
    "    {'mu': 3, 'variance': 0.4}\n",
    "]\n",
    "\n",
    "X = np.arange(-5, 5, 0.1)\n",
    "for v, dist in enumerate(dists):\n",
    "    plt.plot(X, scipy.stats.norm.pdf(X, loc=dist['mu'], scale=dist['variance']), label='z=%s' % v)\n",
    "#     plt.ylim(0, 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and some points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAFkCAYAAACNTikJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDBmSJKmJ/w9BUYLz7WHizAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22d7443358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import math\n",
    "\n",
    "num_samples = 20\n",
    "for v, dist in enumerate(dists):\n",
    "    X = np.zeros((num_samples,), dtype=np.float32)\n",
    "    for i in range(num_samples):\n",
    "        X[i] = scipy.stats.norm.rvs(loc=dist['mu'], scale=dist['variance'])\n",
    "    plt.plot(X, [0] * len(X), '|', label='z=%s' % v)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could say that $z$ is distributed for example as:\n",
    "\n",
    "$$\n",
    "P(z = 1) = 0.8\n",
    "$$\n",
    "\n",
    "And now we sample $z$ too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAFkCAYAAACNTikJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22d7538278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import math\n",
    "\n",
    "z_prob_1 = 0.8\n",
    "\n",
    "num_samples = 40\n",
    "X = np.zeros((num_samples,), dtype=np.float32)\n",
    "for i in range(num_samples):\n",
    "    v = np.random.choice(2, p=[1.0 - z_prob_1, z_prob_1])\n",
    "    dist = dists[v]\n",
    "    X[i] = scipy.stats.norm.rvs(loc=dist['mu'], scale=dist['variance'])\n",
    "plt.plot(X, [0] * len(X), '|')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've lost the color here, though we could keep it, since during the sampling, we do in fact know the sampled values of $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior over $\\mathbf{z}$\n",
    "\n",
    "Then, the probability of a set of samples $\\mathbf{x}$ here, where $\\mathbf{x}$ is a vector, of length $N$, is... Well, we should assume we know $\\mathbf{z}$, the $N$-length vector of latent values of $z$.  And lets call $P(z=1)$ $\\alpha$.  Sooo... we can write down a probability distribution over $\\mathbf{z}$ quite easily:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z} \\mid \\alpha)\n",
    "= \\prod_{i=1}^N\n",
    "\\alpha^{z_i}\n",
    "(1-\\alpha)^{(1 - z_i)}\n",
    "$$\n",
    "\n",
    "- each of the examples where $z$ is $0$, the term in $\\alpha$ evaluates to $1$, and the probability evaluates to $(1 - \\alpha)$\n",
    "- and each of the examples where $z$ is $1$, the term in $(1 - \\alpha)$ evaluates to $1$, and the probability evaluates to $\\alpha$\n",
    "- ... and all of these probabilities are multiplied together\n",
    "\n",
    "This probability distribution tells us the probability for a vector in $z$, in the absence of any knowledge of the data, but given that we know the form of the distribution, and the value of $\\alpha$.\n",
    "\n",
    "I *guess* this is a 'prior' on $\\mathbf{z}$, since it is only based on knowing the form of the distribution, and a hyper-parameter, $\\alpha$.\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "Now, let's say we are given a bunch of points $\\mathbf{x}$, and let's say we want to write down the probability distribution of $\\mathbf{x}$ given $\\mathbf{z}$, which I think is the likelihood.  Let's have the distributions:\n",
    "\n",
    "$$\n",
    "p(x \\mid z=1, \\mu_1, \\sigma_1) = \\mathcal{N}(x; \\mu_1, \\sigma_1)\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "p(x \\mid z=0, \\mu_0, \\sigma_0) = \\mathcal{N}(\\mu_0, \\sigma_0)\n",
    "$$\n",
    "\n",
    "So, for one data point:\n",
    "\n",
    "$$\n",
    "p(x \\mid z, \\mathbf{\\mu}, \\mathbf{\\sigma}) =\n",
    "\\mathcal{N}(\\mu_0, \\sigma_0)^{(1 - z)}\n",
    "\\mathcal{N}(\\mu_1, \\sigma_1)^{(z)}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "- if $z$ is $1$, then $(1 - z)$ evaluates to $0$, meaning that term drops out, and we are left with the term in $\\mathcal{N}(x; \\mu_1, \\sigma_1)$\n",
    "- similarly, if $z$ is $0$, then $(z)$ evaluates to $0$, meaning that term drops out, and we are left with the term in $\\mathcal{N}(x; \\mu_0, \\sigma_0)$\n",
    "\n",
    "For the entire dataset, we have:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{\\mu}, \\mathbf{\\sigma}) =\n",
    "\\prod_{i=1}^N\n",
    "\\mathcal{N}(x_i; \\mu_0, \\sigma_0)^{(1 - z_i)}\n",
    "\\mathcal{N}(x_i; \\mu_1, \\sigma_1)^{(z_i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior for $\\mathbf{z}$\n",
    "\n",
    "Next, imagine we want to estimate the vector $z$ given $x$, so we want:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{z} \\mid \\mathbf{x})\n",
    "$$\n",
    "\n",
    "But let's nuance this slightly: let's say we know $\\mu_0$, $\\mu_1$, $\\sigma_0$, $\\sigma_1$, so we want:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{z} \\mid \\mathbf{x}, \\mathbf{\\sigma}, \\mathbf{\\mu})\n",
    "$$\n",
    "\n",
    "Using Bayes, and treating $\\mathbf{\\sigma}$ and $\\mathbf{\\mu}$ as given constants, this is:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{z} \\mid \\mathbf{x}, \\mathbf{\\sigma}, \\mathbf{\\mu})\n",
    "=\n",
    "\\frac{p(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{\\sigma}, \\mathbf{\\mu})\\, P(\\mathbf{z} \\mid \\mathbf{\\sigma}, \\mathbf{\\mu})}\n",
    "   {p(\\mathbf{x} \\mid \\mathbf{\\sigma}, \\mathbf{\\mu})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at each of these terms:\n",
    "\n",
    "#### $p(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{\\sigma}, \\mathbf{mu})$\n",
    "\n",
    "We have $p(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{\\sigma}, \\mathbf{mu})$ from above.  It's the likelihood term.  It is a function of both $\\mathbf{x}$ and $\\mathbf{z}$.  Although the $\\mathbf{z}$ is a conditional constant, since the conditional probability gives the distribution of $\\mathbf{x}$ for many possible values of $\\mathbf{z}$, any in the support of the distribution.  So it's a function of both $\\mathbf{z}$ and $\\mathbf{x}$.\n",
    "\n",
    "#### $P(\\mathbf{z} \\mid \\sigma, \\mu)$\n",
    "\n",
    "This is the prior probability of $\\mathbf{z}$.  Actually we are probably missing an $\\alpha$, in the conditional bit.  It should probalby look like:\n",
    "\n",
    "$$P(\\mathbf{z} \\mid \\alpha)$$\n",
    "\n",
    "(and with $\\sigma$ and $\\mu$ removed, since they have no effect on the prior probability of $\\mathbf{z}$, which we can see by re-writing down the equation:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z} \\mid \\alpha)\n",
    "= \\prod_{i=1}^N\n",
    "\\alpha^{z_i}\n",
    "(1-\\alpha)^{(1 - z_i)}\n",
    "$$\n",
    "\n",
    "It is a function only of $\\mathbf{z}$ and on $\\alpha$.  We are finding the probability distribution of $\\mathbf{z}$, conditional on $\\alpha$, and both of these terms appear in the equation.  The probability distribution of $\\mathbf{z}$ varies with $\\alpha$.\n",
    "\n",
    "Since this equation is conditional on $\\alpha$, so, the conditional probability for $\\mathbf{z}$ given $\\mathbf{x}$, $\\sigma$, $\\mu$ should also be conditional on $\\alpha$, and should look like:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{z} \\mid \\mathbf{x}, \\mathbf{\\sigma}, \\mathbf{\\mu}, \\alpha)\n",
    "=\n",
    "\\frac{p(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{\\sigma}, \\mathbf{\\mu}, \\alpha)\\, P(\\mathbf{z} \\mid \\mathbf{\\alpha})}\n",
    "   {p(\\mathbf{x} \\mid \\mathbf{\\sigma}, \\mathbf{\\mu})}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the probability distribution for $\\mathbf{z}$ is shaped by both the term $P(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{\\sigma}, \\mathbf{\\mu}, \\alpha)$ and by the term $P(\\mathbf{z} \\mid \\alpha)$.  What about the denominator term?\n",
    "\n",
    "#### $p(\\mathbf{x} \\mid \\sigma, \\mu)$\n",
    "\n",
    "This is the prior distribution of $\\mathbf{x}$, given $\\sigma$, and $\\mu$.  But we dont have this directly.  But we can get it by introducing a prior over $\\alpha$, and then marginalizing over $\\alpha$.  Without the prior, we have no idea what the distribution over $\\mathbf{z}$ will be, so how can we calculate the probabilty of $\\mathbf{x}$?  We could state that the probability of any $z$ is $0.5$, but that is in fact a prior.\n",
    "\n",
    "### Graphical models\n",
    "\n",
    "Let's draw a graphical model:\n",
    "\n",
    "![](img/simple_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, looking at this model, if $\\alpha$ is observed/known, we can write down the probaiblity of $\\mathbf{z}$, given $\\alpha$, ie:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z} \\mid \\alpha)\n",
    "= \\prod_{i=1}^N\n",
    "\\alpha^{z_i}\n",
    "(1-\\alpha)^{(1 - z_i)}\n",
    "$$\n",
    "\n",
    "Corresponding diagram, with the variable whose probability we are writing down in red outline, and the observed variables filled in grey:\n",
    "\n",
    "![](img/alpha_observed2.png)\n",
    "\n",
    "If $\\mathbf{z}$ is observed/known/estimated, we can write down the probability of $\\mathbf{x}$, given $\\mathbf{z}$, $\\mathbf{\\sigma}$ and $\\mathbf{\\mu}$, ie:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{\\mu}, \\mathbf{\\sigma}) =\n",
    "\\prod_{i=1}^N\n",
    "\\mathcal{N}(x_i; \\mu_0, \\sigma_0)^{(1 - z_i)}\n",
    "\\mathcal{N}(x_i; \\mu_1, \\sigma_1)^{(z_i)}\n",
    "$$\n",
    "\n",
    "Corresponding diagram:\n",
    "\n",
    "![](img/z_observed2.png)\n",
    "\n",
    "If neither $\\mathbf{z}$ or $\\mathbf{x}$ are observed, but we know $\\alpha$, $\\sigma$ and $\\mu$, then we can write down the joint probability of $\\mathbf{z}$ and $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}, \\mathbf{x} \\mid \\alpha, \\mathbf{\\omega}, \\mathbf{\\mu})\n",
    "=\n",
    "\\prod_{i=1}^N\n",
    "\\mathcal{N}(x_i; \\mu_0, \\sigma_0)^{(1 - z_i)}\n",
    "\\mathcal{N}(x_i; \\mu_1, \\sigma_1)^{(z_i)}\n",
    "\\cdot\n",
    "\\prod_{i=1}^N\n",
    "\\alpha^{z_i}\n",
    "(1-\\alpha)^{(1 - z_i)}\n",
    "$$\n",
    "\n",
    "Corresponding diagram:\n",
    "\n",
    "![](img/joint_x_z.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dont think we can directly write down $p(\\mathbf{x} \\mid \\alpha, \\sigma, \\mu)$, because $\\mathbf{x}$ has no prior as such, unless we bring $\\mathbf{z}$ in, and marginalize that.  ie, I dont think we can directly write down an expression for:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\alpha, \\sigma, \\mu)\n",
    "$$\n",
    "\n",
    "... but I think we can write:\n",
    "\n",
    "$$\n",
    "(\\mathbf{x} \\mid \\alpha, \\sigma, \\mu) =\n",
    "\\int_{\\mathbf{z}} p(\\mathbf{x} \\mid \\mathbf{z}, \\sigma, \\mu)\n",
    "\\,\n",
    "p(\\mathbf{z} \\mid \\alpha)\n",
    "\\,\n",
    "d\\mathbf{z}\n",
    "$$\n",
    "\n",
    "The observed variables correspond to the joint probability diagram above, just now we only want the probability of $\\mathbf{x}$, ie something like:\n",
    "\n",
    "![](img/marginalize_z.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginalization over $\\mathbf{z}$\n",
    "\n",
    "I guess the marginalization wont work nicely, because I guess the prior on $\\mathbf{z}$ is not conjugate to the Gaussian?  Let's try, and see what happens:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\alpha, \\sigma, \\mu) =\n",
    "\\int_{\\mathbf{z}}\n",
    "p(\\mathbf{x} \\mid \\mathbf{z}, \\sigma, \\mu)\n",
    "\\,\n",
    "p(\\mathbf{z} \\mid \\alpha)\n",
    "\\,\n",
    "d\\mathbf{z}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=\n",
    "\\int_{\\mathbf{z}}\n",
    "\\prod_{i=1}^N\n",
    "\\mathcal{N}(x_i; \\mu_0, \\sigma_0)^{(1 - z_i)}\n",
    "\\mathcal{N}(x_i; \\mu_1, \\sigma_1)^{(z_i)}\n",
    "\\cdot\n",
    "\\prod_{i=1}^N\n",
    "\\alpha^{z_i}\n",
    "(1-\\alpha)^{(1 - z_i)}\n",
    "\\,\n",
    "d\\mathbf{z}\n",
    "$$\n",
    "\n",
    "Note that this corresponds to marginalization of the joint probability of $\\mathbf{x}$ and $\\mathbf{z}$, as it should.\n",
    "\n",
    "The latent variables, $z_i$, are independent, given $\\alpha$.  And the individual data samples $x_i$ are independent, given $z_i$, $\\sigma$ and $\\mu$.  Actually, since the $z_i$ are independent given $\\alpha$, this means I think that the $x_i$ values are independent given only $\\alpha$, $\\sigma$ and $\\mu$?\n",
    "\n",
    "So, we can move the product outside of hte integral, I think?\n",
    "\n",
    "$$\n",
    "=\n",
    "\\prod_{i=1}^N\n",
    "\\int_{z_i}\n",
    "\\mathcal{N}(x_i; \\mu_0, \\sigma_0)^{(1 - z_i)}\n",
    "\\mathcal{N}(x_i; \\mu_1, \\sigma_1)^{(z_i)}\n",
    "\\alpha^{z_i}\n",
    "(1-\\alpha)^{(1 - z_i)}\n",
    "\\,\n",
    "dz_i\n",
    "$$\n",
    "\n",
    "Seems to work ok?  There is nothing inside the integral that needs knowledge of anything other than $x_i$ or $z_i$ in the $\\mathbf{x}$ and $\\mathbf{z}$ vectors, I think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's expand the normal distribution pdfs:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\prod_{i=1}^N\n",
    "\\int_{z_i}\n",
    "\\left(\n",
    "\\frac{1}{\\sqrt{2\\sigma_0^2\\pi}}\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\frac{(x_i - \\mu_0}\n",
    "    {\\sigma_0^2}\n",
    "\\right)\n",
    "\\right)^{(1 - z_i)}\n",
    "\\left(\n",
    "\\frac{1}{\\sqrt{2\\sigma_1^2\\pi}}\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\frac{(x_i - \\mu_1}\n",
    "    {\\sigma_1^2}\n",
    "\\right)\n",
    "\\right)^{z_i}\n",
    "\\alpha^{z_i}\n",
    "(1-\\alpha)^{(1 - z_i)}\n",
    "\\,\n",
    "dz_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, following how things worked in the Griffiths and Ghahramani tutorial, for general mixture models, let's define $m_0$ and $m_1$ to be the count of $\\sum_{i=1}^N \\delta(z_i, 0)$ and $\\sum_{i=1}^N \\delta(z_i, 1)$ respectively,where $\\delta(i,j)$ is Kronecker delta.  Then we have:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}\n",
    "  {(2\\pi)^{n} \\sigma_0^{2m_0} \\sigma_1^{2m_1}}\n",
    "\\prod_{i=1}^N\n",
    "\\int_{z_i}\n",
    "\\left(\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\frac{(x_i - \\mu_0}\n",
    "    {\\sigma_0^2}\n",
    "\\right)\n",
    "\\right)^{(1 - z_i)}\n",
    "\\left(\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\frac{(x_i - \\mu_1}\n",
    "    {\\sigma_1^2}\n",
    "\\right)\n",
    "\\right)^{z_i}\n",
    "\\alpha^{z_i}\n",
    "(1-\\alpha)^{(1 - z_i)}\n",
    "\\,\n",
    "dz_i\n",
    "$$\n",
    "\n",
    "Hmmm.\n",
    "\n",
    "Actually...\n",
    "\n",
    "Each $z_i$ is discrete.  Can only have one of the values $z_i \\in \\{0, 1\\}$.  So we can replace the integrals by sums?\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}\n",
    "  {(2\\pi)^{n} \\sigma_0^{2m_0} \\sigma_1^{2m_1}}\n",
    "\\prod_{i=1}^N\n",
    "\\sum_{z=0}^{1}\n",
    "\\left(\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\frac{(x_i - \\mu_0}\n",
    "    {\\sigma_0^2}\n",
    "\\right)\n",
    "\\right)^{(1 - z)}\n",
    "\\left(\n",
    "\\exp \\left(\n",
    "  - \\frac{1}{2}\n",
    "  \\frac{(x_i - \\mu_1}\n",
    "    {\\sigma_1^2}\n",
    "\\right)\n",
    "\\right)^{z}\n",
    "\\alpha^{z}\n",
    "(1-\\alpha)^{(1 - z)}\n",
    "$$\n",
    "\n",
    "... and then split this sum/product into two parts, one for $z=0$ and one for $z=1$:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{1}\n",
    "  {(2\\pi)^{n} \\sigma_0^{2m_0} \\sigma_1^{2m_1}}\n",
    "\\prod_{i=1}^N\n",
    "\\left(\n",
    "    (1-\\alpha)\n",
    "    \\exp \\left(\n",
    "      - \\frac{1}{2}\n",
    "      \\frac{(x_i - \\mu_0}\n",
    "        {\\sigma_0^2}\n",
    "    \\right)\n",
    "+\n",
    "   \\alpha\n",
    "    \\exp \\left(\n",
    "      - \\frac{1}{2}\n",
    "      \\frac{(x_i - \\mu_1}\n",
    "        {\\sigma_1^2}\n",
    "    \\right)\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the term inside the product is the sum of the probability of $z$ being 0, with the probability of a particular data point $x_i$, given $z$ is 0.  And then we add also the same thing, but for $z = 1$.  Does it makes sense to add probabilities?  In this case, we are replacing the integral for marginalization, so that seems reasonable-ish?\n",
    "\n",
    "We got rid of the integral, and got a closed-form expression, in terms of $\\mathbf{x}$, for the probability distribution over $\\mathbf{x}$, assuming $\\mathbf{z}$ is unobserved, but given the prior over $\\mathbf{z}$, and observed $\\alpha$, as well as $\\mu$ and $\\sigma$.\n",
    "\n",
    "(Edit: oh, hmmm, somehow the $m_0$ and $m_1$ terms escaped, and these depend on $\\mathbf{z}$, so the above analysis is wrong/incomplete.  anywya....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of expressions so far\n",
    "\n",
    "So we have closed-form expressions for all of:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z} \\mid \\alpha)\n",
    "= \\prod_{i=1}^N\n",
    "\\alpha^{z_i}\n",
    "(1-\\alpha)^{(1 - z_i)}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\mathbf{z}, \\mathbf{\\mu}, \\mathbf{\\sigma}) =\n",
    "\\prod_{i=1}^N\n",
    "\\mathcal{N}(x_i; \\mu_0, \\sigma_0)^{(1 - z_i)}\n",
    "\\mathcal{N}(x_i; \\mu_1, \\sigma_1)^{(z_i)}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}, \\mathbf{x} \\mid \\alpha, \\mathbf{\\omega}, \\mathbf{\\mu})\n",
    "=\n",
    "\\prod_{i=1}^N\n",
    "\\mathcal{N}(x_i; \\mu_0, \\sigma_0)^{(1 - z_i)}\n",
    "\\mathcal{N}(x_i; \\mu_1, \\sigma_1)^{(z_i)}\n",
    "\\cdot\n",
    "\\prod_{i=1}^N\n",
    "\\alpha^{z_i}\n",
    "(1-\\alpha)^{(1 - z_i)}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\alpha, \\sigma, \\mu) =\n",
    "\\frac{1}\n",
    "  {(2\\pi)^{n} \\sigma_0^{2m_0} \\sigma_1^{2m_1}}\n",
    "\\prod_{i=1}^N\n",
    "\\left(\n",
    "    (1-\\alpha)\n",
    "    \\exp \\left(\n",
    "      - \\frac{1}{2}\n",
    "      \\frac{(x_i - \\mu_0}\n",
    "        {\\sigma_0^2}\n",
    "    \\right)\n",
    "+\n",
    "   \\alpha\n",
    "    \\exp \\left(\n",
    "      - \\frac{1}{2}\n",
    "      \\frac{(x_i - \\mu_1}\n",
    "        {\\sigma_1^2}\n",
    "    \\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "I think these terms have names:\n",
    "\n",
    "- the first one is the prior over $\\mathbf{z}$\n",
    "- the second is the likelihood, of $\\mathbf{x}$, given the latent variables $\\mathbf{z}$, and the parameters $\\mu$, $\\sigma$.\n",
    "- the third one is the joint, but I'm not sure if it has any name more specific than that?\n",
    "- the fourth one resembles a prior probability somewhat, since doesnt depend on any other variables.  I'm not sure if it can be called a prior though?  It is marginalized over the prior of $\\mathbf{z}$, so it's a derived distribution. But it's still a distribution, that doesnt depend on other variables, so ... ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior on $\\mathbf{z}$\n",
    "\n",
    "So, what we're missing is $P(\\mathbf{z} \\mid \\mathbf{x}, \\alpha, \\sigma, \\mu)$, which I think is the 'posterior for $\\mathbf{z}$'.  We can get it by using Bayes Rule to reverse the conditional direction between $\\mathbf{z}$ and $\\mathbf{x}$.  We note in passing that [wikipedia](https://en.wikipedia.org/wiki/Conditional_probability) is quite clear that the conditional probability is simply defined as the joint of two variables divided by the conditioning variable.  So, by multiplying by the original conditioning variable, and dividing by a new one, we can change the direction of conditioning:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(A,B)}{P(B)}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "P(B \\mid A) = \\frac{P(A,B)}{P(A)}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "P(B \\mid A) = \\frac{P(A \\mid B)\\,P(B)}{P(A)}\n",
    "$$\n",
    "\n",
    "No causality is stated or implied by the direction of conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "    \n",
    "$$\n",
    "P(\\mathbf{z} \\mid \\mathbf{x}, \\alpha, \\sigma, \\mu)\n",
    "=\n",
    "\\frac{P(\\mathbf{x} \\mid \\mathbf{z}, \\alpha, \\sigma, \\mu)\\, P(\\mathbf{z} \\mid \\alpha, \\sigma, \\mu)}\n",
    "  {P(\\mathbf{x} \\mid \\alpha, \\sigma, \\mu)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, bearing in mind that the expression for $P(\\mathbf{z} \\mid \\alpha, \\sigma, \\mu)$ is independent of $\\sigma$ and $\\mu$, and that $P(\\mathbf{z} \\mid \\mathbf{z}, \\alpha, \\sigma, \\mu)$ is independent of $\\alpha$, so we have:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{z} \\mid \\mathbf{x}, \\alpha, \\sigma, \\mu)\n",
    "=\n",
    "\\frac{P(\\mathbf{x} \\mid \\mathbf{z}, \\sigma, \\mu)\\, P(\\mathbf{z} \\mid \\alpha)}\n",
    "  {P(\\mathbf{x} \\mid \\alpha, \\sigma, \\mu)}\n",
    "$$\n",
    "\n",
    "We have closed-form expressions for each of these probability terms, so this means we also have a closed-form expression for the probability distribution of $\\mathbf{z}$ given $\\mathbf{x}$.\n",
    "\n",
    "### Proportionality, revisited\n",
    "\n",
    "As far as the proportional-to bit, which was the main thing I was pondering about originally, so basically anything on the right hand side which has terms in any $z_i$ will change the shape of the probability distribution.  Other terms will simply normalize the distribution up or down, so that it integrates to 1.\n",
    "\n",
    "The term in the denominator $P(\\mathbf{x} \\mid \\alpha, \\sigma, \\mu)$ has a closed-form expression that is independent of any $z_i$.  Its a function of $x_i$, $\\alpha$, $\\sigma$ and $\\mu$, but there is no $z_i$ in it, since we integrated/marginalized/summed that out.  Therefore, since it doesnt contain $z_i$, it wont affect the shape of the probability distribution of $z_i$, and is a constant of normalization.  Its value will change depending on the values of $\\mathbf{x}$, $\\alpha$, $\\sigma$ and $\\mu$, to ensure the integral of the distribution sums to 1.  However, it wont actually change the shape of the probability distribution, given specific values of $\\mathbf{x}$, $\\alpha$, $\\sigma$ and $\\mu$.\n",
    "\n",
    "So, it seems like a reasonable guideline/rule is:\n",
    "\n",
    "- if the variable whose distribution we want is contained as either the probability variable, or a conditioning variable, in any right hand probability term, then that term should be included, even for proportionality\n",
    "- otherwise, if we only want proportionality, the shape of the probability distribution, without the normalizing constnat, then we can drop any probability terms that dont contain the probabiliy variable from the left hand side, either as their probaiblity variable, or as a conditioning variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form of posterior on $\\mathbf{z}$\n",
    "\n",
    "Note that, for the probability distribution of $\\mathbf{z}$ conditional on $\\mathbf{x}$, that:\n",
    "\n",
    "- the numerator is just the joint, of $\\mathbf{z}$ and $\\mathbf{x}$, which makes sense, since the conditional of $\\mathbf{z}$ on $\\mathbf{x}$ is just the joint, divided by the probability of $\\mathbf{x}$, the unconditional probability of $\\mathbf{x}$\n",
    "- the denominator is 'just' the integral of the joint.  Which makes sense, since this is exactly how we normalize a probability distribution, ie:\n",
    "\n",
    "$$p(y) = \\frac{f(y)}{\\int f(y) \\,dy}$$\n",
    "\n",
    ".... for some function $f(\\cdot)$ of some variable $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerated Sampling Presentation, continued\n",
    "\n",
    "### Basic sampling (uncollapsed)\n",
    "\n",
    "So, let's revisit basic sampling again, start from the beginning.  This is the holistic view of basic sampling:\n",
    "\n",
    "![](img/basicsampling_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-draw it for each sampling step, in terms of 'observed', and 'target variable'.  Where 'observed' here will mean either 'directly observed', or 'sampled from Gibbs'.  So, for $\\mathbf{z}_w$ sampling we have:\n",
    "\n",
    "- $\\mathbf{A}$ is observed, from previous sampling\n",
    "- $\\mathbf{X}$ is observed, since it's our data\n",
    "- $\\mathbf{z}_{-w}$ is observed, since we use exchangeability to sample this first.  So, our observed diagram will look something like:\n",
    "\n",
    "![](img/acceleratedsampling_zw.png)\n",
    "\n",
    "I'm like 80% sure though, that when we sample from $\\mathbf{Z}_w$ we only need $\\mathbf{z}_{-w}$ and $\\mathbf{x}_{w}$, since those are observed variables, and are basically Markov with respect to anything on the other side of them (not sure if the 'Markov' terminology is used correctly here?  But I think the concept is correct-ish?)\n",
    "\n",
    "So, we will sample from:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{Z}_w \\mid \\mathbf{Z}_{-w}, \\mathbf{X}_w)\n",
    "$$\n",
    "\n",
    "... and then we can use exchangeability to flip $w$ and $-w$ around, or choose another $w$, etc.  And sample from the new $\\mathbf{z}_{w}$.\n",
    "\n",
    "But what about $\\mathbf{A}$?  Since $\\mathbf{x}$ is observed, there is no dependence of the sampling of $\\mathbf{z}$ on $\\mathbf{A}$?\n",
    "\n",
    "So, how about, maybe it's something like: technically we can sample $\\mathbf{z}$ from the probability expression above.  If we have it.  However, at the start, we dont actually have an expression for it: to get the probability of $\\mathbf{Z}$ given $\\mathbf{X}$, we need the joint of $\\mathbf{X}$ and $\\mathbf{Z}$, and the marginal probability of $\\mathbf{X}$.  But we have neither, yet, since both of those depend on $\\mathbf{A}$.  So, we'd have to marginalize over $\\mathbf{A}$ to get the above probability expression of $\\mathbf{Z}$ conditioned on $\\mathbf{X}$.  And marginalizing over $\\mathbf{A}$ is exactly what 'collapsing' a sampler does.\n",
    "\n",
    "So, at the base, we haven't marginalized over $\\mathbf{A}$, which means initially we can write down expressions for:\n",
    "\n",
    "$$P(\\mathbf{A})$$\n",
    "\n",
    "...given some prior on $\\mathbf{A}$, and any associated hyper-parameters\n",
    "\n",
    "and:\n",
    "\n",
    "$$P(\\mathbf{Z})$$\n",
    "\n",
    "... given some prior on $\\mathbf{Z})$ (ie, Indian Buffet Process), and any associated hyper-parameters (ie $\\alpha$).\n",
    "\n",
    "And we can write down an expression for:\n",
    "\n",
    "$$P(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A})$$\n",
    "\n",
    "... the probability of the observed data, conditonal on the latent variables.  Which I think is called the 'likelihood'.\n",
    "\n",
    "Presumably, for Gibbs sampling, we'd want to sample from something like:\n",
    "\n",
    "$$P(\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z})$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$P(\\mathbf{Z} \\mid \\mathbf{X}, \\mathbf{A})$$\n",
    "\n",
    "Let's look at each of these in turn, not rush through Doshi-Velez's paper, and get lost :-D  So, for $\\mathbf{A}$, we want:\n",
    "\n",
    "$$P(\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z})$$\n",
    "\n",
    "We could sample $\\mathbf{A}$ from the prior on $\\mathbf{A}$.  But if we do that, then we're sampling from a distribution whch doesnt take into account the data.  It is not conditional on $\\mathbf{X}$.  At the least, we'd want something like the joint of $\\mathbf{X}$ and $\\mathbf{A}$, divided by the marginalized probability of $\\mathbf{X}$.  Or, at least, somehow get $\\mathbf{X}$ into the conditioning variables.\n",
    "\n",
    "We can get the probability of $\\mathbf{A}$ conditioned on $\\mathbf{X}$ by applying Bayes Rule to the likelihood:\n",
    "\n",
    "$$P(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A})$$\n",
    "&nbsp;\n",
    "$$\n",
    "=\\frac\n",
    "   {p(\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z}) \\, p(\\mathbf{X} \\mid \\mathbf{Z})}\n",
    "   {p(\\mathbf{A} \\mid \\mathbf{Z})}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z})\n",
    "=\\frac\n",
    "   {P(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A})\\, p(\\mathbf{A} \\mid \\mathbf{Z})}\n",
    "   {p(\\mathbf{X} \\mid \\mathbf{Z})}\n",
    "$$\n",
    "\n",
    "Looking at this:\n",
    "\n",
    "- $P(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A})$ is the likelihood. We have this\n",
    "- $p(\\mathbf{X} \\mid \\mathbf{Z})$ is the marginalized probability of $\\mathbf{X}$ given $\\mathbf{Z}$.  Getting this probably involves some integration...\n",
    "- $p(\\mathbf{A} \\mid \\mathbf{Z})$ is kind of mysterious.  Looks like we'd need to marginalize over $\\mathbf{X}$ to get this?  That sounds tricky...\n",
    "\n",
    "Let's try flipping some other variables, using Bayes, and work the other way around, from what we want, trying to find stuff we know:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z})\n",
    "=\\frac\n",
    "   {P(\\mathbf{Z} \\mid \\mathbf{X}, \\mathbf{A})\\, p(\\mathbf{A} \\mid \\mathbf{X})}\n",
    "   {p(\\mathbf{Z} \\mid \\mathbf{X})}\n",
    "$$\n",
    "\n",
    "Hmmmm.  So... this is not 'accelerated sampling', this is standard non-collapsed sampling.  So, let's find a paper about uncollapsed Gibbs sampling on an IBP.  Per the Doshi-Velez 'accelerated sampling' paper, the Griffiths and Ghahramani tutorial already covers this :-P  So, let's look back at the Griffiths and Ghahramani tutorial.\n",
    "\n",
    "... Ok, so... initially I found nothing, but I noticed that Doshi-Velez's paper writes down $p(\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z})$ for the case of linear-Gaussian model , equations (6) in the paper:\n",
    "\n",
    "$$\n",
    "\\mu^A = \\left(\n",
    "  \\mathbf{Z}^T \\mathbf{Z} + \\frac{\\sigma_x^2}{\\sigma_a^2}\\mathbf{I}\n",
    "\\right)^{-1}\n",
    "\\mathbf{Z}^T \\mathbf{X}\n",
    "$$\n",
    "and variance:\n",
    "\n",
    "$$\n",
    "\\Sigma^A = \\sigma_x^2\n",
    "\\left(\n",
    "  \\mathbf{Z}^T \\mathbf{Z} + \\frac{\\sigma_x^2}{\\sigma_a^2}\\mathbf{I}\n",
    "\\right)^{-1}\n",
    "$$\n",
    "\n",
    "Then, looking through the section on the linear-Gaussian model in the Griffiths and Ghahramani tutorial, section 5.3 states the following expression for the expectation of $\\mathbf{A}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z}]\n",
    "=\n",
    "\\left(\n",
    "  \\mathbf{Z}^T \\mathbf{Z}\n",
    "  +\n",
    "  \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}\n",
    "\\right)^{-1}\n",
    "  \\mathbf{Z}^T\\mathbf{X}\n",
    "$$\n",
    "... which matches the expression for the mean of $\\mathbf{A}$ in the Doshi-Velez paper.\n",
    "\n",
    "As far as how to derive this, earlier in the Griffiths and Ghahramani tutorial, section 5.1, expressions are written down for:\n",
    "\n",
    "- $p(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A}, \\sigma_X)$ (ie, the likelihood, I think), and\n",
    "- $p(\\mathbf{A} \\mid \\sigma_A)$ (ie the prior, on $\\mathbf{A}$)\n",
    "\n",
    "Then, these are multiplied together, to give the joint of $\\mathbf{X}$ and $\\mathbf{A}$, given $\\mathbf{Z}$, which, just underneath equation 20 in the paper, gives an exponentiated expression involving the trace of:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma_X^2}\n",
    "(\\mathbf{X}^T(\\mathbf{I} - \\mathbf{Z}\\mathbf{M}\\mathbf{Z}^T)\\mathbf{X} + (\\mathbf{M}\\mathbf{Z}^T - \\mathbf{X}  - \\mathbf{A})^T (\\sigma_X^2 \\mathbf{M})^{-1} (\\mathbf{M}\\mathbf{Z}^T\\mathbf{X} - \\mathbf{A})\n",
    "$$\n",
    "\n",
    "... where:\n",
    "\n",
    "$$\n",
    "\\mathbf{M} = (\\mathbf{Z}^T\\mathbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I})^{-1}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... so the mean of this Gaussian, for $\\mathbf{A}$, without thinking yet about conditioning, or marginalizing, etc, is given by:\n",
    "\n",
    "$$\n",
    "(\\mathbf{M}^T\\mathbf{X} - \\mu_A) = 0\n",
    "$$\n",
    "Therefore:\n",
    "$$\n",
    "\\mu_A = (\\mathbf{Z}^T\\mathbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I})^{-1}\\mathbf{Z}^T\\mathbf{X}\n",
    "$$\n",
    "\n",
    "... which matches the mean above, so seems plausibly this expression is closely related to what we need.  This expression is from inside the exponential trace of the joint of $\\mathbf{A}$ and $\\mathbf{X}$:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X}, \\mathbf{A} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)\n",
    "$$\n",
    "\n",
    "If we want to condition on $\\mathbf{X}$, we need to divide by the probability of $\\mathbf{X}$, conditioned only on $\\mathbf{Z}$, $\\sigma_X$, and/or $\\sigma_A$, but not over $\\mathbf{A}$. ie:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z}, \\sigma_X, \\sigma_A)\n",
    "= \\frac{p(\\mathbf{X}, \\mathbf{A} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)}\n",
    "  {p(\\mathbf{X} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)}\n",
    "$$\n",
    "\n",
    "However, the denominator, $p(\\mathbf{X} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)$ does not contain the variable we are obtaining hte probaiblity distribution of, ie $\\mathbf{A}$, neither as its probability varaible, nor as a conditioning variable.  Therefore the denominator is just a normalizing constant, and does not change the shape of the probability distribution in $\\mathbf{A}$. Concretely, this means the denominator does not change the mean of the distribution.  We can remove the denominator, change to proportionality, and the mean of $\\mathbf{A}$ will be the mean of whats left.  Whats left is the Gaussian from earlier, with the mean as stated earlier.  As proportionality, it looks like:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z}, \\sigma_X, \\sigma_A)\n",
    "\\propto p(\\mathbf{X}, \\mathbf{A} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)\n",
    "$$\n",
    "\n",
    "... which has the mean required, as earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So, concretely, expressed at a high-ish level, without going into the detailed Gaussians and so on, to get the distribution of $\\mathbf{A}$ conditional on $\\mathbf{X}$ and $\\mathbf{Z}$, what we're doing is:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z}, \\sigma_X, \\sigma_A)\n",
    "= \\frac{p(\\mathbf{X}, \\mathbf{A} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)}\n",
    "  {p(\\mathbf{X} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "p(\\mathbf{A} \\mid \\mathbf{X}, \\mathbf{Z}, \\sigma_X, \\sigma_A)\n",
    "= \\frac{ p(\\mathbf{X} \\mid \\mathbf{A}, \\mathbf{Z}, \\sigma_X) \\, p(\\mathbf{A} \\mid \\sigma_A)}\n",
    "  {p(\\mathbf{X} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\propto\n",
    "p(\\mathbf{X} \\mid \\mathbf{A}, \\mathbf{Z}, \\sigma_X) \\, p(\\mathbf{A} \\mid \\sigma_A)\n",
    "$$\n",
    "\n",
    ".... so its basically not really using Bayes Rules, just directly using the relationship between the joint and the conditional.  Since Bayes Rule is just derived from the same relationship, but is less general, I might just try going via the joint in hte future, rahter than using Bayes Rule?\n",
    "\n",
    "Also, note that this starts to answer my question from earlier, from just before starting to read Doshi-Velez's presentation: ie, how to handle the case when we want to sample from a distribution that is not normalized. A partial/complete answer to this question is that if our distribution is a Gaussian, we dont actually need the normalization, we can just read the mean off directly, from the exponentiated expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however that this does imply a constraint on the distributions we can use for the likelihood of $\\mathbf{X}$ and the prior of $\\mathbf{A}$: their product should form a distribution, in $\\mathbf{A}$, whose shape is sufficient for us to be able to sample from, without needing to know its normalization.  I guess there are not so many distributions that meet this requirement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, going back to the graphical model, I guess that both $\\mathbf{X}$ and $\\mathbf{Z}$ are observed in the expression for sampling from $\\mathbf{A}$?  So, the diagram for sampling from $\\mathbf{A}$ will look like? :\n",
    "\n",
    "![](img/simple_samplea.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So thats strange.  I thought it was the case that when writing down probabilities from such graphs, one doesnt need to proceed further than any observed variables.  But I guess I'm wrong/have misunderstood this point somewhat?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapsed sampling\n",
    "\n",
    "Collapsed sampling was already covered in the Ghahramani tutorial.  We marginalize over $\\mathbf{A}$, and can directly sample from $P(\\mathbf{Z} \\mid \\mathbf{X})$ and/or from $P(z_nk \\mid \\mathbf{Z}_{-nk}, \\mathbf{X})$.  In the Griffiths and Ghahramani tutorial, the expression for $P([\\mathbf{Z}])$ is obtained, which is:\n",
    "\n",
    "$$\n",
    "P([\\mathbf{Z}] \\mid \\alpha)\n",
    "=\n",
    "\\frac{\\alpha^{K_+}}\n",
    "  {\\prod_{h=1}^{2^N - 1}}\n",
    "\\cdot\n",
    "\\exp(-\\alpha H_N\n",
    ")\n",
    "\\cdot\n",
    "\\prod_{k=1}^{K_+}\n",
    "\\frac{(N - m_k)!(m_k-1)!}\n",
    "  {N!}\n",
    "$$\n",
    "\n",
    "Should $N$ etc be in the conditioning variables?  $N$, $m_k$, $H_N$, $K^+$ are all properties that can be read directly from any specific assignment of $\\mathbf{Z}$, so they are perhaps implied by $\\mathbf{Z}$, and thus implicitly a part of $\\mathbf{Z}$ itself?\n",
    "\n",
    "In any case, we need to sample from:\n",
    "\n",
    "$$P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\mathbf{X}, \\alpha)$$\n",
    "\n",
    "We have the following known expressions, ignoring expressions in $\\mathbf{A}$:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{X} \\mid \\mathbf{Z}, \\theta_A)\n",
    "$$\n",
    "\n",
    "... where $\\theta_A$ is any prior parameters of $\\mathbf{A}$.  This expression is the likelihood.  And we have, for the IBP prior:\n",
    "\n",
    "$$\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\alpha)\n",
    "$$\n",
    "\n",
    "Let's try going via the joint directly, rather than using Bayes Theorem.  Let's start by factorizing $\\mathbf{Z}$ into $\\{ z_{nk}, \\mathbf{Z}_{-nk} \\}$:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{X} \\mid \\mathbf{Z}, \\theta_A)\n",
    "= P(\\mathbf{X} \\mid z_{nk}, \\mathbf{Z}_{-nk}, \\theta_A)\n",
    "$$\n",
    "\n",
    "Form the joint, in terms of variables for which we have distributions, which are $\\mathbf{X}$ and $z_{nk}$:\n",
    "\n",
    "$$P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\mathbf{X}, \\alpha, \\theta_A)$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{P(z_{nk}, \\mathbf{X} \\mid \\mathbf{Z}_{-nk}, \\alpha, \\theta_A)}\n",
    "   {p(\\mathbf{X} \\mid \\mathbf{Z}_{-nk}, \\alpha, \\theta_A)}\n",
    "$$\n",
    "\n",
    "Condition on $z_{nk}$, to try to get the likelihood probability expression:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{p(\\mathbf{X} \\mid z_{nk}, \\mathbf{Z}_{-nk}, \\alpha, \\theta_A) \\, p(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\alpha, \\theta_A)}\n",
    "   {p(\\mathbf{X} \\mid \\mathbf{Z}_{-nk}, \\alpha, \\theta_A)}\n",
    "$$\n",
    "\n",
    "Remove superfluous hyperparameters:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{p(\\mathbf{X} \\mid z_{nk}, \\mathbf{Z}_{-nk}, \\theta_A) \\, p(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\alpha)}\n",
    "   {p(\\mathbf{X} \\mid \\mathbf{Z}_{-nk}, \\theta_A, \\alpha)}\n",
    "$$\n",
    "\n",
    "... but the denominator here, ie conditional probability of $\\mathbf{X}$ given $\\mathbf{Z}_{-nk}$, is independent of $z_{nk}$, so we can remove that as a constant of normalization:\n",
    "\n",
    "$$\n",
    "\\propto\n",
    "P(\\mathbf{X} \\mid z_{nk}, \\mathbf{Z}_{-nk}, \\theta_A) \\, P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\alpha)\n",
    "$$\n",
    "\n",
    "And then defactorize $\\mathbf{Z}$:\n",
    "\n",
    "$$\n",
    "=\n",
    "P(\\mathbf{X} \\mid \\mathbf{Z}, \\theta_A) \\, P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\alpha)\n",
    "$$\n",
    "\n",
    ".... which matches the Griffiths and Ghahramani tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerated Sampling\n",
    "\n",
    "Let's work from the beginning of \"accelerated sampling\" again.  Working through the expressions in Doshi-Velez icml 2009 slides, we have:\n",
    "\n",
    "$$\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\mathbf{X})\n",
    "$$\n",
    "\n",
    "Expand conditioning on $\\mathbf{X}$, as division of joint by $p(\\mathbf{X} \\mid \\dots)$:\n",
    "$$\n",
    "= \\frac{P(z_{nk}, \\mathbf{X} \\mid \\mathbf{Z}_{-nk})}\n",
    "  {p(\\mathbf{X} \\mid \\mathbf{Z}_{-nk})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Factorize the joint:\n",
    "\n",
    "$$\n",
    "= \\frac{P(\\mathbf{X} \\mid z_{nk}, \\mathbf{Z}_{-nk})\\, P(z_{nk} \\mid \\mathbf{Z}_{-nk})}\n",
    "  {p(\\mathbf{X} \\mid \\mathbf{Z}_{-nk})}\n",
    "$$\n",
    "\n",
    "Merge $z_{nk}$ and $\\mathbf{Z}_{-nk}$:\n",
    "\n",
    "$$\n",
    "= \\frac{p(\\mathbf{X} \\mid \\mathbf{Z})\\, P(z_{nk} \\mid \\mathbf{Z}_{-nk})}\n",
    "  {p(\\mathbf{X} \\mid \\mathbf{Z}_{-nk})}\n",
    "$$\n",
    "\n",
    "And since denominator is independent of $z_{nk}$ convert to proportionality:\n",
    "\n",
    "$$\n",
    "\\propto p(\\mathbf{X} \\mid \\mathbf{Z})\\, P(z_{nk} \\mid \\mathbf{Z}_{-nk})\n",
    "$$\n",
    "\n",
    "Note that the above was entirely done without reference to any \"Bayes Rule\" :-)  (this is different from how the slides derive this; but I like not using Bayes Rule, since not using Bayes Rule is tentatively more general, I think.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\mathbf{X} \\mid \\mathbf{Z})$ is a valid expression, but we cannot directly write it down: must be obtained through marginalization, over $\\mathbf{A}$. Let's make the marginalization explicit:\n",
    "\n",
    "$$\n",
    "=\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk})\n",
    "\\int_{\\mathbf{A}}\n",
    "p(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A})\n",
    "\\,\n",
    "p(\\mathbf{A})\n",
    "\\,\n",
    "d\\mathbf{A}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorize over $n$ and $-n$.  We can do this, since the probability of each $\\mathbf{x}_i$ is conditionally independent, given $\\mathbf{A}$, and $\\mathbf{z}_i$, and:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{X} \\mid \\mathbf{Z}, \\mathbf{A})\n",
    "= \\prod_{i=1}^n\n",
    "p(\\mathbf{x}_i \\mid \\mathbf{z}_i, \\mathbf{A})\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=\n",
    "p(\\mathbf{x}_n \\mid \\mathbf{z}_n, \\mathbf{A})\n",
    "\\prod_{i=1}^{n-1}\n",
    "p(\\mathbf{x}_i \\mid \\mathbf{z}_i, \\mathbf{A})\n",
    "$$\n",
    "\n",
    "So:\n",
    "$$\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\mathbf{X})\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk})\n",
    "\\int_{\\mathbf{A}}\n",
    "p(\\mathbf{x}_n \\mid \\mathbf{z}_n, \\mathbf{A})\n",
    "\\,\n",
    "p(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n}, \\mathbf{A})\n",
    "\\,\n",
    "p(\\mathbf{A})\n",
    "\\,\n",
    "d\\mathbf{A}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then Doshi-Velez combines:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n}, \\mathbf{A})\\, p(\\mathbf{A})\n",
    "$$\n",
    "\n",
    "into:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{A} \\mid \\mathbf{Z}_{-n}, \\mathbf{X}_{-n})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that in slow motion.  We have:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n}, \\mathbf{A})\n",
    "\\,\n",
    "p(\\mathbf{A})\n",
    "$$\n",
    "\n",
    "Form the joint:\n",
    "$$\n",
    "=\n",
    "p(\\mathbf{A}, \\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n})\n",
    "$$\n",
    "\n",
    "Condition on $\\mathbf{X}_{-n}$:\n",
    "\n",
    "$$\n",
    "=\n",
    "p(\\mathbf{A} \\mid \\mathbf{Z}_{-n}, \\mathbf{X}_{-n})\\, p(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n})\n",
    "$$\n",
    "\n",
    "So, the left-hand term matches the Doshi-Velez slides. I imagine that the right-hand term vanishes into proportionality.  Things we should probably check:\n",
    "\n",
    "- does $p(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n})$ change the shape of what we are integrating over?\n",
    "- ... and I guess that's sufficient?  If it doesnt change the shape of what we are integrating over, then the result of the integration should be unchanged, to within a constant of proportionality?\n",
    "\n",
    "In any case, the term $p(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n})$:\n",
    "\n",
    "- does not contain the variable in the probability distribution we are seeking, ie $z_n$\n",
    "- does not contain the variable we are integrating over, ie $\\mathbf{A}$\n",
    "\n",
    "... so that seems reasonable evidence that it wont effect the shape either of the integral, or of the entire probability distribution we are seeking?\n",
    "\n",
    "So, remove it, in proportionality, and we have:\n",
    "\n",
    "$$\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\mathbf{X}) \\propto\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk})\n",
    "\\int_{\\mathbf{A}}\n",
    "p(\\mathbf{x}_n \\mid \\mathbf{z}_n, \\mathbf{A})\n",
    "\\,\n",
    "p(\\mathbf{A} \\mid \\mathbf{Z}_{-n}, \\mathbf{X}_{-n})\n",
    "\\,\n",
    "d\\mathbf{A}\n",
    "$$\n",
    "\n",
    "... as required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm kind of curious how we get a posterior only conditioned on $\\mathbf{Z}_{-n}$ though, so let's write down the terms in full, without doing the proportionality bit, and just look at it a bit:\n",
    "\n",
    "$$\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\mathbf{X})\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk})\n",
    "\\int_{\\mathbf{A}}\n",
    "p(\\mathbf{x}_n \\mid \\mathbf{z}_n, \\mathbf{A})\n",
    "\\,\n",
    "p(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n}, \\mathbf{A})\n",
    "\\,\n",
    "p(\\mathbf{A})\n",
    "\\,\n",
    "d\\mathbf{A}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=\n",
    "P(z_{nk} \\mid \\mathbf{Z}_{-nk})\n",
    "\\int_{\\mathbf{A}}\n",
    "p(\\mathbf{x}_n \\mid \\mathbf{z}_n, \\mathbf{A})\n",
    "\\,\n",
    "p(\\mathbf{A} \\mid \\mathbf{Z}_{-n}, \\mathbf{X}_{-n})\n",
    "\\,\n",
    "p(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n})\n",
    "\\,\n",
    "P(\\mathbf{Z}_{-n})\n",
    "\\,\n",
    "d\\mathbf{A}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redo that term in slow motion again:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n}, \\mathbf{A})\n",
    "\\,\n",
    "p(\\mathbf{A})\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=\n",
    "p(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n}, \\mathbf{A})\n",
    "\\,\n",
    "p(\\mathbf{A}, \\mathbf{Z}_{-n} \\mid \\mathbf{Z}_{-n})\n",
    "\\,\n",
    "P(\\mathbf{Z}_{-n})\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=\n",
    "p(\\mathbf{A}, \\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n})\n",
    "\\,\n",
    "P(\\mathbf{Z}_{-n})\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=\n",
    "p(\\mathbf{A} \\mid \\mathbf{Z}_{-n}, \\mathbf{X}_{-n})\n",
    "\\,\n",
    "p(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n})\n",
    "\\,\n",
    "P(\\mathbf{Z}_{-n})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, wait.... I did:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X}_{−n} \\mid \\mathbf{Z}_{−n}, \\mathbf{A})\\, p(\\mathbf{A})\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=p(\\mathbf{A}, \\mathbf{X}_{−n} \\mid \\mathbf{Z}_{−n})\n",
    "$$\n",
    "\n",
    "... but that's not valid. We could only form the joint if we had:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X}_{−n} \\mid \\mathbf{Z}_{−n}, \\mathbf{A})\\, p(\\mathbf{A} \\mid \\mathbf{Z}_{-n})\n",
    "$$\n",
    "\n",
    "So, let's ponder that / redo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Let's condition \\mathbf{A} on \\mathbf{Z}_{-n}:~~\n",
    "\n",
    "\n",
    "~~p(\\mathbf{X}_{−n} \\mid \\mathbf{Z}_{−n}, \\mathbf{A})\\, p(\\mathbf{A})~~\n",
    "\n",
    "~~&nbsp;~~\n",
    "\n",
    "\n",
    "~~=\n",
    "p(\\mathbf{X}_{−n} \\mid \\mathbf{Z}_{−n}, \\mathbf{A})\\, p(\\mathbf{A}, \\mathbf{Z}_{-n} \\mid \\mathbf{Z}_{-n})\\, P(\\mathbf{Z}_{-n})~~\n",
    "\n",
    "~~So, we'll get the same expression as before, just multiplied by \\mathbf{Z}_{-n}, ie:~~\n",
    "\n",
    "~~P(z_{nk} \\mid \\mathbf{Z}_{-nk}, \\mathbf{X})\\,P(\\mathbf{Z}_{-n})~~\n",
    "\n",
    "~~... but then we just make the same argument of proportionality as before.  ie, we state that \\mathbf{Z}_{-n}:~~\n",
    "\n",
    "- ~~is not the random variable in the probability distribution we are seeking, ie z_n~~\n",
    "- ~~is not the variable we are integrating over, ie \\mathbf{A}~~\n",
    "\n",
    "~~... and therefore can be removed in proportionality:~~\n",
    "\n",
    "~~\\propto p(\\mathbf{A} \\mid \\mathbf{Z}_{-n}, \\mathbf{X}_{-n})~~\n",
    "\n",
    "~~... as before~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's form the joint with $\\mathbf{Z}_{-n}$, to get it out of the conditioning variables:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X}_{−n} \\mid \\mathbf{Z}_{−n}, \\mathbf{A})\\, p(\\mathbf{A})\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{p(\\mathbf{X}_{-n}, \\mathbf{Z}_{-n} \\mid \\mathbf{A})}\n",
    "  {p(\\mathbf{Z}_{-n} \\mid \\mathbf{A})}\n",
    "p(\\mathbf{A})\n",
    "$$\n",
    "\n",
    "Joint with $\\mathbf{A}$:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{p(\\mathbf{X}_{-n}, \\mathbf{Z}_{-n}, \\mathbf{A})}\n",
    "  {p(\\mathbf{Z}_{-n} \\mid \\mathbf{A})}\n",
    "$$\n",
    "\n",
    "Recondition on $\\mathbf{Z}_{-n}$:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{p(\\mathbf{X}_{-n}, \\mathbf{A} \\mid \\mathbf{Z}_{-n})\\, P(\\mathbf{Z}_{-n})}\n",
    "  {p(\\mathbf{Z}_{-n} \\mid \\mathbf{A})}\n",
    "$$\n",
    "\n",
    "Condition on $\\mathbf{X}_{-n}$:\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{p(\\mathbf{A} \\mid \\mathbf{Z}_{-n}, \\mathbf{X}_{-n})\\, P(\\mathbf{X}_{-n} \\mid \\mathbf{Z}_{-n})\\, P(\\mathbf{Z}_{-n})}\n",
    "  {p(\\mathbf{Z}_{-n} \\mid \\mathbf{A})}\n",
    "$$\n",
    "\n",
    "The left hand term, on the numerator, is the term we want.  The other terms in the numerator are plausibly just normalization constants.  But what about that term in $\\mathbf{A}$, in the denominator? That doesnt look much like a normalizing constant...\n",
    "\n",
    "It seems like, whatever we do with it, we're still going to end up with a term in $\\mathbf{A}$ somewhere from it, unlikely that the term in $\\mathbf{A}$ is just going to vanish?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we ignore $P(\\mathbf{Z}_{-n} \\mid \\mathbf{A})$?\n",
    "\n",
    "Thinking this through, if my maths is right (which... I wouldnt hold my breath on this point :-P ), then the question is, can we come up with a scenario where knowing $\\mathbf{A}$, without knowing $\\mathbf{X}$, tells us something about $\\mathbf{Z}$?  I guess we can come up with a scenario fairly easily, based on the linear-Gaussian toy example in the Griffiths and Ghahramani tutorial?\n",
    "\n",
    "Let's go through the toy example bit by bit:\n",
    "\n",
    "#### Means, $\\mathbf{A}$\n",
    "\n",
    "We have a matrix of means, $\\mathbf{A}$.  This matrix has $K_+$ rows, and $D$ columns.  In the example, each row represents an image, where each row, $\\mathbf{a}_k$ given image matrix $\\mathbf{B}_k$ is formed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_k = (\\mathrm{vec}(\\mathbf{B}_k^T))^T\n",
    "$$\n",
    "\n",
    "It's more interesting to plot these images, so here is an example of a depiction of the mean matrices, $\\mathbf{A}$ for the toy example.  We have $K_+ = 4$, and we are plotting here the underlying images $\\mathbf{B}_k$ for each value of $k$:\n",
    "\n",
    "![](img/A_means.png)\n",
    "\n",
    "Here we have the dark grey is a mean of 0, and the light grey is a mean of 1\n",
    "\n",
    "And, well, actually, this is already sufficient to come up with a way in which knowing $\\mathbf{A}$ tells us something about the distribution of $\\mathbf{Z}$: since it tells us straight away how many features there are?  Without knowing $\\mathbf{A}$, we dont know if there is 1 feature, or a thousand, and have to sample over this for a while, to determine it.\n",
    "\n",
    "So, thinking further, it means that the probability mass for any $\\mathbf{Z}$ matrices with the number of columns not equal to the number of rows of $\\mathbf{A}$ will be automatically 0.  I think?  At least, such matrices will have a significantly lower density/mass I think?  Whereas matrices where the number of columns does match should I think have significantly higher probability mass?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
