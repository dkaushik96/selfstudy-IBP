{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration on 6x6 images\n",
    "\n",
    "The code for this demo is at [toysamples1.py](toysamples1.py). I moved it out of the notebook, for now, because displaying the output images here was eating up too much web browser memory.  It might move back at some point, since I could simply run that code from here, now that the images are now saved to files; and then just display images I want here, just as now.\n",
    "\n",
    "I'll paste some of the images output from [toysamples1.py](toysamples1.py) here.  They might or might not be up to date with the real output of the code.  They also might or might not be all generated from the same set of data samples.\n",
    "\n",
    "*Note that the sampler is broken for now, in the sense of, gives nonsense samples.  You can see below.*\n",
    "\n",
    "### Example X, generated input data\n",
    "\n",
    "Here is an example of the samples drawn from initial generative process:\n",
    "\n",
    "![](img/toysamples1/samples.png)\n",
    "\n",
    "These are not at all noisy, because I cnat even get the drawn samples to be reasonable on un-noisy data yet :-D  I'll make it noisier once it's working on un-noisy data\n",
    "\n",
    "### Corresponding input Z features, ie ground truth Z\n",
    "\n",
    "![](img/toysamples1/samples_Z.png)\n",
    "\n",
    "The Z values look ok-ish.  Note that these are the Z values used to generate the input data.  They're not the results of sampling, which are further down below.\n",
    "\n",
    "### Expectation of A, based on ground truth Z\n",
    "\n",
    "![](img/A_means.png)\n",
    "\n",
    "This suggests that the method for calculating the expectation of A works for at least the ground truth data.\n",
    "\n",
    "### The matrix $(\\mathbf{Z}^\\mathbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I})^{-1}\\mathbf{Z}^T$, using ground truth Z\n",
    "\n",
    "![](img/toysamples1/A_from_ground_truth_Z.png_ZTZIinvZT.png)\n",
    "\n",
    "We can see that this takes a bit of every data example, giving emphasis to features in examples that only contain 1 feature, and less emphasis to features in examples that have multiple features.  But drawing in any case from all data points.\n",
    "\n",
    "### Sampled A, from Gibbs iteration 500\n",
    "\n",
    "![](img/toysamples1/A_draws_it500.png)\n",
    "\n",
    "It has at least not 0 or tons of features, but the number of features is not quite right.  The features themselves are also not very right...\n",
    "\n",
    "### The matrix $(\\mathbf{Z}^\\mathbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I})^{-1}\\mathbf{Z}^T$, from Gibbs iteration 500\n",
    "\n",
    "![](img/toysamples1/A_draws_it500.png_ZTZIinvZT.png)\n",
    "\n",
    "Unlike the equivalent matrix for the ground truth Z values, almost all data points are ignored.  Only two data points are used, with one feature from each.  This is ... odd, and probably gives some insight into what the bug is currently, whatever/wherever that is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges/observations/ideas\n",
    "\n",
    "### Challenge: sampling from non-normalized distribution?\n",
    "\n",
    "For sampling from the posterior, per the tutorial, we should use the tutorial's equation 22.  But there's a proportional-to sign, $\\propto$.  How to handle this?  Since I'm not trying to do this research from scratch, just reproduce/understand the existing research, I reached out to Mr Google, to look for more explanations.  I found a video from Finale Doshi-Velez, and interluded out to her \"accelerated sampling\" presentation, slides and paper.  The interlude is at: [accelerated_gibbs_samplings.ipynb](accelerated_gibbs_sampling.ipynb).  So, if you're trying to follow along with my own thought processes, challenges, solutions, etc, you might want to go there now.  Or you can just continue, it's all good :-)\n",
    "\n",
    "From reaching out to Doshi-Velez's presentation, it looks like we can sample a distribution that is only provided in proportionality, as long as it has a shape, which is well-defined, that we know how to sample from, ie typically a Gaussian.  There are probably other ways of handling other distributions, but a Gaussian should be reasonably straightforward to sample from, if we can get the distribution in that form.  Let's reach right back to the first section, and see what distribution(s) we need to sample from.\n",
    "\n",
    "Equation 22 from the Griffiths and Ghahramani tutorial states:\n",
    "\n",
    "$$\n",
    "P(z_{ik} \\mid \\mathbf{X}, \\mathbf{Z}_{-(i,k)}, \\sigma_X, \\sigma_A)\n",
    "\\propto\n",
    "p(\\mathbf{X} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)\n",
    "\\,\n",
    "P(z_{ik} \\mid \\mathbf{z}_{-i,k})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term of this equation, ie the likelihood of $\\mathbf{X}$, given the latent variables, and the hyper-parameters, is a Gaussian.  For the finite model, $P(z_{ik} \\mid \\mathbf{Z}_{-i,k})$ is given by equation 17 in the tutorial:\n",
    "\n",
    "$$\n",
    "P(z_{ik} = 1 \\mid \\mathbf{z}_{-i,k})\n",
    "= \\frac{m_{-i,k} + \\frac{\\alpha}{K}}\n",
    "  {N + \\frac{\\alpha}{K}}\n",
    "$$\n",
    "\n",
    "This seems not to be a Gaussian.  How to sample from the product of a Gaussian and this term?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: is the product of two normalized distributions also normalized?\n",
    "\n",
    "Brainstorming a bit, we could sample from the Gaussian, which we could normalize first, and then multiply by $p(z_{ik} = 1 \\mid \\mathbf{z}_{-i,k})$.  Is it fair to say that the product of two normalized probability functions will be normalized?  Probably not, eg we could have the following two distributions:\n",
    "\n",
    "$$\n",
    "f(x) = 1\n",
    "\\mathrm{\\,when\\,} x \\ge 0 \\mathrm{\\,and\\,} x \\le 1 \\\\ \n",
    "= 0 \\mathrm{\\, otherwise}\n",
    "$$\n",
    "\n",
    "(which integrates to 1), and:\n",
    "\n",
    "$$\n",
    "g(x) = 1\n",
    "\\mathrm{\\,when\\,} x \\ge 2 \\mathrm{\\,and\\,} x \\le 3 \\\\ \n",
    "= 0 \\mathrm{\\, otherwise}\n",
    "$$\n",
    "\n",
    "... which integrates to 1 too.  But their product integrates to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate the un-normalized distribution over $z_{ik}$?\n",
    "\n",
    "Actually, the equation for the probaiblty of $z_{ik} = 1$ is not actually a probability distribution: it's the value of this probaiblity for one specific value of $z_{ik}$, ie $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try integrating over $c \\cdot p(\\mathbf{X} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A) \\cdot P(z_{ik} \\mid \\mathbf{z}_{-i,k})$ $z_{ik}$, using a probability distribution of $z_{ik}$, rather than just one specific value, and where $c$ is a constant of normalization, that will make the integrant integrate to $1$.\n",
    "\n",
    "$$\n",
    "\\int\n",
    "c\n",
    "\\cdot\n",
    "P(\\mathbf{X} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)\n",
    "\\cdot\n",
    "P(z_{ik})\n",
    "\\,\n",
    "dz_{ik}\n",
    "$$\n",
    "\n",
    "And since $z_{ik}$ is discrete, ie $z_{ik} \\in \\{0, 1\\}$, then we can rewrite the integral as a sum:\n",
    "\n",
    "$$\n",
    "=\n",
    "c\n",
    "\\sum_{z_{ik}=0}^1\n",
    "\\left(\n",
    "    P(\\mathbf{X} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)\n",
    "    \\cdot\n",
    "    P(z_{ik})\n",
    "\\right)\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "=\n",
    "c\n",
    "\\sum_{z_{ik}=0}^1\n",
    "\\left(\n",
    "    \\mathcal{N}(\\mathbf{X}; \\mu_{\\mathbf{Z}, \\sigma_A, \\sigma_X}, \\Sigma_{\\mathbf{Z}, \\sigma_A, \\sigma_X})\n",
    "    \\cdot\n",
    "    P(z_{ik})\n",
    "\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it seems like maybe we can simply calculate the value of the gaussian, for $z_{ik} \\in \\{0, 1\\}$, and multiply by $P(z_{ik} \\mid \\mathbf{z}_{-i,k})$, each time; and then normalize the sum of these two products?  Just to imagine this a bit, let's say we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_X_given_Z = [0.03, 0.02]  # pretend Gaussian samples, not normalized\n",
    "p_zik_given_Z_minus = [0.8, 0.2]  # normalized, sum to 1.0\n",
    "\n",
    "#Then\n",
    "p_zik_given_X_Z = [0] * 2\n",
    "for zik in [0, 1]:\n",
    "    p_zik_given_X_Z[zik] = p_X_given_Z[zik] * p_zik_given_Z_minus[zik]\n",
    "\n",
    "print(p_zik_given_X_Z)\n",
    "\n",
    "# normalize\n",
    "p_zik_given_X_Z /= np.sum(p_zik_given_X_Z)\n",
    "print('normalized p_zik_given_X_Z', p_zik_given_X_Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the normalized values, with this toy data, are influenced by both the likelihood, and by the prior.\n",
    "\n",
    "Let's run with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving for expected A\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mathbf{A}] = (\\mathbf{Z}^T\\mathbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I})^{-1}\\mathbf{Z}^T\\mathbf{X}\n",
    "$$\n",
    "\n",
    "It'd be good to avoid that inverse.  Can we avoid it using a solver?  The numpy solver, [numpy.linalg.solve](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html#numpy.linalg.solve) solves equations in the form:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{X} = \\mathbf{B}\n",
    "$$\n",
    "\n",
    "So, let's put the equation in $\\mathbb{E}[\\mathbf{A}]$ above into this form:\n",
    "\n",
    "$$\n",
    "(\\mathbf{Z}^T\\mathbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I})\\mathbb{E}[\\mathbf{A}] = \\mathbf{Z}^T\\mathbf{X}\n",
    "$$\n",
    "\n",
    "So, using $\\mathrm{\\backslash}$ to denote \"solve\", we have:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mathbf{A}] = \\mathbf{Z}^T\\mathbf{X} \\mathrm{\\,\\backslash\\,} (\\mathbf{Z}^T\\mathbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use solver for  the exponential trace term?\n",
    "\n",
    "The exponential trace term is:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T\n",
    "\\left(\n",
    "    \\mathbf{I}\n",
    "    -\n",
    "    \\mathbf{Z}_+\n",
    "    \\left(\n",
    "      \\mathbf{Z}_+^T\\mathbf{Z}_+ + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}_{K_+}\n",
    "    \\right)^{-1}\n",
    "    \\mathbf{Z}_+^T\n",
    "\\right)\n",
    "\\mathbf{X}\n",
    "$$\n",
    "\n",
    "Let's call this $\\mathbf{R}$, so:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}\n",
    "=\n",
    "\\mathbf{X}^T\n",
    "\\left(\n",
    "    \\mathbf{I}\n",
    "    -\n",
    "    \\mathbf{Z}_+\n",
    "    \\left(\n",
    "      \\mathbf{Z}_+^T\\mathbf{Z}_+ + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}_{K_+}\n",
    "    \\right)^{-1}\n",
    "    \\mathbf{Z}_+^T\n",
    "\\right)\n",
    "\\mathbf{X}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "X^{T^{-1}}\n",
    "\\mathbf{R}\n",
    "X^{-1}\n",
    "=\n",
    "\\mathbf{I}\n",
    "-\n",
    "\\mathbf{Z}_+\n",
    "\\left(\n",
    "  \\mathbf{Z}_+^T\\mathbf{Z}_+ + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}_{K_+}\n",
    "\\right)^{-1}\n",
    "\\mathbf{Z}_+^T\n",
    "$$\n",
    "\n",
    "So:\n",
    "$$\n",
    "\\mathbf{Z}_+^{-1}\n",
    "\\left(\n",
    "    \\mathbf{I}\n",
    "    -\n",
    "    X^{T^{-1}}\n",
    "    \\mathbf{R}\n",
    "    X^{-1}\n",
    "\\right)\n",
    "\\mathbf{Z}_+^{T^{-1}}\n",
    "=\n",
    "\\left(\n",
    "  \\mathbf{Z}_+^T\\mathbf{Z}_+ + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}_{K_+}\n",
    "\\right)^{-1}\n",
    "$$\n",
    "\n",
    "So:\n",
    "$$\n",
    "\\left(\n",
    "  \\mathbf{Z}_+^T\\mathbf{Z}_+ + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}_{K_+}\n",
    "\\right)\n",
    "\\mathbf{Z}_+^{-1}\n",
    "\\left(\n",
    "    \\mathbf{I}\n",
    "    -\n",
    "    X^{T^{-1}}\n",
    "    \\mathbf{R}\n",
    "    X^{-1}\n",
    "\\right)\n",
    "\\mathbf{Z}_+^{T^{-1}}\n",
    "=\n",
    "\\mathbf{I}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "  \\mathbf{Z}_+^T\\mathbf{Z}_+ + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}_{K_+}\n",
    "\\right)\n",
    "\\mathbf{Z}_+^{-1}\n",
    "\\left(\n",
    "    \\mathbf{I}\n",
    "    -\n",
    "    X^{T^{-1}}\n",
    "    \\mathbf{R}\n",
    "    X^{-1}\n",
    "\\right)\n",
    "=\n",
    "\\mathbf{Z}_+^T\n",
    "$$\n",
    "\n",
    "Seems no obvious way to get this down to a single inverse, that we could use a solver against?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what about if we just try to get one level down for now?, ie solve for $\\mathbf{S}$, where:\n",
    "\n",
    "$$\n",
    "\\mathbf{S}\n",
    "= \\mathbf{I} - \\mathbf{Z}_+\n",
    "\\left(\n",
    "  \\mathbf{Z}_+^T\\mathbf{Z}_+ + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}_{K_+}\n",
    "\\right)^{-1}\n",
    "\\mathbf{Z}_+^T\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "\\mathbf{I} - \\mathbf{S}\n",
    "= \\mathbf{Z}_+\n",
    "\\left(\n",
    "  \\mathbf{Z}_+^T\\mathbf{Z}_+ + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}_{K_+}\n",
    "\\right)^{-1}\n",
    "\\mathbf{Z}_+^T\n",
    "$$\n",
    "\n",
    "Seems no way forward, even at this level?  Have to 'solve' the inner inverse, ie $\\left(\n",
    "  \\mathbf{Z}_+^T\\mathbf{Z}_+ + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}_{K_+}\n",
    "\\right)$ against $\\mathbf{I}$, which is just the inverse...  Not a biggie, just an optimization that is not obviously possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of exponential trace term\n",
    "\n",
    "The exponential trace term, for $P(\\mathbf{X} \\mid \\mathbf{Z}, \\sigma_X, \\sigma_A)$, gives very extreme outputs, following exponentiation.  Can we normalize it somewhat, prior to exponentiation, to reduce the extreme values?  Let's use the following symbols:\n",
    "\n",
    "- $e_0$, $e_1$: the un-normalized result of exponentiation, for $z_{nk}=0$ and $z_{nk}=1$ respectively\n",
    "- $t_0$, $t_1$: the exponent of the exponential, for $z_{nk}=0$ and $z_{nk}=1$ respectively\n",
    "\n",
    "So, $t_1$ and $t_0$ are the results of the following expression:\n",
    "\n",
    "$$\n",
    "- \\frac{1}{2\\sigma_X^2} \\mathrm{tr}\\left(\\mathbf{X}^T\\left(\\mathbf{I} - \\mathbf{Z}_+\\left(\\mathbf{Z}_+^T\\mathbf{Z}_+ + \\frac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}_{K_+}\\right)^{-1}\\mathbf{Z}_+^T\\right)\\mathbf{X}\\right)\n",
    "$$\n",
    "\n",
    "And $e_0$ and $e_1$ are the result of, eg for $e_0$:\n",
    "\n",
    "$$\n",
    "e_0 = \\exp(t_0)\n",
    "$$\n",
    "\n",
    "Then, let's normalize so that:\n",
    "\n",
    "$$\n",
    "ce_0 + ce_1 = 1\n",
    "$$\n",
    "(where $c$ is a scalar constant of normalization)\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "c\\exp(t_0) + c\\exp(t_1) = 1\n",
    "$$\n",
    "\n",
    "Hmmm, that seems not to be easy to take logs of.  Let's instead calculate $e_{min}$ and $e_{max}$:\n",
    "\n",
    "$$\n",
    "e_{min} = \\min\\{e_0, e_1\\}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "e_{max} = \\max\\{e_0, e_1\\}\n",
    "$$\n",
    "\n",
    "Then lets divide both terms by $e_{min}$.  The results of this will be $r_{min}$ and $r_{max}$, where:\n",
    "\n",
    "$$\n",
    "r_{min} = 1\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "r_{max} = \\frac{e_{max}}{e_{min}}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "Let's define trace terms for $e_{min}$ and $e_{max}$ as $t_{min}$ and $t_{max}$ respectively.  Since $\\exp(x)$ is monotonic in $x$, $t_{min}$ < $t_{max}$.  We have:\n",
    "\n",
    "$$\n",
    "r_{max} = \\frac{\\exp(t_{max})}{\\exp(t_{min})}\n",
    "$$\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "= \\exp(t_{max} - t_{min})\n",
    "$$\n",
    "\n",
    "And, taking logs:\n",
    "\n",
    "$$\n",
    "\\log(r_{max}) = t_{max} - t_{min}\n",
    "$$\n",
    "\n",
    "To normalize the exponential trace terms therefore, we can first subtract the trace terms, then take the exponential of that differene.  The result should be one term is 1, and the other is greater than 1, but hopefully not orders of magnitudes away from 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Results for $\\sigma_X= 0.01$\n",
    "\n",
    "In this scenario, there is not much randomness in the generated samples.  One would expect this to make the task easy.  In fact, what happens is that the log gaussian terms differ by several thousand, at least, for $z_{ik}=0$ versus $z_{ik}= 1$.  Therefore the exponential terms differ by infinity, and nothing really works...\n",
    "\n",
    "These are the results currently depicted at the top of this notebook, for now.\n",
    "\n",
    "### Results for: generate with $\\sigma_X=0.01$, Gibbs sample with $\\sigma_X=1.0$\n",
    "\n",
    "In this configuration, sampling does actually produce results.  On the other hand $K_+$ ends up somewhere around 20-30 or so, rather than the 4 that we'd hope for.\n",
    "\n",
    "Example draws:\n",
    "\n",
    "#### Matrix A after 100 iterations\n",
    "\n",
    "![](img/toysamples1/alphaxdiff/A_draws_it100.png)\n",
    "\n",
    "#### Matrix Z after 100 iterations\n",
    "\n",
    "![](img/toysamples1/alphaxdiff/A_draws_it100.png_Z.png)\n",
    "\n",
    "#### Matrix ZTZinvZT after 100 iterations\n",
    "\n",
    "![](img/toysamples1/alphaxdiff/A_draws_it100.png_ZTZIinvZT.png)\n",
    "\n",
    "I wonder if some combination of reducing $\\alpha$, or Metropolis-Sampling $\\alpha$ might help?  Having to tweak $\\alpha$ seems to defeat the point of using an infinite model?\n",
    "\n",
    "### Results for: generate with $\\sigma_X=0.01$, Gibbs sample with $\\sigma_X=1.0$; drop alpha to 0.1\n",
    "\n",
    "With alpha dropped to 0.1, the number of features $K_+$ becomes approximately right, near-ish 4.  But the sampled values of A are not quite right.  Many of the sampled A images have multiple features in.  And then some have negative features, shown in jet black in the pictures.  Here are the results of iteration 500, for one run with this configuration:\n",
    "\n",
    "#### Matrix A after 100 iterations\n",
    "\n",
    "![](img/toysamples1/alpha0_1/A_draws_it500.png)\n",
    "\n",
    "#### Matrix Z after 100 iterations\n",
    "\n",
    "![](img/toysamples1/alpha0_1/A_draws_it500.png_Z.png)\n",
    "\n",
    "#### Matrix ZTZinvZT after 100 iterations\n",
    "\n",
    "![](img/toysamples1/alpha0_1/A_draws_it500.png_ZTZIinvZT.png)\n",
    "\n",
    "\n",
    "### Results for, keep $\\sigma_X=0.01$ for sampling, check log prob first\n",
    "\n",
    "At this point I thought, what happens if we first check the log probaiblity,for the likelihood, and just directly choose the value of z_ik, if the log probaiblity is greater than 12 or so?  $\\exp(12)$ is about 100,000, which seems pretty close to certain.  After making this change, it was then possible to sample with $\\alpha_X=0.01$, and get ok-ish results.  At least, similar to the previous results section, so, the features keep increasing, rather than staying at around 4 or so.  But the features do look approximately reasonable:\n",
    "\n",
    "#### Matrix A after 100 iterations\n",
    "\n",
    "![](img/toysamples1/checklogprobfirst/A_draws_it200.png)\n",
    "\n",
    "#### Matrix Z after 100 iterations\n",
    "\n",
    "![](img/toysamples1/checklogprobfirst/A_draws_it200.png_Z.png)\n",
    "\n",
    "#### Matrix ZTZinvZT after 100 iterations\n",
    "\n",
    "![](img/toysamples1/checklogprobfirst/A_draws_it200.png_ZTZIinvZT.png)\n",
    "\n",
    "Ooohhhh....! I missed something :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to add in the posterior probability when calculating the probaiblity of adding new features :-)\n",
    "\n",
    "From the Griffiths and Ghahramani tutorial, end of section 5.2:\n",
    "\n",
    "![](img/distributionovernewfeatures.png)\n",
    "\n",
    "The current code (up to this point), doesnt \"quite\" include the posterior probability.  Let's update the code to do this.\n",
    "\n",
    "Note that the old code, up to this point is at https://github.com/hughperkins/selfstudy-IBP/blob/af73ad7f5c29a86fc0dd4a95a427faade078497f/toysamples1.py\n",
    "\n",
    "[at this point, I went away and updated the code to handle this]\n",
    "\n",
    "So, after updating the code, the issue with too-many-features is actually worse.  And in retrospect it seems like it should make this issue worse: increasing the number of features in Z improve the likelihood probability, in the absence of a regularizing prior.  So, then, the question is, why isnt the prior regularizing very much?  Is it because alpha is too high?  Should we be sampling alpha, eg using Metropolis-Hastings?  (edit: added commandline option to [toysamples1.py](toysamples1.py), `--new-features-ignore-posterior`, which recovers the old behavior).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
